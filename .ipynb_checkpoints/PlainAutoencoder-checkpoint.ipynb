{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training loss: -577.746\n",
      "step 500, training loss: -209.508\n",
      "step 1000, training loss: -205.987\n",
      "step 1500, training loss: -181.681\n",
      "step 2000, training loss: -182.018\n",
      "step 2500, training loss: -167.174\n",
      "step 3000, training loss: -169.483\n",
      "step 3500, training loss: -168.534\n",
      "step 4000, training loss: -160.489\n",
      "step 4500, training loss: -165.525\n",
      "step 5000, training loss: -169.596\n",
      "step 5500, training loss: -169.189\n",
      "step 6000, training loss: -156.628\n",
      "step 6500, training loss: -160.242\n",
      "step 7000, training loss: -157.868\n",
      "step 7500, training loss: -147.558\n",
      "step 8000, training loss: -153.02\n",
      "step 8500, training loss: -158.341\n",
      "step 9000, training loss: -146.48\n",
      "step 9500, training loss: -152\n",
      "step 10000, training loss: -145.407\n",
      "step 10500, training loss: -145.296\n",
      "step 11000, training loss: -144.616\n",
      "step 11500, training loss: -150.377\n",
      "step 12000, training loss: -139.373\n",
      "step 12500, training loss: -138.872\n",
      "step 13000, training loss: -143.516\n",
      "step 13500, training loss: -149.234\n",
      "step 14000, training loss: -144.215\n",
      "step 14500, training loss: -142.568\n",
      "step 15000, training loss: -148.042\n",
      "step 15500, training loss: -143.081\n",
      "step 16000, training loss: -134.218\n",
      "step 16500, training loss: -143.309\n",
      "step 17000, training loss: -144.168\n",
      "step 17500, training loss: -142.748\n",
      "step 18000, training loss: -134.622\n",
      "step 18500, training loss: -134.516\n",
      "step 19000, training loss: -139.934\n",
      "step 19500, training loss: -141.125\n",
      "step 20000, training loss: -142.746\n",
      "step 20500, training loss: -135.532\n",
      "step 21000, training loss: -139.949\n",
      "step 21500, training loss: -140.284\n",
      "step 22000, training loss: -139.662\n",
      "step 22500, training loss: -133.453\n",
      "step 23000, training loss: -145.096\n",
      "step 23500, training loss: -143.416\n",
      "step 24000, training loss: -135.41\n",
      "step 24500, training loss: -137.642\n",
      "step 25000, training loss: -136.384\n",
      "step 25500, training loss: -135.959\n",
      "step 26000, training loss: -141.15\n",
      "step 26500, training loss: -135.446\n",
      "step 27000, training loss: -140.129\n",
      "step 27500, training loss: -140.538\n",
      "step 28000, training loss: -131.262\n",
      "step 28500, training loss: -132.251\n",
      "step 29000, training loss: -139.183\n",
      "step 29500, training loss: -132.104\n",
      "step 30000, training loss: -127.643\n",
      "step 30500, training loss: -137.825\n",
      "step 31000, training loss: -139.026\n",
      "step 31500, training loss: -140.12\n",
      "step 32000, training loss: -140.858\n",
      "step 32500, training loss: -130.297\n",
      "step 33000, training loss: -132.416\n",
      "step 33500, training loss: -145.308\n",
      "step 34000, training loss: -128.713\n",
      "step 34500, training loss: -135.007\n",
      "step 35000, training loss: -137.424\n",
      "step 35500, training loss: -141.387\n",
      "step 36000, training loss: -137.49\n",
      "step 36500, training loss: -137.876\n",
      "step 37000, training loss: -131.901\n",
      "step 37500, training loss: -132.673\n",
      "step 38000, training loss: -132.197\n",
      "step 38500, training loss: -135.278\n",
      "step 39000, training loss: -140.169\n",
      "step 39500, training loss: -125.818\n",
      "step 40000, training loss: -133.125\n",
      "step 40500, training loss: -136.334\n",
      "step 41000, training loss: -129.181\n",
      "step 41500, training loss: -140.602\n",
      "step 42000, training loss: -136.846\n",
      "step 42500, training loss: -134.771\n",
      "step 43000, training loss: -135.191\n",
      "step 43500, training loss: -134.653\n",
      "step 44000, training loss: -126.046\n",
      "step 44500, training loss: -133.752\n",
      "step 45000, training loss: -124.237\n",
      "step 45500, training loss: -130.356\n",
      "step 46000, training loss: -132.408\n",
      "step 46500, training loss: -126.696\n",
      "step 47000, training loss: -131.068\n",
      "step 47500, training loss: -135.328\n",
      "step 48000, training loss: -128.352\n",
      "step 48500, training loss: -127.706\n",
      "step 49000, training loss: -126.801\n",
      "step 49500, training loss: -131.341\n",
      "step 50000, training loss: -127.556\n",
      "step 50500, training loss: -132.638\n",
      "step 51000, training loss: -126.773\n",
      "step 51500, training loss: -125.06\n",
      "step 52000, training loss: -127.919\n",
      "step 52500, training loss: -133.267\n",
      "step 53000, training loss: -135.562\n",
      "step 53500, training loss: -132.708\n",
      "step 54000, training loss: -129.694\n",
      "step 54500, training loss: -131.985\n",
      "step 55000, training loss: -132.693\n",
      "step 55500, training loss: -135.333\n",
      "step 56000, training loss: -137.298\n",
      "step 56500, training loss: -132.745\n",
      "step 57000, training loss: -128.458\n",
      "step 57500, training loss: -132.722\n",
      "step 58000, training loss: -131.742\n",
      "step 58500, training loss: -131.732\n",
      "step 59000, training loss: -125.645\n",
      "step 59500, training loss: -133.869\n",
      "step 60000, training loss: -132.86\n",
      "step 60500, training loss: -132.884\n",
      "step 61000, training loss: -130.937\n",
      "step 61500, training loss: -134.265\n",
      "step 62000, training loss: -132.167\n",
      "step 62500, training loss: -128.709\n",
      "step 63000, training loss: -131.007\n",
      "step 63500, training loss: -130.415\n",
      "step 64000, training loss: -124.041\n",
      "step 64500, training loss: -127.416\n",
      "step 65000, training loss: -129.23\n",
      "step 65500, training loss: -126.767\n",
      "step 66000, training loss: -129.533\n",
      "step 66500, training loss: -132.153\n",
      "step 67000, training loss: -135.364\n",
      "step 67500, training loss: -127.209\n",
      "step 68000, training loss: -127.389\n",
      "step 68500, training loss: -132.656\n",
      "step 69000, training loss: -133.401\n",
      "step 69500, training loss: -138.22\n",
      "step 70000, training loss: -120.654\n",
      "step 70500, training loss: -129.674\n",
      "step 71000, training loss: -126.921\n",
      "step 71500, training loss: -127.263\n",
      "step 72000, training loss: -136.107\n",
      "step 72500, training loss: -133.863\n",
      "step 73000, training loss: -125.04\n",
      "step 73500, training loss: -130.978\n",
      "step 74000, training loss: -125.123\n",
      "step 74500, training loss: -133.61\n",
      "step 75000, training loss: -126.965\n",
      "step 75500, training loss: -128.873\n",
      "step 76000, training loss: -124.894\n",
      "step 76500, training loss: -128.059\n",
      "step 77000, training loss: -127.448\n",
      "step 77500, training loss: -131.209\n",
      "step 78000, training loss: -125.407\n",
      "step 78500, training loss: -129.573\n",
      "step 79000, training loss: -127.175\n",
      "step 79500, training loss: -126.998\n",
      "step 80000, training loss: -136.392\n",
      "step 80500, training loss: -130.463\n",
      "step 81000, training loss: -126.43\n",
      "step 81500, training loss: -127.311\n",
      "step 82000, training loss: -126.88\n",
      "step 82500, training loss: -131.582\n",
      "step 83000, training loss: -125.51\n",
      "step 83500, training loss: -128.787\n",
      "step 84000, training loss: -129.274\n",
      "step 84500, training loss: -135.544\n",
      "step 85000, training loss: -131.216\n",
      "step 85500, training loss: -129.66\n",
      "step 86000, training loss: -125.384\n",
      "step 86500, training loss: -132.268\n",
      "step 87000, training loss: -133.513\n",
      "step 87500, training loss: -127.321\n",
      "step 88000, training loss: -135.413\n",
      "step 88500, training loss: -127.866\n",
      "step 89000, training loss: -129.304\n",
      "step 89500, training loss: -130.47\n",
      "step 90000, training loss: -136.266\n",
      "step 90500, training loss: -126.584\n",
      "step 91000, training loss: -121.443\n",
      "step 91500, training loss: -126.278\n",
      "step 92000, training loss: -126.702\n",
      "step 92500, training loss: -130.463\n",
      "step 93000, training loss: -126.916\n",
      "step 93500, training loss: -128.376\n",
      "step 94000, training loss: -126.87\n",
      "step 94500, training loss: -135.821\n",
      "step 95000, training loss: -127.037\n",
      "step 95500, training loss: -127.287\n",
      "step 96000, training loss: -133.071\n",
      "step 96500, training loss: -122.439\n",
      "step 97000, training loss: -126.265\n",
      "step 97500, training loss: -130.347\n",
      "step 98000, training loss: -126.815\n",
      "step 98500, training loss: -126.97\n",
      "step 99000, training loss: -128.118\n",
      "step 99500, training loss: -126.543\n",
      "step 100000, training loss: -131.814\n",
      "step 100500, training loss: -134.851\n",
      "step 101000, training loss: -134.659\n",
      "step 101500, training loss: -126.916\n",
      "step 102000, training loss: -124.302\n",
      "step 102500, training loss: -130.759\n",
      "step 103000, training loss: -131.11\n",
      "step 103500, training loss: -124.772\n",
      "step 104000, training loss: -126.657\n",
      "step 104500, training loss: -129.298\n",
      "step 105000, training loss: -125.269\n",
      "step 105500, training loss: -132.87\n",
      "step 106000, training loss: -124.181\n",
      "step 106500, training loss: -133.325\n",
      "step 107000, training loss: -126.769\n",
      "step 107500, training loss: -119.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 108000, training loss: -135.387\n",
      "step 108500, training loss: -127.11\n",
      "step 109000, training loss: -132.123\n",
      "step 109500, training loss: -122.311\n",
      "step 110000, training loss: -123.347\n",
      "step 110500, training loss: -127.52\n",
      "step 111000, training loss: -130.938\n",
      "step 111500, training loss: -127.781\n",
      "step 112000, training loss: -125.892\n",
      "step 112500, training loss: -129.847\n",
      "step 113000, training loss: -133.069\n",
      "step 113500, training loss: -132.7\n",
      "step 114000, training loss: -132.848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf71713ac979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-cf71713ac979>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m#    28]), feed_dict={x : first_batch[0]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Save latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2191\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m     \"\"\"\n\u001b[0;32m-> 2193\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4756\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4757\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4758\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"TensorFlow MNIST AutoEncoder\n",
    "\n",
    "This is my attempt to write the autoencoder for MNIST by Andrej Karpathy using \n",
    "ConvNetJS in TensorFlow. Mostly to get some more experience working in \n",
    "Tensorflow.\n",
    "\n",
    "Sources:\n",
    "    - http://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html\n",
    "    - https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "Author: Gertjan van den Burg\n",
    "Date: Thu Oct 26 16:49:29 CEST 2017\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# from magenta.models.image_stylization.image_utils import form_image_grid\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GRID_ROWS = 5\n",
    "GRID_COLS = 10\n",
    "USE_RELU = False\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    # From the mnist tutorial\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def fc_layer(previous, input_size, output_size):\n",
    "    W = weight_variable([input_size, output_size])\n",
    "    b = bias_variable([output_size])\n",
    "    return tf.matmul(previous, W) + b\n",
    "\n",
    "def get_ll(target, output):\n",
    "    return tf.reduce_sum(\n",
    "        target * tf.log(output + 1e-10) + \\\n",
    "        (1 - target) * tf.log(1 - output + 1e-10),\n",
    "        reduction_indices=[-1]\n",
    "    )\n",
    "\n",
    "\n",
    "def autoencoder(x):\n",
    "    # first fully connected layer with 50 neurons using tanh activation\n",
    "    l1 = tf.nn.tanh(fc_layer(x, 28*28, 256))\n",
    "    tf.summary.histogram('outputs_l_1', l1)\n",
    "    # second fully connected layer with 50 neurons using tanh activation\n",
    "    l2 = tf.nn.tanh(fc_layer(l1, 256, 128))\n",
    "    tf.summary.histogram('outputs_l_2', l2)\n",
    "    # third fully connected layer with 2 neurons\n",
    "    l3 = fc_layer(l2, 128, 2)\n",
    "    tf.summary.histogram('outputs_l_3', l3)\n",
    "    # fourth fully connected layer with 50 neurons and tanh activation\n",
    "    l4 = tf.nn.tanh(fc_layer(l3, 2, 128))\n",
    "    tf.summary.histogram('outputs_l_4', l4)\n",
    "    # fifth fully connected layer with 50 neurons and tanh activation\n",
    "    l5 = tf.nn.tanh(fc_layer(l4, 128, 256))\n",
    "    tf.summary.histogram('outputs_l_5', l5)\n",
    "    # readout layer\n",
    "    if USE_RELU:\n",
    "        out = tf.nn.relu(fc_layer(l5, 50, 28*28))\n",
    "    else:\n",
    "        out = tf.sigmoid(fc_layer(l5, 256, 28*28))\n",
    "    tf.summary.histogram('outputs_l_6', out)\n",
    "    # let's use an l2 loss on the output image\n",
    "    loss = tf.reduce_mean(get_ll(x, out))\n",
    "    return loss, out, l3\n",
    "\n",
    "\"\"\"\n",
    "def layer_grid_summary(name, var, image_dims):\n",
    "    prod = np.prod(image_dims)\n",
    "    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n",
    "        GRID_COLS], image_dims, 1)\n",
    "    return tf.summary.image(name, grid)\n",
    "\"\"\"\n",
    "\n",
    "def create_summaries(loss, x, latent, output):\n",
    "    writer = tf.summary.FileWriter(\"./logs\")\n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    #layer_grid_summary(\"Input\", x, [28, 28])\n",
    "    #layer_grid_summary(\"Encoder\", latent, [2, 1])\n",
    "    #layer_grid_summary(\"Output\", output, [28, 28])\n",
    "    return writer, tf.summary.merge_all()\n",
    "\n",
    "\"\"\"\n",
    "def make_image(name, var, image_dims):\n",
    "    prod = np.prod(image_dims)\n",
    "    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n",
    "        GRID_COLS], image_dims, 1)\n",
    "    s_grid = tf.squeeze(grid, axis=0)\n",
    "\n",
    "    # This reproduces the code in: tensorflow/core/kernels/summary_image_op.cc\n",
    "    im_min = tf.reduce_min(s_grid)\n",
    "    im_max = tf.reduce_max(s_grid)\n",
    "\n",
    "    kZeroThreshold = tf.constant(1e-6)\n",
    "    max_val = tf.maximum(tf.abs(im_min), tf.abs(im_max))\n",
    "\n",
    "    offset = tf.cond(\n",
    "            im_min < tf.constant(0.0),\n",
    "            lambda: tf.constant(128.0),\n",
    "            lambda: tf.constant(0.0)\n",
    "            )\n",
    "    scale = tf.cond(\n",
    "            im_min < tf.constant(0.0),\n",
    "            lambda: tf.cond(\n",
    "                max_val < kZeroThreshold,\n",
    "                lambda: tf.constant(0.0),\n",
    "                lambda: tf.div(127.0, max_val)\n",
    "                ),\n",
    "            lambda: tf.cond(\n",
    "                im_max < kZeroThreshold,\n",
    "                lambda: tf.constant(0.0),\n",
    "                lambda: tf.div(255.0, im_max)\n",
    "                )\n",
    "            )\n",
    "    s_grid = tf.cast(tf.add(tf.multiply(s_grid, scale), offset), tf.uint8)\n",
    "    enc = tf.image.encode_jpeg(s_grid)\n",
    "\n",
    "    fwrite = tf.write_file(name, enc)\n",
    "    return fwrite\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # initialize the data\n",
    "    mnist = input_data.read_data_sets('/tmp/MNIST_data')\n",
    "\n",
    "    # placeholders for the images\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "    # build the model\n",
    "    loss, output, latent = autoencoder(x)\n",
    "\n",
    "    # and we use the Adam Optimizer for training\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    train_step = optimizer.minimize(-loss)\n",
    "\n",
    "    # We want to use Tensorboard to visualize some stuff\n",
    "    writer, summary_op = create_summaries(loss, x, latent, output)\n",
    "\n",
    "    first_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "    # Run the training loop\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(make_image(\"images/input.jpg\", x, [28, 28]), feed_dict={x : \n",
    "            #first_batch[0]})\n",
    "        for i in range(int(200001)):\n",
    "            batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "            feed = {x : batch[0]}\n",
    "            if i % 500 == 0:\n",
    "                summary, train_loss = sess.run([summary_op, loss], \n",
    "                        feed_dict=feed)\n",
    "                print(\"step %d, training loss: %g\" % (i, train_loss))\n",
    "\n",
    "                writer.add_summary(summary, i)\n",
    "                writer.flush()\n",
    "\n",
    "            #if i % 1000 == 0:\n",
    "                #sess.run(make_image(\"images/output_%06i.jpg\" % i, output, [28, \n",
    "                #    28]), feed_dict={x : first_batch[0]})\n",
    "\n",
    "            train_step.run(feed_dict=feed)\n",
    "\n",
    "        # Save latent space\n",
    "        #pred = sess.run(latent, feed_dict={x : mnist.test._images})\n",
    "        #pred = np.asarray(pred)\n",
    "        #pred = np.reshape(pred, (mnist.test._num_examples, 2))\n",
    "        #labels = np.reshape(mnist.test._labels, (mnist.test._num_examples, 1))\n",
    "        #pred = np.hstack((pred, labels))\n",
    "        if USE_RELU:\n",
    "            fname = \"latent_relu.csv\"\n",
    "        else:\n",
    "            fname = \"latent_default.csv\"\n",
    "        #np.savetxt(fname, pred)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step 0, training loss: -577.485\n",
    "step 500, training loss: -202.786\n",
    "step 1000, training loss: -207.21\n",
    "step 1500, training loss: -202.004\n",
    "step 2000, training loss: -187.898\n",
    "step 2500, training loss: -186.689\n",
    "step 3000, training loss: -175.804\n",
    "step 3500, training loss: -172.61\n",
    "step 4000, training loss: -161.926\n",
    "step 4500, training loss: -170.936\n",
    "step 5000, training loss: -158.872\n",
    "step 5500, training loss: -164.085\n",
    "step 6000, training loss: -151.848\n",
    "step 6500, training loss: -159.407\n",
    "step 7000, training loss: -159.056\n",
    "step 7500, training loss: -157.305\n",
    "step 8000, training loss: -158.951\n",
    "step 8500, training loss: -161.98\n",
    "step 9000, training loss: -146.214\n",
    "step 9500, training loss: -154.979\n",
    "step 10000, training loss: -145.297\n",
    "step 10500, training loss: -143.071\n",
    "step 11000, training loss: -150.968\n",
    "step 11500, training loss: -157.205\n",
    "step 12000, training loss: -146.146\n",
    "step 12500, training loss: -144.243\n",
    "step 13000, training loss: -144.019\n",
    "step 13500, training loss: -163.697\n",
    "step 14000, training loss: -150.674\n",
    "step 14500, training loss: -152.286\n",
    "step 15000, training loss: -146.486\n",
    "step 15500, training loss: -157.113\n",
    "step 16000, training loss: -140.504\n",
    "step 16500, training loss: -136.147\n",
    "step 17000, training loss: -140.873\n",
    "step 17500, training loss: -144.909\n",
    "step 18000, training loss: -131.302\n",
    "step 18500, training loss: -147.198\n",
    "step 19000, training loss: -147.718\n",
    "step 19500, training loss: -145.261\n",
    "step 20000, training loss: -139.972\n",
    "step 20500, training loss: -140.979\n",
    "step 21000, training loss: -147.93\n",
    "step 21500, training loss: -140.522\n",
    "step 22000, training loss: -155.479\n",
    "step 22500, training loss: -147.228\n",
    "step 23000, training loss: -139.919\n",
    "step 23500, training loss: -132.262\n",
    "step 24000, training loss: -142.463\n",
    "step 24500, training loss: -130.251\n",
    "step 25000, training loss: -148.649\n",
    "step 25500, training loss: -141.536\n",
    "step 26000, training loss: -140.558\n",
    "step 26500, training loss: -137.23\n",
    "step 27000, training loss: -144.847\n",
    "step 27500, training loss: -135.864\n",
    "step 28000, training loss: -137.215\n",
    "step 28500, training loss: -128.116\n",
    "step 29000, training loss: -139.246\n",
    "step 29500, training loss: -131.523\n",
    "step 30000, training loss: -132.965\n",
    "step 30500, training loss: -137.978\n",
    "step 31000, training loss: -141.618\n",
    "step 31500, training loss: -140.294\n",
    "step 32000, training loss: -132.861\n",
    "step 32500, training loss: -130.252\n",
    "step 33000, training loss: -138.328\n",
    "step 33500, training loss: -142.932\n",
    "step 34000, training loss: -137.06\n",
    "step 34500, training loss: -134.373\n",
    "step 35000, training loss: -131.89\n",
    "step 35500, training loss: -132.643\n",
    "step 36000, training loss: -141.806\n",
    "step 36500, training loss: -147.092\n",
    "step 37000, training loss: -129.737\n",
    "step 37500, training loss: -138.656\n",
    "step 38000, training loss: -134.118\n",
    "step 38500, training loss: -139.848\n",
    "step 39000, training loss: -132.592\n",
    "step 39500, training loss: -125.842\n",
    "step 40000, training loss: -138.065\n",
    "step 40500, training loss: -143.349\n",
    "step 41000, training loss: -141.07\n",
    "step 41500, training loss: -137.043\n",
    "step 42000, training loss: -126.963\n",
    "step 42500, training loss: -136.094\n",
    "step 43000, training loss: -141.677\n",
    "step 43500, training loss: -129.368\n",
    "step 44000, training loss: -131.486\n",
    "step 44500, training loss: -134.046\n",
    "step 45000, training loss: -135.94\n",
    "step 45500, training loss: -131.42\n",
    "step 46000, training loss: -139.378\n",
    "step 46500, training loss: -133.611\n",
    "step 47000, training loss: -137.304\n",
    "step 47500, training loss: -142.816\n",
    "step 48000, training loss: -127.18\n",
    "step 48500, training loss: -129.672\n",
    "step 49000, training loss: -146.451\n",
    "step 49500, training loss: -138.04\n",
    "step 50000, training loss: -132.081\n",
    "step 50500, training loss: -130.092\n",
    "step 51000, training loss: -144.464\n",
    "step 51500, training loss: -139.677\n",
    "step 52000, training loss: -134.809\n",
    "step 52500, training loss: -139.341\n",
    "step 53000, training loss: -131.66\n",
    "step 53500, training loss: -136.415\n",
    "step 54000, training loss: -130.769\n",
    "step 54500, training loss: -133.07\n",
    "step 55000, training loss: -125.89\n",
    "step 55500, training loss: -143.225\n",
    "step 56000, training loss: -131.217\n",
    "step 56500, training loss: -143.6\n",
    "step 57000, training loss: -130.617\n",
    "step 57500, training loss: -143.195\n",
    "step 58000, training loss: -135.302\n",
    "step 58500, training loss: -141.117\n",
    "step 59000, training loss: -137.994\n",
    "step 59500, training loss: -123.757\n",
    "step 60000, training loss: -134.097\n",
    "step 60500, training loss: -134.512\n",
    "step 61000, training loss: -137.98\n",
    "step 61500, training loss: -137.607\n",
    "step 62000, training loss: -124.163\n",
    "step 62500, training loss: -136.144\n",
    "step 63000, training loss: -122.271\n",
    "step 63500, training loss: -129.956\n",
    "step 64000, training loss: -122.583\n",
    "step 64500, training loss: -131.837\n",
    "step 65000, training loss: -145.533\n",
    "step 65500, training loss: -134.17\n",
    "step 66000, training loss: -131.014\n",
    "step 66500, training loss: -128.104\n",
    "step 67000, training loss: -141.223\n",
    "step 67500, training loss: -120.874\n",
    "step 68000, training loss: -142.412\n",
    "step 68500, training loss: -128.993\n",
    "step 69000, training loss: -130.299\n",
    "step 69500, training loss: -120.395\n",
    "step 70000, training loss: -130.241\n",
    "step 70500, training loss: -128.016\n",
    "step 71000, training loss: -124.969\n",
    "step 71500, training loss: -129.618\n",
    "step 72000, training loss: -131.069\n",
    "step 72500, training loss: -140.832\n",
    "step 73000, training loss: -128.5\n",
    "step 73500, training loss: -135.724\n",
    "step 74000, training loss: -131.912\n",
    "step 74500, training loss: -137.808\n",
    "step 75000, training loss: -122.381\n",
    "step 75500, training loss: -134.815\n",
    "step 76000, training loss: -129.52\n",
    "step 76500, training loss: -137.294\n",
    "step 77000, training loss: -129.343\n",
    "step 77500, training loss: -137.771\n",
    "step 78000, training loss: -131.339\n",
    "step 78500, training loss: -126.507\n",
    "step 79000, training loss: -129.441\n",
    "step 79500, training loss: -136.201\n",
    "step 80000, training loss: -127.229\n",
    "step 80500, training loss: -129.502\n",
    "step 81000, training loss: -126.975\n",
    "step 81500, training loss: -130.243\n",
    "step 82000, training loss: -124.352\n",
    "step 82500, training loss: -133.099\n",
    "step 83000, training loss: -135\n",
    "step 83500, training loss: -128.062\n",
    "step 84000, training loss: -134.459\n",
    "step 84500, training loss: -124.606\n",
    "step 85000, training loss: -125.123\n",
    "step 85500, training loss: -123.388\n",
    "step 86000, training loss: -133.21\n",
    "step 86500, training loss: -129.994\n",
    "step 87000, training loss: -126.751\n",
    "step 87500, training loss: -128.521\n",
    "step 88000, training loss: -129.421\n",
    "step 88500, training loss: -134.652\n",
    "step 89000, training loss: -118.857\n",
    "step 89500, training loss: -127.451\n",
    "step 90000, training loss: -135.981\n",
    "step 90500, training loss: -128.619\n",
    "step 91000, training loss: -127.653\n",
    "step 91500, training loss: -131.686\n",
    "step 92000, training loss: -132.896"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
