{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalInference(object):\n",
    "    def __init__(self, n_datapoints, neurons_per_layer, mc_samples, batch_size, constant_prior=False):\n",
    "        # SIZES\n",
    "        self.N = n_datapoints\n",
    "        self.layers = len(neurons_per_layer)\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.M = batch_size\n",
    "        ## Set the number of Monte Carlo samples as a placeholder so that it can be different for training and test\n",
    "        # self.L =  tf.placeholder(tf.int32)\n",
    "        self.L = mc_samples\n",
    "        \n",
    "        self.constant_prior = constant_prior\n",
    "        \n",
    "        ## Batch data placeholders\n",
    "        with tf.name_scope('input'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[0]], name='x-input')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[-1]], name='y-input')\n",
    "            \n",
    "        with tf.name_scope('input_reshape'):\n",
    "            image_shaped_input = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            tf.summary.image('input', image_shaped_input, 10)\n",
    "        \n",
    "        # PRIOR OF WEIGHTS\n",
    "        self.prior_mean_W, self.log_prior_var_W = self.get_prior_W()\n",
    "    \n",
    "        # POSTERIOR OF WEIGHTS\n",
    "        self.mean_W, self.log_var_W = self.init_posterior_W()\n",
    "        ## Builds whole computational graph with relevant quantities as part of the class\n",
    "        self.loss, self.kl, self.ell = self.get_nelbo()\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.Session()\n",
    "    \n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def get_prior_W(self):\n",
    "        \"\"\"\n",
    "        Define a prior for the weight distribution.\n",
    "        We assume them to be standard normal iid.\n",
    "        \"\"\"\n",
    "        prior_mean_W = []\n",
    "        log_prior_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            \n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_prior_weights\"):\n",
    "                if self.constant_prior:\n",
    "                    prior_mean = tf.constant(0.0, shape=[d_in, d_out])\n",
    "                    log_prior_var = tf.constant(0.0, shape=[d_in, d_out])\n",
    "                else:\n",
    "                    prior_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                    log_prior_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                    \n",
    "                tf.summary.histogram('prior_mean', tf.reshape(prior_mean, [-1]))\n",
    "                tf.summary.histogram('prior_logvar', tf.reshape(log_prior_var, [-1]))\n",
    "            \n",
    "            prior_mean_W.append(prior_mean)\n",
    "            log_prior_var_W.append(log_prior_var)\n",
    "        \n",
    "        return prior_mean_W, log_prior_var_W\n",
    "\n",
    "    def init_posterior_W(self):\n",
    "        \"\"\"\n",
    "        The (variational) posterior is assumed to be\n",
    "        drawn from P mutually independent normal distributions.\n",
    "        Hence, we have a diagonal covariance matrix and only need to store an array.\n",
    "        \"\"\"\n",
    "        mean_W = []\n",
    "        log_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "\n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_posterior_weights\"):\n",
    "                post_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                post_log_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                tf.summary.histogram('posterior_mean', tf.reshape(post_mean, [-1]))\n",
    "                tf.summary.histogram('posterior_logvar', tf.reshape(post_log_var, [-1]))\n",
    "            \n",
    "            mean_W.append(post_mean)\n",
    "            log_var_W.append(post_log_var)\n",
    "            \n",
    "        return mean_W, log_var_W\n",
    "    \n",
    "    def get_std_norm_samples(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        Draws N(0,1) samples of dimension [d_in, d_out].\n",
    "        \"\"\"\n",
    "        return tf.random_normal(shape=[d_in, d_out])\n",
    "\n",
    "    def sample_from_W(self):\n",
    "        \"\"\"\n",
    "        Samples from the variational posterior approximation.\n",
    "        We draw W-samples for each layer using the reparameterization trick.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            z = self.get_std_norm_samples(d_in, d_out)\n",
    "            ## division by 2 to obtain pure standard deviation\n",
    "            w_from_q = tf.add(tf.multiply(z, tf.exp(self.log_var_W[i] / 2)), self.mean_W[i])\n",
    "        \n",
    "            yield w_from_q\n",
    "    \n",
    "    def feedforward(self, intermediate=0):\n",
    "        \"\"\"\n",
    "        Feedforward pass excluding last layer's transfer function.\n",
    "        intermediate : index of intermediate layer for output generation\n",
    "        \"\"\"\n",
    "        \n",
    "        # We will generate L output samples\n",
    "        for i in range(self.L):\n",
    "            \n",
    "            inputs = self.X\n",
    "            \n",
    "            # Go through each layer (one weight matrix at a time)\n",
    "            # and compute the (intermediate) output\n",
    "            j = 0\n",
    "            for weight_matrix in self.sample_from_W():\n",
    "                activations = tf.matmul(inputs, weight_matrix[1:,:]) + weight_matrix[0,:]\n",
    "                tf.summary.histogram('activations', activations)\n",
    "\n",
    "                # if last layer is reached, do not use transfer function (softmax later on)\n",
    "                if j == (self.layers - 2):\n",
    "                    outputs = tf.sigmoid(activations)\n",
    "                else:\n",
    "                    outputs = tf.nn.tanh(activations)\n",
    "\n",
    "                #outputs = tf.sigmoid(activations)\n",
    "                tf.summary.histogram('outputs', outputs)\n",
    "\n",
    "                inputs = outputs\n",
    "                j += 1\n",
    "                \n",
    "                if j == intermediate:\n",
    "                    break\n",
    "                \n",
    "            # use generator to save memory space\n",
    "            yield outputs\n",
    "    \n",
    "    def predict(self, intermediate=0):\n",
    "        \"\"\"\n",
    "        Predict using monte carlo sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_output = 0\n",
    "        \n",
    "        for output in self.feedforward(intermediate):\n",
    "            expected_output += output\n",
    "            \n",
    "        return expected_output / self.L\n",
    "    \n",
    "    def get_ell(self):\n",
    "        \"\"\"\n",
    "        Returns the expected log-likelihood of the lower bound.\n",
    "        For this we draw L samples from W, compute the log-likelihood for each\n",
    "        and average the log-likelihoods in the end (expectation approximation).\n",
    "        \"\"\"\n",
    "        \n",
    "        log_p = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            # y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]) + b)\n",
    "            # log_p_per_sample = tf.reduce_mean(tf.reduce_sum(self.Y * tf.log(y), reduction_indices=[1]))\n",
    "            # soft_max_cross_entropy_with_logits is a numerically stable version of cross entropy\n",
    "            # log_p_per_sample = tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.Y))\n",
    "            \n",
    "            log_p_per_sample = tf.reduce_mean(tf.reduce_sum(\n",
    "                                    self.Y * tf.log(output + 1e-10) + (1 - self.Y) * tf.log(1 - output + 1e-10),\n",
    "                                    reduction_indices=[1]))\n",
    "            log_p += log_p_per_sample\n",
    "        \n",
    "        return log_p / self.L\n",
    "\n",
    "    def get_kl(self, mean_W, log_var_W, prior_mean_W, log_prior_var_W):\n",
    "        \"\"\"\n",
    "        KL[q || p] returns the KL-divergence between the prior p and the variational posterior q.\n",
    "        :param mq: vector of means for q\n",
    "        :param log_vq: vector of log-variances for q\n",
    "        :param mp: vector of means for p\n",
    "        :param log_vp: vector of log-variances for p\n",
    "        :return: KL divergence between q and p\n",
    "        \"\"\"\n",
    "        mq = mean_W\n",
    "        log_vq = log_var_W\n",
    "        mp = prior_mean_W\n",
    "        log_vp = log_prior_var_W\n",
    "        \n",
    "        #log_vp = tf.reshape(log_vp, (-1, 1))\n",
    "        return 0.5 * tf.reduce_sum(log_vp - log_vq + (tf.pow(mq - mp, 2) / tf.exp(log_vp)) + tf.exp(log_vq - log_vp) - 1)\n",
    "\n",
    "    def get_kl_multi(self):\n",
    "        \"\"\"\n",
    "        Compute KL divergence between variational and prior using a multi-layer-network\n",
    "        \"\"\"\n",
    "        kl = 0\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            kl = kl + self.get_kl(\n",
    "                        self.mean_W[i],\n",
    "                        self.log_var_W[i],\n",
    "                        self.prior_mean_W[i],\n",
    "                        self.log_prior_var_W[i]\n",
    "            )\n",
    "        \n",
    "        return kl\n",
    "    \n",
    "    def get_nelbo(self):\n",
    "        \"\"\" Returns the negative ELBOW, which allows us to minimize instead of maximize. \"\"\"\n",
    "        kl = self.get_kl_multi()\n",
    "        # ell, layer_out = self.get_ell()\n",
    "        ell = self.get_ell()\n",
    "        # DKL_gaussian - tf.mean([log_likelihood(w) for w in w_from_q])\n",
    "        nelbo = kl - self.N/tf.cast(self.M, \"float32\") * ell\n",
    "        # return nelbo, kl, ell, layer_out\n",
    "        return nelbo, kl, ell\n",
    "    \n",
    "    def learn(self, learning_rate=0.01, epochs=50):\n",
    "        \"\"\" Our learning procedure \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        tf.summary.scalar('negative_elbo', self.loss)\n",
    "        tf.summary.scalar('kl_div', self.kl)\n",
    "        tf.summary.scalar('ell', self.ell)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter('logs/train', self.session.graph)\n",
    "        test_writer = tf.summary.FileWriter('logs/test')        \n",
    "        \n",
    "        ## Initialize all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        ## Initialize TF session\n",
    "        self.session.run(init)\n",
    "        \n",
    "\n",
    "        for i in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print(\"Epoch: \", i)\n",
    "            train_cost = 0\n",
    "            \n",
    "            old_progress = 0\n",
    "            for batch_i in range(mnist.train.num_examples // self.M):\n",
    "                progress = round(float(batch_i) / (mnist.train.num_examples // self.M) * 100)\n",
    "                if progress % 10 == 0 and progress != old_progress:\n",
    "                    print('Progress: ', str(progress) + '%')\n",
    "                    old_progress = progress\n",
    "                batch_xs, _ = mnist.train.next_batch(self.M)\n",
    "\n",
    "                _, loss, summary = self.session.run([train_step, self.loss, merged], feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "                train_writer.add_summary(summary, i)\n",
    "                train_cost += loss\n",
    "            \n",
    "            #summary, nelbo = self.session.run([merged, self.get_nelbo()],\n",
    "            #                                  feed_dict={self.X: mnist.test.images, self.Y: mnist.test.images})\n",
    "            #print(\"i=\" + repr(i)  + \"  kl=\" + repr(nelbo[1]) + \"  nell=\" + repr(-nelbo[2])  + \"  nelbo=\" + repr(nelbo[0]), end=\"\\n\")\n",
    "            print(\"NELBO: \", train_cost / (mnist.train.num_examples // batch_size))\n",
    "            #test_writer.add_summary(summary, i)\n",
    "            print('Epoch training time: ', time.time() - start_time)\n",
    "        \n",
    "        \n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "    def benchmark(self, validation=False):\n",
    "        if validation:\n",
    "            benchmark_data = mnist.validation\n",
    "            label = 'Validation loss:'\n",
    "        else:\n",
    "            benchmark_data = mnist.test\n",
    "            label = 'Test loss:'\n",
    "        \n",
    "        cost = 0\n",
    "        for batch_i in range(benchmark_data.num_examples // self.M):\n",
    "            batch_xs, _ = benchmark_data.next_batch(batch_size)\n",
    "            cost += self.session.run(self.loss,\n",
    "                                   feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "        print(label, cost /\n",
    "              (benchmark_data.num_examples // self.M))\n",
    "        \n",
    "    def serialize(self, path):\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.session, path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    def restore(self, path):\n",
    "        saver = tf.train.Saver()   \n",
    "        sess = tf.Session()\n",
    "        saver.restore(sess, save_path=path)\n",
    "        self.session = sess\n",
    "    \n",
    "    def encode(self, input_vector, intermediate=0):\n",
    "        input_vector = np.reshape(input_vector, [1, -1])\n",
    "        output = self.predict(intermediate)\n",
    "        return self.session.run(output, feed_dict={self.X: input_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = mnist.train.num_examples\n",
    "# including input neurons\n",
    "mc_samples = 10\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  608710.294216\n",
      "Epoch training time:  58.778414726257324\n",
      "Epoch:  1\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  284779.657925\n",
      "Epoch training time:  58.72993087768555\n",
      "Epoch:  2\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  211011.16368\n",
      "Epoch training time:  59.77456974983215\n",
      "Epoch:  3\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  169700.964307\n",
      "Epoch training time:  59.38583278656006\n",
      "Epoch:  4\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  144677.598904\n",
      "Epoch training time:  60.14256954193115\n",
      "Epoch:  5\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  128670.971263\n",
      "Epoch training time:  60.33465528488159\n",
      "Epoch:  6\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  118144.654265\n",
      "Epoch training time:  59.3399863243103\n",
      "Epoch:  7\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  110836.693382\n",
      "Epoch training time:  61.95111083984375\n",
      "Epoch:  8\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  105782.157288\n",
      "Epoch training time:  65.37501096725464\n",
      "Epoch:  9\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  102084.528646\n",
      "Epoch training time:  65.08343553543091\n",
      "Epoch:  10\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  99351.5425226\n",
      "Epoch training time:  65.08508563041687\n",
      "Epoch:  11\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  97304.7950357\n",
      "Epoch training time:  65.07253527641296\n",
      "Epoch:  12\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  95732.9868335\n",
      "Epoch training time:  64.5537497997284\n",
      "Epoch:  13\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  94668.0786167\n",
      "Epoch training time:  64.2313244342804\n",
      "Epoch:  14\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  93644.1753351\n",
      "Epoch training time:  65.8041250705719\n",
      "Epoch:  15\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  92923.6442854\n",
      "Epoch training time:  65.1499285697937\n",
      "Epoch:  16\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  92309.3645105\n",
      "Epoch training time:  62.744651317596436\n",
      "Epoch:  17\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  91862.9337303\n",
      "Epoch training time:  63.03101873397827\n",
      "Epoch:  18\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  91552.3242461\n",
      "Epoch training time:  64.15030431747437\n",
      "Epoch:  19\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  91261.0277717\n",
      "Epoch training time:  60.581252098083496\n",
      "Epoch:  20\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  89777.7897545\n",
      "Epoch training time:  59.335400342941284\n",
      "Epoch:  21\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  86137.988436\n",
      "Epoch training time:  60.964839220047\n",
      "Epoch:  22\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  83193.5611524\n",
      "Epoch training time:  58.23246192932129\n",
      "Epoch:  23\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  81628.2371977\n",
      "Epoch training time:  58.396668434143066\n",
      "Epoch:  24\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  81317.1165501\n",
      "Epoch training time:  58.164923429489136\n",
      "Epoch:  25\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  81102.3740348\n",
      "Epoch training time:  58.03889274597168\n",
      "Epoch:  26\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  80944.2687391\n",
      "Epoch training time:  62.045814990997314\n",
      "Epoch:  27\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  80693.1857517\n",
      "Epoch training time:  57.605454206466675\n",
      "Epoch:  28\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  80629.4368808\n",
      "Epoch training time:  58.99302315711975\n",
      "Epoch:  29\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  80338.4999272\n",
      "Epoch training time:  61.43138599395752\n",
      "Epoch:  30\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  80223.7195513\n",
      "Epoch training time:  59.34238958358765\n",
      "Epoch:  31\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  79994.4118954\n",
      "Epoch training time:  61.1317572593689\n",
      "Epoch:  32\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  79863.99603\n",
      "Epoch training time:  60.06620693206787\n",
      "Epoch:  33\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  79551.4356425\n",
      "Epoch training time:  57.977070331573486\n",
      "Epoch:  34\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  78638.8364474\n",
      "Epoch training time:  58.932310819625854\n",
      "Epoch:  35\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  76465.462449\n",
      "Epoch training time:  61.31573820114136\n",
      "Epoch:  36\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  74031.7609266\n",
      "Epoch training time:  61.52370285987854\n",
      "Epoch:  37\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  72027.1007066\n",
      "Epoch training time:  59.63910508155823\n",
      "Epoch:  38\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  70845.5070112\n",
      "Epoch training time:  59.63853335380554\n",
      "Epoch:  39\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  69807.2581039\n",
      "Epoch training time:  59.777077436447144\n",
      "Epoch:  40\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  69073.0891791\n",
      "Epoch training time:  60.464897871017456\n",
      "Epoch:  41\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  68439.8009269\n",
      "Epoch training time:  65.86509609222412\n",
      "Epoch:  42\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  67908.4746321\n",
      "Epoch training time:  61.435950756073\n",
      "Epoch:  43\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  67432.8127276\n",
      "Epoch training time:  59.68720030784607\n",
      "Epoch:  44\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  67159.4670746\n",
      "Epoch training time:  59.26962733268738\n",
      "Epoch:  45\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  66902.5600142\n",
      "Epoch training time:  61.509443521499634\n",
      "Epoch:  46\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  66599.1649548\n",
      "Epoch training time:  58.961440086364746\n",
      "Epoch:  47\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  66364.4495648\n",
      "Epoch training time:  60.05184078216553\n",
      "Epoch:  48\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  66175.9247705\n",
      "Epoch training time:  63.88962364196777\n",
      "Epoch:  49\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  66059.1409164\n",
      "Epoch training time:  66.4256763458252\n",
      "Epoch:  50\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  66034.9846391\n",
      "Epoch training time:  65.21582365036011\n",
      "Epoch:  51\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65810.210983\n",
      "Epoch training time:  60.63674998283386\n",
      "Epoch:  52\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65625.2798387\n",
      "Epoch training time:  60.70595192909241\n",
      "Epoch:  53\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65770.4885089\n",
      "Epoch training time:  59.955169439315796\n",
      "Epoch:  54\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65529.6080638\n",
      "Epoch training time:  59.370609283447266\n",
      "Epoch:  55\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65247.6311098\n",
      "Epoch training time:  61.15727400779724\n",
      "Epoch:  56\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65331.9149548\n",
      "Epoch training time:  62.07829141616821\n",
      "Epoch:  57\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65272.0161895\n",
      "Epoch training time:  59.52124905586243\n",
      "Epoch:  58\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65291.8515261\n",
      "Epoch training time:  59.57087540626526\n",
      "Epoch:  59\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65115.5915738\n",
      "Epoch training time:  60.25659966468811\n",
      "Epoch:  60\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65072.3812737\n",
      "Epoch training time:  59.34602499008179\n",
      "Epoch:  61\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65040.9842384\n",
      "Epoch training time:  62.055994510650635\n",
      "Epoch:  62\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64939.5226635\n",
      "Epoch training time:  65.16851878166199\n",
      "Epoch:  63\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  65016.6294162\n",
      "Epoch training time:  61.84098553657532\n",
      "Epoch:  64\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64883.5520651\n",
      "Epoch training time:  60.36287498474121\n",
      "Epoch:  65\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64818.1835573\n",
      "Epoch training time:  60.35001182556152\n",
      "Epoch:  66\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64608.0757667\n",
      "Epoch training time:  60.93134164810181\n",
      "Epoch:  67\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64734.8566525\n",
      "Epoch training time:  67.46268343925476\n",
      "Epoch:  68\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64655.5954709\n",
      "Epoch training time:  64.52265763282776\n",
      "Epoch:  69\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64481.7283472\n",
      "Epoch training time:  65.2617998123169\n",
      "Epoch:  70\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64655.6991914\n",
      "Epoch training time:  65.5811984539032\n",
      "Epoch:  71\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64501.5508632\n",
      "Epoch training time:  65.45004057884216\n",
      "Epoch:  72\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64470.9382375\n",
      "Epoch training time:  64.79546284675598\n",
      "Epoch:  73\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64520.6635071\n",
      "Epoch training time:  65.17252564430237\n",
      "Epoch:  74\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64433.0464561\n",
      "Epoch training time:  59.99990463256836\n",
      "Epoch:  75\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64414.8472556\n",
      "Epoch training time:  59.821667432785034\n",
      "Epoch:  76\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NELBO:  64445.2479422\n",
      "Epoch training time:  59.99286389350891\n",
      "Epoch:  77\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64424.8704382\n",
      "Epoch training time:  59.74034810066223\n",
      "Epoch:  78\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64313.9651169\n",
      "Epoch training time:  59.72189164161682\n",
      "Epoch:  79\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64270.451222\n",
      "Epoch training time:  60.370147943496704\n",
      "Epoch:  80\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64264.5148419\n",
      "Epoch training time:  61.75212836265564\n",
      "Epoch:  81\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64252.0942872\n",
      "Epoch training time:  59.483580589294434\n",
      "Epoch:  82\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64238.2284382\n",
      "Epoch training time:  64.60980033874512\n",
      "Epoch:  83\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64304.3028755\n",
      "Epoch training time:  63.91695261001587\n",
      "Epoch:  84\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64424.461238\n",
      "Epoch training time:  60.14407658576965\n",
      "Epoch:  85\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64251.0858464\n",
      "Epoch training time:  64.89230585098267\n",
      "Epoch:  86\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64319.0767956\n",
      "Epoch training time:  61.89274787902832\n",
      "Epoch:  87\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64284.8104513\n",
      "Epoch training time:  59.532610177993774\n",
      "Epoch:  88\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64155.5010289\n",
      "Epoch training time:  61.844603300094604\n",
      "Epoch:  89\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64306.4007958\n",
      "Epoch training time:  60.043399810791016\n",
      "Epoch:  90\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64122.7676828\n",
      "Epoch training time:  59.79843330383301\n",
      "Epoch:  91\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64061.2618189\n",
      "Epoch training time:  59.96751070022583\n",
      "Epoch:  92\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  64173.3150131\n",
      "Epoch training time:  59.851507902145386\n",
      "Epoch:  93\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63887.4178413\n",
      "Epoch training time:  60.780386209487915\n",
      "Epoch:  94\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63954.4410967\n",
      "Epoch training time:  59.56408500671387\n",
      "Epoch:  95\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63774.6815541\n",
      "Epoch training time:  59.70426630973816\n",
      "Epoch:  96\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63884.6540283\n",
      "Epoch training time:  59.64030981063843\n",
      "Epoch:  97\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63738.0378242\n",
      "Epoch training time:  60.18047881126404\n",
      "Epoch:  98\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63593.1984084\n",
      "Epoch training time:  60.30319666862488\n",
      "Epoch:  99\n",
      "Progress:  10\n",
      "Progress:  20\n",
      "Progress:  30\n",
      "Progress:  40\n",
      "Progress:  50\n",
      "Progress:  60\n",
      "Progress:  70\n",
      "Progress:  80\n",
      "Progress:  90\n",
      "Progress:  100\n",
      "NELBO:  63756.0587668\n",
      "Epoch training time:  62.027451515197754\n",
      "Test loss: 63678.9911358\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 128, 64, 2, 64, 128, 784]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size, constant_prior=False)\n",
    "vi.learn(learning_rate=0.01, epochs=epochs)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 128, 64, 32, 64, 128, 784]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size, constant_prior=False)\n",
    "vi.learn(learning_rate=0.01, epochs=epochs)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: tmp/model_2_hidden.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 2_hidden: [784, 128, 64, 2, 64, 128, 784]\n",
    "# vi.serialize('tmp/model_2_hidden.ckpt')\n",
    "vi.serialize('tmp/model_32_hidden.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()     \n",
    "neurons_per_layer = [784, 512, 32, 512, 784]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size, constant_prior=False)\n",
    "vi.restore('tmp/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with [784, 128, 64, 2, 64, 128, 784] architecture and 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGSAhzgAAAAAAAAZ4CEOAAAAAABABv4fAwgQ2myJDEsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b4a41d978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(mnist.test.images[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(vi.encode(mnist.test.images[i]).reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
