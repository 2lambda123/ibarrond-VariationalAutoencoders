{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncuZIxfduAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5e6bb6eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[10].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalInference(object):\n",
    "    def __init__(self, n_datapoints, neurons_per_layer, mc_samples, batch_size):\n",
    "        # SIZES\n",
    "        self.N = n_datapoints\n",
    "        self.layers = len(neurons_per_layer)\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.M = batch_size\n",
    "        ## Set the number of Monte Carlo samples as a placeholder so that it can be different for training and test\n",
    "        # self.L =  tf.placeholder(tf.int32)\n",
    "        self.L = mc_samples\n",
    "        \n",
    "        ## Batch data placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, neurons_per_layer[0]])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, neurons_per_layer[-1]])\n",
    "        \n",
    "        # PRIOR OF WEIGHTS\n",
    "        self.prior_mean_W, self.log_prior_var_W = self.get_prior_W()\n",
    "    \n",
    "        # POSTERIOR OF WEIGHTS\n",
    "        self.mean_W, self.log_var_W = self.init_posterior_W()\n",
    "        ## Builds whole computational graph with relevant quantities as part of the class\n",
    "        # self.loss, self.kl, self.ell, self.layer_out = self.get_nelbo()\n",
    "        self.loss, self.kl, self.ell = self.get_nelbo()\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.Session()\n",
    "    \n",
    "\n",
    "    def get_prior_W(self):\n",
    "        \"\"\"\n",
    "        Define a prior for the weight distribution.\n",
    "        We assume them to be standard normal iid.\n",
    "        \"\"\"\n",
    "        prior_mean_W = []\n",
    "        log_prior_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            prior_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "            log_prior_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "            \n",
    "            prior_mean_W.append(prior_mean)\n",
    "            log_prior_var_W.append(log_prior_var)\n",
    "        \n",
    "        return prior_mean_W, log_prior_var_W\n",
    "\n",
    "    def init_posterior_W(self):\n",
    "        \"\"\"\n",
    "        The (variational) posterior is assumed to be\n",
    "        drawn from P mutually independent normal distributions.\n",
    "        Hence, we have a diagonal covariance matrix and only need to store an array.\n",
    "        \"\"\"\n",
    "        mean_W = []\n",
    "        log_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            post_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "            post_log_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "            \n",
    "            mean_W.append(post_mean)\n",
    "            log_var_W.append(post_log_var)\n",
    "            \n",
    "        return mean_W, log_var_W\n",
    "    \n",
    "    def get_samples(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        Draws L N(0,1) samples of dimension P.\n",
    "        \"\"\"\n",
    "        return tf.random_normal(shape=[self.L, d_in, d_out])\n",
    "\n",
    "    def sample_from_W(self):\n",
    "        \"\"\"\n",
    "        Samples from the variational posterior approximation.\n",
    "        We draw L w-samples of dimension P using the reparameterization trick.\n",
    "        \"\"\"\n",
    "        w_from_q = []\n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            z = self.get_samples(d_in, d_out)\n",
    "            ## division by 2 to obtain pure standard deviation\n",
    "            w_from_q.append(tf.add(tf.multiply(z, tf.exp(self.log_var_W[i] / 2)), self.mean_W[i]))\n",
    "        return w_from_q\n",
    "    \n",
    "    def feedforward(self):\n",
    "        \"\"\"\n",
    "        Feedforward pass excluding last layer's transfer function.\n",
    "        \"\"\"\n",
    "        \n",
    "        W_sample = self.sample_from_W()\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            inputs = self.X\n",
    "            \n",
    "            for j in range(self.layers - 1):\n",
    "                activations = tf.matmul(inputs, W_sample[j][i,1:,:]) + tf.reshape(W_sample[j][i,0,:], [1,-1])\n",
    "                # if last layer is reached, do not use transfer function (softmax later on)\n",
    "                if j == (self.layers - 2):\n",
    "                    outputs = activations\n",
    "                else:\n",
    "                    outputs = tf.sigmoid(activations)\n",
    "                inputs = outputs\n",
    "                \n",
    "            # use generator to save memory space\n",
    "            yield outputs\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict using monte carlo sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_output = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            expected_output += tf.nn.softmax(output)\n",
    "            \n",
    "        return expected_output / self.L\n",
    "    \n",
    "    def get_ell(self):\n",
    "        \"\"\"\n",
    "        Returns the expected log-likelihood of the lower bound.\n",
    "        For this we draw L samples from W, compute the log-likelihood for each\n",
    "        and average the log-likelihoods in the end (expectation approximation).\n",
    "        \"\"\"\n",
    "        \n",
    "        log_p = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            #y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]) + b)\n",
    "            # y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]))\n",
    "            # log_p_per_sample = tf.reduce_mean(tf.reduce_sum(self.Y * tf.log(y), reduction_indices=[1]))\n",
    "            # output = tf.matmul(self.X, W_sample[i,1:,:]) + tf.reshape(W_sample[i,0,:], [1,-1])\n",
    "            # soft_max_cross_entropy_with_logits is a numerically stable version of cross entropy\n",
    "            log_p_per_sample = tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.Y))\n",
    "            log_p += log_p_per_sample\n",
    "        \n",
    "        return log_p / self.L\n",
    "\n",
    "    def get_kl(self, mean_W, log_var_W, prior_mean_W, log_prior_var_W):\n",
    "        \"\"\"\n",
    "        KL[q || p] returns the KL-divergence between the prior p and the variational posterior q.\n",
    "        :param mq: vector of means for q\n",
    "        :param log_vq: vector of log-variances for q\n",
    "        :param mp: vector of means for p\n",
    "        :param log_vp: vector of log-variances for p\n",
    "        :return: KL divergence between q and p\n",
    "        \"\"\"\n",
    "        mq = mean_W\n",
    "        log_vq = log_var_W\n",
    "        mp = prior_mean_W\n",
    "        log_vp = log_prior_var_W\n",
    "        \n",
    "        #log_vp = tf.reshape(log_vp, (-1, 1))\n",
    "        return 0.5 * tf.reduce_sum(log_vp - log_vq + (tf.pow(mq - mp, 2) / tf.exp(log_vp)) + tf.exp(log_vq - log_vp) - 1)\n",
    "\n",
    "    def get_kl_multi(self):\n",
    "        \"\"\"\n",
    "        Compute KL divergence between variational and prior using a multi-layer-network\n",
    "        \"\"\"\n",
    "        kl = 0\n",
    "        for i in range(self.layers - 1):\n",
    "            kl = kl + self.get_kl(self.mean_W[i], self.log_var_W[i], self.prior_mean_W[i], self.log_prior_var_W[i])\n",
    "        return kl\n",
    "    \n",
    "    def get_nelbo(self):\n",
    "        \"\"\" Returns the negative ELBOW, which allows us to minimize instead of maximize. \"\"\"\n",
    "        kl = self.get_kl_multi()\n",
    "        # ell, layer_out = self.get_ell()\n",
    "        ell = self.get_ell()\n",
    "        # DKL_gaussian - tf.mean([log_likelihood(w) for w in w_from_q])\n",
    "        nelbo = kl - self.N/tf.cast(self.M, \"float32\") * ell\n",
    "        # return nelbo, kl, ell, layer_out\n",
    "        return nelbo, kl, ell\n",
    "    \n",
    "    def learn(self, learning_rate=0.01, iterations=5000):\n",
    "        \"\"\" Our learning procedure \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        ## Initialize all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        ## Initialize TF session\n",
    "        self.session.run(init)\n",
    "\n",
    "        ## Set the folder where the logs are going to be written \n",
    "        # summary_writer = tf.train.SummaryWriter('logs/', self.session.graph)\n",
    "        #summary_writer = tf.summary.FileWriter('logs/', self.session.graph)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(self.M)\n",
    "            self.session.run(train_step, feed_dict={self.X: batch_xs, self.Y: batch_ys})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                nelbo, kl, ell = self.session.run(self.get_nelbo(),\n",
    "                                feed_dict={self.X: mnist.train.images, self.Y: mnist.train.labels})\n",
    "                print(\"i=\" + repr(i)  + \"  kl=\" + repr(kl) + \"  nell=\" + repr(-ell)  + \"  nelbo=\" + repr(nelbo), end=\"\\n\")\n",
    "        \n",
    "        mean_W_final = self.session.run(self.mean_W)\n",
    "        log_var_W_final = self.session.run(self.log_var_W)\n",
    "        \n",
    "        return mean_W_final, log_var_W_final\n",
    "    \n",
    "    def benchmark(self):\n",
    "        output = self.predict()\n",
    "        correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(self.Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(self.session.run(accuracy, feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))\n",
    "        \n",
    "    def debug(self):\n",
    "        all_variables = tf.trainable_variables()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session.run(init)\n",
    "        print(self.session.run([self.prior_mean_W, self.log_prior_var_W], feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_datapoints, weight_dim, n_samples, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = mnist.train.num_examples\n",
    "# including input neurons\n",
    "mc_samples = 10\n",
    "batch_size = 100\n",
    "iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.3748349  nell=14.308055  nelbo=7869.8052\n",
      "i=100  kl=0.09215337  nell=3.5772705  nelbo=1967.5909\n",
      "i=200  kl=0.052568287  nell=2.6519072  nelbo=1458.6016\n",
      "i=300  kl=0.053241134  nell=1.8439789  nelbo=1014.2416\n",
      "i=400  kl=0.048801363  nell=1.7498658  nelbo=962.47498\n",
      "i=500  kl=0.039165616  nell=1.6181542  nelbo=890.02399\n",
      "i=600  kl=0.068335146  nell=1.3585074  nelbo=747.24744\n",
      "i=700  kl=0.057480007  nell=1.2546666  nelbo=690.12408\n",
      "i=800  kl=0.042308807  nell=1.3013623  nelbo=715.79156\n",
      "i=900  kl=0.059466034  nell=1.0635817  nelbo=585.02936\n",
      "i=1000  kl=0.060791343  nell=1.060887  nelbo=583.54865\n",
      "i=1100  kl=0.060575902  nell=1.0242478  nelbo=563.39679\n",
      "i=1200  kl=0.058959305  nell=0.98470193  nelbo=541.64502\n",
      "i=1300  kl=0.065800965  nell=0.89284247  nelbo=491.12915\n",
      "i=1400  kl=0.07269159  nell=0.9609068  nelbo=528.57141\n",
      "i=1500  kl=0.066827178  nell=0.81370479  nelbo=447.60446\n",
      "i=1600  kl=0.084425807  nell=0.85472906  nelbo=470.18539\n",
      "i=1700  kl=0.087394714  nell=0.78491414  nelbo=431.79019\n",
      "i=1800  kl=0.102873  nell=0.74216914  nelbo=408.2959\n",
      "i=1900  kl=0.095225602  nell=0.67870921  nelbo=373.38528\n",
      "i=2000  kl=0.14174157  nell=0.69695753  nelbo=383.46838\n",
      "i=2100  kl=0.11833084  nell=0.66184568  nelbo=364.13345\n",
      "i=2200  kl=0.12911016  nell=0.64445937  nelbo=354.58176\n",
      "i=2300  kl=0.15438527  nell=0.67713493  nelbo=372.57861\n",
      "i=2400  kl=0.1486612  nell=0.629565  nelbo=346.40939\n",
      "i=2500  kl=0.20062307  nell=0.58005053  nelbo=319.22842\n",
      "i=2600  kl=0.27069142  nell=0.56101477  nelbo=308.82883\n",
      "i=2700  kl=0.13609353  nell=0.53948414  nelbo=296.85239\n",
      "i=2800  kl=0.18770984  nell=0.60039085  nelbo=330.40268\n",
      "i=2900  kl=0.30911037  nell=0.5579859  nelbo=307.20135\n",
      "i=3000  kl=0.22007015  nell=0.51305687  nelbo=282.40134\n",
      "i=3100  kl=0.44663006  nell=0.50887316  nelbo=280.32687\n",
      "i=3200  kl=0.34342265  nell=0.5026558  nelbo=276.80411\n",
      "i=3300  kl=0.34429273  nell=0.49208412  nelbo=270.99057\n",
      "i=3400  kl=0.303339  nell=0.45991889  nelbo=253.25873\n",
      "i=3500  kl=0.29175818  nell=0.47710067  nelbo=262.69711\n",
      "i=3600  kl=0.31268314  nell=0.45842895  nelbo=252.44861\n",
      "i=3700  kl=0.46743736  nell=0.43769884  nelbo=241.2018\n",
      "i=3800  kl=0.62523687  nell=0.44395408  nelbo=244.79999\n",
      "i=3900  kl=0.48367402  nell=0.40466371  nelbo=223.04872\n",
      "i=4000  kl=0.62970567  nell=0.41204485  nelbo=227.25436\n",
      "i=4100  kl=0.60087764  nell=0.41443086  nelbo=228.53784\n",
      "i=4200  kl=0.46230778  nell=0.39262784  nelbo=216.40762\n",
      "i=4300  kl=0.53062117  nell=0.37434  nelbo=206.41762\n",
      "i=4400  kl=0.49295032  nell=0.40253043  nelbo=221.88469\n",
      "i=4500  kl=0.59350145  nell=0.37899715  nelbo=209.04193\n",
      "i=4600  kl=0.91865182  nell=0.36928976  nelbo=204.02802\n",
      "i=4700  kl=0.6831708  nell=0.39403799  nelbo=217.40407\n",
      "i=4800  kl=0.70240867  nell=0.37134132  nelbo=204.94014\n",
      "i=4900  kl=0.75760609  nell=0.35434157  nelbo=195.64546\n",
      "0.9175\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40948671  nell=3.7837021  nelbo=2081.4456\n",
      "i=100  kl=0.0097986162  nell=2.527931  nelbo=1390.3718\n",
      "i=200  kl=0.0025273561  nell=2.44379  nelbo=1344.087\n",
      "i=300  kl=0.004008472  nell=2.40974  nelbo=1325.361\n",
      "i=400  kl=0.0078063905  nell=2.2114987  nelbo=1216.3322\n",
      "i=500  kl=0.012652636  nell=2.0823584  nelbo=1145.3098\n",
      "i=600  kl=0.016389519  nell=2.0481839  nelbo=1126.5175\n",
      "i=700  kl=0.01134482  nell=2.0340207  nelbo=1118.7227\n",
      "i=800  kl=0.0098190308  nell=2.0190938  nelbo=1110.5114\n",
      "i=900  kl=0.012732029  nell=2.0130954  nelbo=1107.2152\n",
      "i=1000  kl=0.0087222755  nell=1.9915596  nelbo=1095.3665\n",
      "i=1100  kl=0.026929289  nell=1.9394716  nelbo=1066.7363\n",
      "i=1200  kl=0.034531385  nell=1.8631294  nelbo=1024.7557\n",
      "i=1300  kl=0.063946992  nell=1.7455273  nelbo=960.10394\n",
      "i=1400  kl=0.050398767  nell=1.6979859  nelbo=933.94263\n",
      "i=1500  kl=0.072067857  nell=1.6569531  nelbo=911.3963\n",
      "i=1600  kl=0.07861957  nell=1.6340542  nelbo=898.80841\n",
      "i=1700  kl=0.053618908  nell=1.6419401  nelbo=903.12067\n",
      "i=1800  kl=0.13229337  nell=1.6135145  nelbo=887.56525\n",
      "i=1900  kl=0.096142292  nell=1.6008078  nelbo=880.54041\n",
      "i=2000  kl=0.055015057  nell=1.5997508  nelbo=879.91791\n",
      "i=2100  kl=0.056247681  nell=1.6005688  nelbo=880.36908\n",
      "i=2200  kl=0.11911348  nell=1.5922419  nelbo=875.85217\n",
      "i=2300  kl=0.062289894  nell=1.579706  nelbo=868.90057\n",
      "i=2400  kl=0.11474741  nell=1.5719258  nelbo=864.67389\n",
      "i=2500  kl=0.13185343  nell=1.5364045  nelbo=845.1543\n",
      "i=2600  kl=0.13562751  nell=1.4264808  nelbo=784.70007\n",
      "i=2700  kl=0.21272939  nell=1.335958  nelbo=734.98962\n",
      "i=2800  kl=0.2468037  nell=1.102206  nelbo=606.46014\n",
      "i=2900  kl=0.29848257  nell=1.0523112  nelbo=579.06964\n",
      "i=3000  kl=0.27547702  nell=1.0161655  nelbo=559.1665\n",
      "i=3100  kl=0.25667536  nell=0.97336024  nelbo=535.6048\n",
      "i=3200  kl=0.36794147  nell=0.91614407  nelbo=504.24719\n",
      "i=3300  kl=0.38029578  nell=0.88068074  nelbo=484.75473\n",
      "i=3400  kl=0.44015628  nell=0.85761392  nelbo=472.12781\n",
      "i=3500  kl=0.46825051  nell=0.7722863  nelbo=425.22574\n",
      "i=3600  kl=0.52459395  nell=0.66057122  nelbo=363.83878\n",
      "i=3700  kl=0.50556594  nell=0.63410473  nelbo=349.26315\n",
      "i=3800  kl=0.46232873  nell=0.60888171  nelbo=335.34729\n",
      "i=3900  kl=0.60634136  nell=0.59175879  nelbo=326.0737\n",
      "i=4000  kl=0.45179713  nell=0.58245885  nelbo=320.80414\n",
      "i=4100  kl=0.47843435  nell=0.5712952  nelbo=314.6908\n",
      "i=4200  kl=0.42793521  nell=0.56561339  nelbo=311.51532\n",
      "i=4300  kl=0.36232567  nell=0.55961448  nelbo=308.1503\n",
      "i=4400  kl=0.40470195  nell=0.5656575  nelbo=311.51633\n",
      "i=4500  kl=0.39614737  nell=0.55668986  nelbo=306.57556\n",
      "i=4600  kl=0.4529824  nell=0.54105657  nelbo=298.03409\n",
      "i=4700  kl=0.46143031  nell=0.54159033  nelbo=298.33612\n",
      "i=4800  kl=0.380595  nell=0.53336573  nelbo=293.73172\n",
      "i=4900  kl=0.49263969  nell=0.5273751  nelbo=290.54895\n",
      "0.8661\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40487623  nell=3.7610855  nelbo=2069.0017\n",
      "i=100  kl=0.012763292  nell=2.5673923  nelbo=1412.0786\n",
      "i=200  kl=0.0024033487  nell=2.4512503  nelbo=1348.1901\n",
      "i=300  kl=0.00085803866  nell=2.3622317  nelbo=1299.2283\n",
      "i=400  kl=0.0012028217  nell=2.357306  nelbo=1296.5195\n",
      "i=500  kl=0.0013866723  nell=2.3526604  nelbo=1293.9646\n",
      "i=600  kl=0.00076860189  nell=2.335717  nelbo=1284.645\n",
      "i=700  kl=0.00076407194  nell=2.3366673  nelbo=1285.1677\n",
      "i=800  kl=0.001691401  nell=2.3222926  nelbo=1277.2626\n",
      "i=900  kl=0.0010777712  nell=2.3226094  nelbo=1277.4363\n",
      "i=1000  kl=0.0017136037  nell=2.3146696  nelbo=1273.0699\n",
      "i=1100  kl=0.0012540817  nell=2.3191648  nelbo=1275.5419\n",
      "i=1200  kl=0.0011963844  nell=2.3168225  nelbo=1274.2537\n",
      "i=1300  kl=0.0020878315  nell=2.3139679  nelbo=1272.6844\n",
      "i=1400  kl=0.0013838708  nell=2.309835  nelbo=1270.4105\n",
      "i=1500  kl=0.003454268  nell=2.3049176  nelbo=1267.7081\n",
      "i=1600  kl=0.055281848  nell=2.0548382  nelbo=1130.2163\n",
      "i=1700  kl=0.097657859  nell=2.0248241  nelbo=1113.751\n",
      "i=1800  kl=0.089481205  nell=2.0085025  nelbo=1104.7659\n",
      "i=1900  kl=0.052276403  nell=1.9970194  nelbo=1098.413\n",
      "i=2000  kl=0.081985593  nell=1.9798348  nelbo=1088.9912\n",
      "i=2100  kl=0.15503988  nell=1.8376513  nelbo=1010.8632\n",
      "i=2200  kl=0.26438686  nell=1.6732323  nelbo=920.54218\n",
      "i=2300  kl=0.3505173  nell=1.5794798  nelbo=869.06439\n",
      "i=2400  kl=0.56506097  nell=1.4203527  nelbo=781.75903\n",
      "i=2500  kl=0.51119769  nell=1.2166258  nelbo=669.65533\n",
      "i=2600  kl=0.70934093  nell=1.0430939  nelbo=574.41101\n",
      "i=2700  kl=0.68368292  nell=0.9097603  nelbo=501.05185\n",
      "i=2800  kl=0.60475111  nell=0.84483922  nelbo=465.2663\n",
      "i=2900  kl=0.52488625  nell=0.79020751  nelbo=435.13901\n",
      "i=3000  kl=0.66629028  nell=0.76230752  nelbo=419.93542\n",
      "i=3100  kl=0.38698798  nell=0.72487801  nelbo=399.06992\n",
      "i=3200  kl=0.57415032  nell=0.68527234  nelbo=377.47394\n",
      "i=3300  kl=0.51340365  nell=0.61119902  nelbo=336.67285\n",
      "i=3400  kl=0.75683075  nell=0.5603193  nelbo=308.93246\n",
      "i=3500  kl=0.48415148  nell=0.52217168  nelbo=287.67859\n",
      "i=3600  kl=0.54991424  nell=0.50539607  nelbo=278.51776\n",
      "i=3700  kl=0.53492677  nell=0.48117226  nelbo=265.17966\n",
      "i=3800  kl=0.86270428  nell=0.46795368  nelbo=258.23721\n",
      "i=3900  kl=0.4567813  nell=0.45330304  nelbo=249.77345\n",
      "i=4000  kl=0.65311629  nell=0.4419044  nelbo=243.70055\n",
      "i=4100  kl=0.47253725  nell=0.42837843  nelbo=236.08067\n",
      "i=4200  kl=0.50137115  nell=0.4175795  nelbo=230.1701\n",
      "i=4300  kl=0.45695493  nell=0.42028108  nelbo=231.61156\n",
      "i=4400  kl=0.87476444  nell=0.40721068  nelbo=224.84064\n",
      "i=4500  kl=0.63167083  nell=0.3994976  nelbo=220.35535\n",
      "i=4600  kl=0.57775754  nell=0.39062712  nelbo=215.42267\n",
      "i=4700  kl=0.69651473  nell=0.39944655  nelbo=220.39212\n",
      "i=4800  kl=0.54928845  nell=0.38847554  nelbo=214.21083\n",
      "i=4900  kl=0.74351627  nell=0.38370633  nelbo=211.782\n",
      "0.9034\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.3899225  nell=3.9055908  nelbo=2148.4648\n",
      "i=100  kl=0.017628968  nell=2.5739517  nelbo=1415.691\n",
      "i=200  kl=0.0053373873  nell=2.4968967  nelbo=1373.2986\n",
      "i=300  kl=0.0026498735  nell=2.3875387  nelbo=1313.1489\n",
      "i=400  kl=0.0013307631  nell=2.358844  nelbo=1297.3656\n",
      "i=500  kl=0.0018210411  nell=2.3528042  nelbo=1294.0442\n",
      "i=600  kl=0.00094613433  nell=2.3309734  nelbo=1282.0364\n",
      "i=700  kl=0.0014608204  nell=2.3246427  nelbo=1278.5549\n",
      "i=800  kl=0.00071841478  nell=2.3241298  nelbo=1278.2721\n",
      "i=900  kl=0.0014398396  nell=2.3202066  nelbo=1276.1151\n",
      "i=1000  kl=0.00084236264  nell=2.3225179  nelbo=1277.3857\n",
      "i=1100  kl=0.0011501014  nell=2.3151436  nelbo=1273.3301\n",
      "i=1200  kl=0.0024946034  nell=2.3169951  nelbo=1274.3497\n",
      "i=1300  kl=0.0005145371  nell=2.3117032  nelbo=1271.4373\n",
      "i=1400  kl=0.0036031902  nell=2.3106358  nelbo=1270.8534\n",
      "i=1500  kl=0.0042002201  nell=2.3089547  nelbo=1269.9292\n",
      "i=1600  kl=0.0049073696  nell=2.308517  nelbo=1269.6892\n",
      "i=1700  kl=0.0033144355  nell=2.3080254  nelbo=1269.4172\n",
      "i=1800  kl=0.0029197335  nell=2.3075283  nelbo=1269.1434\n",
      "i=1900  kl=0.0032874346  nell=2.3078742  nelbo=1269.3341\n",
      "i=2000  kl=0.004760623  nell=2.30777  nelbo=1269.2783\n",
      "i=2100  kl=0.0077551901  nell=2.3065803  nelbo=1268.627\n",
      "i=2200  kl=0.0062434971  nell=2.3052135  nelbo=1267.8737\n",
      "i=2300  kl=0.0059710443  nell=2.3062921  nelbo=1268.4666\n",
      "i=2400  kl=0.01074338  nell=2.3049064  nelbo=1267.7092\n",
      "i=2500  kl=0.013221473  nell=2.305249  nelbo=1267.9001\n",
      "i=2600  kl=0.0098716617  nell=2.3058577  nelbo=1268.2316\n",
      "i=2700  kl=0.0096235871  nell=2.3054032  nelbo=1267.9814\n",
      "i=2800  kl=0.0096855462  nell=2.3042769  nelbo=1267.3619\n",
      "i=2900  kl=0.0050621033  nell=2.3042657  nelbo=1267.3512\n",
      "i=3000  kl=0.016658932  nell=2.3032041  nelbo=1266.7788\n",
      "i=3100  kl=0.013097078  nell=2.3038568  nelbo=1267.1343\n",
      "i=3200  kl=0.01260069  nell=2.3033233  nelbo=1266.8403\n",
      "i=3300  kl=0.0029832125  nell=2.3045061  nelbo=1267.4813\n",
      "i=3400  kl=0.012356728  nell=2.3038135  nelbo=1267.1097\n",
      "i=3500  kl=0.017881095  nell=2.3033302  nelbo=1266.8494\n",
      "i=3600  kl=0.01146698  nell=2.3036819  nelbo=1267.0365\n",
      "i=3700  kl=0.012897521  nell=2.3033431  nelbo=1266.8516\n",
      "i=3800  kl=0.034250349  nell=2.3029814  nelbo=1266.6741\n",
      "i=3900  kl=0.0080036819  nell=2.3033147  nelbo=1266.8312\n",
      "i=4000  kl=0.015862644  nell=2.3028922  nelbo=1266.6066\n",
      "i=4100  kl=0.015979528  nell=2.3024969  nelbo=1266.3893\n",
      "i=4200  kl=0.026220798  nell=2.3023005  nelbo=1266.2915\n",
      "i=4300  kl=0.031633705  nell=2.3024812  nelbo=1266.3962\n",
      "i=4400  kl=0.61174083  nell=2.1106341  nelbo=1161.4604\n",
      "i=4500  kl=0.36657575  nell=2.0143466  nelbo=1108.2572\n",
      "i=4600  kl=0.16898921  nell=1.9988861  nelbo=1099.5563\n",
      "i=4700  kl=0.26344308  nell=1.9921234  nelbo=1095.9313\n",
      "i=4800  kl=0.45963189  nell=1.9130726  nelbo=1052.6495\n",
      "i=4900  kl=0.54329944  nell=1.7111919  nelbo=941.69879\n",
      "0.3246\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
