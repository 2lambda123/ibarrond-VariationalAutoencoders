{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9bd96c79341a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Agg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlikelihoods\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "## Copyright 2016 Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, Maurizio Filippone\n",
    "##\n",
    "## Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "## you may not use this file except in compliance with the License.\n",
    "## You may obtain a copy of the License at\n",
    "##\n",
    "##     http://www.apache.org/licenses/LICENSE-2.0\n",
    "##\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "## distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "## See the License for the specific language governing permissions and\n",
    "## limitations under the License.\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import DataSet\n",
    "import utils\n",
    "import likelihoods\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "class DgpRff(object):\n",
    "    def __init__(self, likelihood_fun, num_examples, d_in, d_out, n_layers, n_rff, df, kernel_type,\n",
    "                 kernel_arccosine_degree, is_ard, feed_forward, q_Omega_fixed, theta_fixed, learn_Omega):\n",
    "        \"\"\"\n",
    "        :param likelihood_fun: Likelihood function\n",
    "        :param num_examples: total number of input samples\n",
    "        :param d_in: Dimensionality of the input\n",
    "        :param d_out: Dimensionality of the output\n",
    "        :param n_layers: Number of hidden layers\n",
    "        :param n_rff: Number of random features for each layer\n",
    "        :param df: Number of GPs for each layer\n",
    "        :param kernel_type: Kernel type: currently only random Fourier features for RBF and arccosine kernels are implemented\n",
    "        :param kernel_arccosine_degree: degree parameter of the arccosine kernel\n",
    "        :param is_ard: Whether the kernel is ARD or isotropic\n",
    "        :param feed_forward: Whether the original inputs should be fed forward as input to each layer\n",
    "        :param Omega_fixed: Whether the Omega weights should be fixed throughout the optimization\n",
    "        :param theta_fixed: Whether covariance parameters should be fixed throughout the optimization\n",
    "        :param learn_Omega: How to treat Omega - fixed (from the prior), optimized, or learned variationally\n",
    "        \"\"\"\n",
    "        self.likelihood = likelihood_fun\n",
    "        self.kernel_type = kernel_type\n",
    "        self.is_ard = is_ard\n",
    "        self.feed_forward = feed_forward\n",
    "        self.q_Omega_fixed = q_Omega_fixed\n",
    "        self.theta_fixed = theta_fixed\n",
    "        self.q_Omega_fixed_flag = q_Omega_fixed > 0\n",
    "        self.theta_fixed_flag = theta_fixed > 0\n",
    "        self.learn_Omega = learn_Omega\n",
    "        self.arccosine_degree = kernel_arccosine_degree\n",
    "\n",
    "        ## These are all scalars\n",
    "        self.num_examples = num_examples\n",
    "        self.nl = n_layers ## Number of hidden layers\n",
    "        self.n_Omega = n_layers  ## Number of weigh matrices is \"Number of hidden layers\"\n",
    "        self.n_W = n_layers\n",
    "\n",
    "        ## These are arrays to allow flexibility in the future\n",
    "        self.n_rff = n_rff * np.ones(n_layers, dtype = np.int64)\n",
    "        self.df = df * np.ones(n_layers, dtype=np.int64)\n",
    "\n",
    "        ## Dimensionality of Omega matrices\n",
    "        if self.feed_forward:\n",
    "            self.d_in = np.concatenate([[d_in], self.df[:(n_layers - 1)] + d_in])\n",
    "        else:\n",
    "            self.d_in = np.concatenate([[d_in], self.df[:(n_layers - 1)]])\n",
    "        self.d_out = self.n_rff\n",
    "\n",
    "        ## Dimensionality of W matrices\n",
    "        if self.kernel_type == \"RBF\":\n",
    "            self.dhat_in = self.n_rff * 2\n",
    "            self.dhat_out = np.concatenate([self.df[:-1], [d_out]])\n",
    "\n",
    "        if self.kernel_type == \"arccosine\":\n",
    "            self.dhat_in = self.n_rff\n",
    "            self.dhat_out = np.concatenate([self.df[:-1], [d_out]])\n",
    "\n",
    "        ## When Omega is learned variationally, define the right KL function and the way Omega are constructed\n",
    "        if self.learn_Omega == \"var_resampled\":\n",
    "            self.get_kl = self.get_kl_Omega_to_learn\n",
    "            self.sample_from_Omega = self.sample_from_Omega_to_learn\n",
    "\n",
    "        ## When Omega is optimized, fix some standard normals throughout the execution that will be used to construct Omega\n",
    "        if self.learn_Omega == \"var_fixed\":\n",
    "            self.get_kl = self.get_kl_Omega_to_learn\n",
    "            self.sample_from_Omega = self.sample_from_Omega_optim\n",
    "\n",
    "            self.z_for_Omega_fixed = []\n",
    "            for i in range(self.n_Omega):\n",
    "                tmp = utils.get_normal_samples(1, self.d_in[i], self.d_out[i])\n",
    "                self.z_for_Omega_fixed.append(tf.Variable(tmp[0,:,:], trainable = False))\n",
    "\n",
    "        ## When Omega is fixed, fix some standard normals throughout the execution that will be used to construct Omega\n",
    "        if self.learn_Omega == \"prior_fixed\":\n",
    "            self.get_kl = self.get_kl_Omega_fixed\n",
    "            self.sample_from_Omega = self.sample_from_Omega_fixed\n",
    " \n",
    "            self.z_for_Omega_fixed = []\n",
    "            for i in range(self.n_Omega):\n",
    "                tmp = utils.get_normal_samples(1, self.d_in[i], self.d_out[i])\n",
    "                self.z_for_Omega_fixed.append(tf.Variable(tmp[0,:,:], trainable = False))\n",
    "\n",
    "        ## Parameters defining prior over Omega\n",
    "        self.log_theta_sigma2 = tf.Variable(tf.zeros([n_layers]), name=\"log_theta_sigma2\")\n",
    "\n",
    "        if self.is_ard:\n",
    "            self.llscale0 = []\n",
    "            for i in range(self.nl):\n",
    "                self.llscale0.append(tf.constant(0.5 * np.log(self.d_in[i]), tf.float32))\n",
    "        else:\n",
    "            self.llscale0 = tf.constant(0.5 * np.log(self.d_in), tf.float32)\n",
    "\n",
    "        if self.is_ard:\n",
    "            self.log_theta_lengthscale = []\n",
    "            for i in range(self.nl):\n",
    "                self.log_theta_lengthscale.append(tf.Variable(tf.multiply(tf.ones([self.d_in[i]]), self.llscale0[i]), name=\"log_theta_lengthscale\"))\n",
    "        else:\n",
    "            self.log_theta_lengthscale = tf.Variable(self.llscale0, name=\"log_theta_lengthscale\")\n",
    "        self.prior_mean_Omega, self.log_prior_var_Omega = self.get_prior_Omega(self.log_theta_lengthscale)\n",
    "\n",
    "        ## Set the prior over weights\n",
    "        self.prior_mean_W, self.log_prior_var_W = self.get_prior_W()\n",
    "\n",
    "        ## Initialize posterior parameters\n",
    "        if self.learn_Omega == \"var_resampled\":\n",
    "            self.mean_Omega, self.log_var_Omega = self.init_posterior_Omega()\n",
    "        if self.learn_Omega == \"var_fixed\":\n",
    "            self.mean_Omega, self.log_var_Omega = self.init_posterior_Omega()\n",
    "\n",
    "        self.mean_W, self.log_var_W = self.init_posterior_W()\n",
    "\n",
    "        ## Set the number of Monte Carlo samples as a placeholder so that it can be different for training and test\n",
    "        self.mc =  tf.placeholder(tf.int32) \n",
    "\n",
    "        ## Batch data placeholders\n",
    "        Din = d_in\n",
    "        Dout = d_out\n",
    "        self.X = tf.placeholder(tf.float32, [None, Din])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, Dout])\n",
    "\n",
    "        ## Builds whole computational graph with relevant quantities as part of the class\n",
    "        self.loss, self.kl, self.ell, self.layer_out = self.get_nelbo()\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.Session()\n",
    "\n",
    "    ## Definition of a prior for Omega - which depends on the lengthscale of the covariance function\n",
    "    def get_prior_Omega(self, log_lengthscale):\n",
    "        if self.is_ard:\n",
    "            prior_mean_Omega = []\n",
    "            log_prior_var_Omega = []\n",
    "            for i in range(self.nl):\n",
    "                prior_mean_Omega.append(tf.zeros([self.d_in[i],1]))\n",
    "            for i in range(self.nl):\n",
    "                log_prior_var_Omega.append(-2 * log_lengthscale[i])\n",
    "        else:\n",
    "            prior_mean_Omega = tf.zeros(self.nl)\n",
    "            log_prior_var_Omega = -2 * log_lengthscale\n",
    "        return prior_mean_Omega, log_prior_var_Omega\n",
    "\n",
    "    ## Definition of a prior over W - these are standard normals\n",
    "    def get_prior_W(self):\n",
    "        prior_mean_W = tf.zeros(self.n_W)\n",
    "        log_prior_var_W = tf.zeros(self.n_W)\n",
    "        return prior_mean_W, log_prior_var_W\n",
    "\n",
    "    ## Function to initialize the posterior over omega\n",
    "    def init_posterior_Omega(self):\n",
    "        mu, sigma2 = self.get_prior_Omega(self.llscale0)\n",
    "\n",
    "        mean_Omega = [tf.Variable(mu[i] * tf.ones([self.d_in[i], self.d_out[i]]), name=\"q_Omega\") for i in range(self.n_Omega)]\n",
    "        log_var_Omega = [tf.Variable(sigma2[i] * tf.ones([self.d_in[i], self.d_out[i]]), name=\"q_Omega\") for i in range(self.n_Omega)]\n",
    "\n",
    "        return mean_Omega, log_var_Omega\n",
    "\n",
    "    ## Function to initialize the posterior over W\n",
    "    def init_posterior_W(self):\n",
    "        mean_W = [tf.Variable(tf.zeros([self.dhat_in[i], self.dhat_out[i]]), name=\"q_W\") for i in range(self.n_W)]\n",
    "        log_var_W = [tf.Variable(tf.zeros([self.dhat_in[i], self.dhat_out[i]]), name=\"q_W\") for i in range(self.n_W)]\n",
    "\n",
    "        return mean_W, log_var_W\n",
    "\n",
    "    ## Function to compute the KL divergence between priors and approximate posteriors over model parameters (Omega and W) when q(Omega) is to be learned \n",
    "    def get_kl_Omega_to_learn(self):\n",
    "        kl = 0\n",
    "        for i in range(self.n_Omega):\n",
    "            kl = kl + utils.DKL_gaussian(self.mean_Omega[i], self.log_var_Omega[i], self.prior_mean_Omega[i], self.log_prior_var_Omega[i])\n",
    "        for i in range(self.n_W):\n",
    "            kl = kl + utils.DKL_gaussian(self.mean_W[i], self.log_var_W[i], self.prior_mean_W[i], self.log_prior_var_W[i])\n",
    "        return kl\n",
    "\n",
    "    ## Function to compute the KL divergence between priors and approximate posteriors over model parameters (W only) when q(Omega) is not to be learned \n",
    "    def get_kl_Omega_fixed(self):\n",
    "        kl = 0\n",
    "        for i in range(self.n_W):\n",
    "            kl = kl + utils.DKL_gaussian(self.mean_W[i], self.log_var_W[i], self.prior_mean_W[i], self.log_prior_var_W[i])\n",
    "        return kl\n",
    "\n",
    "    ## Returns samples from approximate posterior over Omega\n",
    "    def sample_from_Omega_to_learn(self):\n",
    "        Omega_from_q = []\n",
    "        for i in range(self.n_Omega):\n",
    "            z = utils.get_normal_samples(self.mc, self.d_in[i], self.d_out[i])\n",
    "            Omega_from_q.append(tf.add(tf.multiply(z, tf.exp(self.log_var_Omega[i] / 2)), self.mean_Omega[i]))\n",
    "\n",
    "        return Omega_from_q\n",
    "\n",
    "    ## Returns Omega values calculated from fixed random variables and mean and variance of q() - the latter are optimized and enter the calculation of the KL so also lengthscale parameters get optimized\n",
    "    def sample_from_Omega_optim(self):\n",
    "        Omega_from_q = []\n",
    "        for i in range(self.n_Omega):\n",
    "            z = tf.multiply(self.z_for_Omega_fixed[i], tf.ones([self.mc, self.d_in[i], self.d_out[i]]))\n",
    "            Omega_from_q.append(tf.add(tf.multiply(z, tf.exp(self.log_var_Omega[i] / 2)), self.mean_Omega[i]))\n",
    "\n",
    "        return Omega_from_q\n",
    "\n",
    "    ## Returns samples from prior over Omega - in this case, randomness is fixed throughout learning (and Monte Carlo samples)\n",
    "    def sample_from_Omega_fixed(self):\n",
    "        Omega_from_q = []\n",
    "        for i in range(self.n_Omega):\n",
    "            z = tf.multiply(self.z_for_Omega_fixed[i], tf.ones([self.mc, self.d_in[i], self.d_out[i]]))\n",
    "\n",
    "            if self.is_ard == True:\n",
    "                reshaped_log_prior_var_Omega = tf.tile(tf.reshape(self.log_prior_var_Omega[i] / 2, [self.d_in[i],1]), [1,self.d_out[i]])\n",
    "                Omega_from_q.append(tf.multiply(z, tf.exp(reshaped_log_prior_var_Omega)))\n",
    "            if self.is_ard == False:\n",
    "                Omega_from_q.append(tf.add(tf.multiply(z, tf.exp(self.log_prior_var_Omega[i] / 2)), self.prior_mean_Omega[i]))\n",
    "\n",
    "        return Omega_from_q\n",
    "\n",
    "    ## Returns samples from approximate posterior over W\n",
    "    def sample_from_W(self):\n",
    "        W_from_q = []\n",
    "        for i in range(self.n_W):\n",
    "            z = utils.get_normal_samples(self.mc, self.dhat_in[i], self.dhat_out[i])\n",
    "            self.z = z\n",
    "            W_from_q.append(tf.add(tf.multiply(z, tf.exp(self.log_var_W[i] / 2)), self.mean_W[i]))\n",
    "        return W_from_q\n",
    "\n",
    "    ## Returns the expected log-likelihood term in the variational lower bound \n",
    "    def get_ell(self):\n",
    "        Din = self.d_in[0]\n",
    "        MC = self.mc\n",
    "        N_L = self.nl\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "        batch_size = tf.shape(X)[0] # This is the actual batch size when X is passed to the graph of computations\n",
    "\n",
    "        ## The representation of the information is based on 3-dimensional tensors (one for each layer)\n",
    "        ## Each slice [i,:,:] of these tensors is one Monte Carlo realization of the value of the hidden units\n",
    "        ## At layer zero we simply replicate the input matrix X self.mc times\n",
    "        self.layer = []\n",
    "        self.layer.append(tf.multiply(tf.ones([self.mc, batch_size, Din]), X))\n",
    "\n",
    "        ## Forward propagate information from the input to the output through hidden layers\n",
    "        Omega_from_q  = self.sample_from_Omega()\n",
    "        W_from_q = self.sample_from_W()\n",
    "        # TODO: basis features should be in a different class\n",
    "        for i in range(N_L):\n",
    "            layer_times_Omega = tf.matmul(self.layer[i], Omega_from_q[i])  # X * Omega\n",
    "\n",
    "            ## Apply the activation function corresponding to the chosen kernel - PHI\n",
    "            if self.kernel_type == \"RBF\": \n",
    "                Phi = tf.exp(0.5 * self.log_theta_sigma2[i]) / (tf.sqrt(1. * self.n_rff[i])) * tf.concat(values=[tf.cos(layer_times_Omega), tf.sin(layer_times_Omega)], axis=2)\n",
    "            if self.kernel_type == \"arccosine\": \n",
    "                if self.arccosine_degree == 0:\n",
    "                    Phi = tf.exp(0.5 * self.log_theta_sigma2[i]) / (tf.sqrt(1. * self.n_rff[i])) * tf.concat(values=[tf.sign(tf.maximum(layer_times_Omega, 0.0))], axis=2)\n",
    "                if self.arccosine_degree == 1:\n",
    "                    Phi = tf.exp(0.5 * self.log_theta_sigma2[i]) / (tf.sqrt(1. * self.n_rff[i])) * tf.concat(values=[tf.maximum(layer_times_Omega, 0.0)], axis=2)\n",
    "                if self.arccosine_degree == 2:\n",
    "                    Phi = tf.exp(0.5 * self.log_theta_sigma2[i]) / (tf.sqrt(1. * self.n_rff[i])) * tf.concat(values=[tf.square(tf.maximum(layer_times_Omega, 0.0))], axis=2)\n",
    "\n",
    "            F = tf.matmul(Phi, W_from_q[i])\n",
    "            if self.feed_forward and not (i == (N_L-1)): ## In the feed-forward case, no concatenation in the last layer so that F has the same dimensions of Y\n",
    "                F = tf.concat(values=[F, self.layer[0]], axis=2)\n",
    "\n",
    "            self.layer.append(F)\n",
    "\n",
    "        ## Output layer\n",
    "        layer_out = self.layer[N_L]\n",
    "\n",
    "        ## Given the output layer, we compute the conditional likelihood across all samples\n",
    "        ll = self.likelihood.log_cond_prob(Y, layer_out)\n",
    "\n",
    "        ## Mini-batch estimation of the expected log-likelihood term\n",
    "        ell = tf.reduce_sum(tf.reduce_mean(ll, 0)) * self.num_examples / tf.cast(batch_size, \"float32\")\n",
    "\n",
    "        return ell, layer_out\n",
    "\n",
    "    ## Maximize variational lower bound --> minimize Nelbo\n",
    "    def get_nelbo(self):\n",
    "        kl = self.get_kl()\n",
    "        ell, layer_out = self.get_ell()\n",
    "        nelbo  = kl - ell\n",
    "        return nelbo, kl, ell, layer_out\n",
    "\n",
    "    ## Return predictions on some data\n",
    "    def predict(self, data, mc_test):\n",
    "        out = self.likelihood.predict(self.layer_out)\n",
    "\n",
    "        nll = - tf.reduce_sum(-np.log(mc_test) + utils.logsumexp(self.likelihood.log_cond_prob(self.Y, self.layer_out), 0))\n",
    "        #nll = - tf.reduce_sum(tf.reduce_mean(self.likelihood.log_cond_prob(self.Y, self.layer_out), 0))\n",
    "        pred, neg_ll = self.session.run([out, nll], feed_dict={self.X:data.X, self.Y: data.Y, self.mc:mc_test})\n",
    "        mean_pred = np.mean(pred, 0)\n",
    "        return mean_pred, neg_ll\n",
    "\n",
    "    ## Return the list of TF variables that should be \"free\" to be optimized\n",
    "    def get_vars_fixing_some(self, all_variables):\n",
    "        if (self.q_Omega_fixed_flag == True) and (self.theta_fixed_flag == True):\n",
    "            variational_parameters = [v for v in all_variables if (not v.name.startswith(\"q_Omega\") and not v.name.startswith(\"log_theta\"))]\n",
    "\n",
    "        if (self.q_Omega_fixed_flag == True) and (self.theta_fixed_flag == False):\n",
    "            variational_parameters = [v for v in all_variables if (not v.name.startswith(\"q_Omega\"))]\n",
    "\n",
    "        if (self.q_Omega_fixed_flag == False) and (self.theta_fixed_flag == True):\n",
    "            variational_parameters = [v for v in all_variables if (not v.name.startswith(\"log_theta\"))]\n",
    "\n",
    "        if (self.q_Omega_fixed_flag == False) and (self.theta_fixed_flag == False):\n",
    "            variational_parameters = all_variables\n",
    "\n",
    "        return variational_parameters\n",
    "\n",
    "    ## Function that learns the deep GP model with random Fourier feature approximation\n",
    "    def learn(self, data, learning_rate, mc_train, batch_size, n_iterations, optimizer = None, display_step=100, test = None, mc_test=None, loss_function=None, duration = 1000000, less_prints=False):\n",
    "        total_train_time = 0\n",
    "\n",
    "        if optimizer is None:\n",
    "            optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        ## Initialize all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        ##init = tf.initialize_all_variables()\n",
    "\n",
    "        ## Fix any variables that are supposed to be fixed\n",
    "        train_step = optimizer.minimize(self.loss, var_list=self.get_vars_fixing_some(all_variables))\n",
    "\n",
    "        ## Initialize TF session\n",
    "        self.session.run(init)\n",
    "\n",
    "        ## Set the folder where the logs are going to be written \n",
    "        # summary_writer = tf.train.SummaryWriter('logs/', self.session.graph)\n",
    "        summary_writer = tf.summary.FileWriter('logs/', self.session.graph)\n",
    "\n",
    "        # random_vec, _, _, layer_out =  self.session.run(self.get_nelbo(), feed_dict={self.X: data.X, self.Y: data.Y, self.mc: mc_train})\n",
    "        # print(layer_out)\n",
    "\n",
    "        if not(less_prints):\n",
    "            nelbo, kl, ell, _ =  self.session.run(self.get_nelbo(), feed_dict={self.X: data.X, self.Y: data.Y, self.mc: mc_train})\n",
    "            print(\"Initial kl=\" + repr(kl) + \"  nell=\" + repr(-ell) + \"  nelbo=\" + repr(nelbo), end=\" \")\n",
    "            print(\"  log-sigma2 =\", self.session.run(self.log_theta_sigma2))\n",
    "        \n",
    "        ## Present data to DGP n_iterations times\n",
    "        ## TODO: modify the code so that the user passes the number of epochs (number of times the whole training set is presented to the DGP)\n",
    "        for iteration in range(n_iterations):\n",
    "\n",
    "            ## Stop after a given budget of minutes is reached\n",
    "            if (total_train_time > 1000 * 60 * duration):\n",
    "                break\n",
    "\n",
    "            ## Present one batch of data to the DGP\n",
    "            start_train_time = current_milli_time()\n",
    "            batch = data.next_batch(batch_size)\n",
    "\n",
    "            monte_carlo_sample_train = mc_train\n",
    "            if (current_milli_time() - start_train_time) < (1000 * 60 * duration / 2.0): \n",
    "                monte_carlo_sample_train = 1\n",
    "\n",
    "            self.session.run(train_step, feed_dict={self.X: batch[0], self.Y: batch[1], self.mc: monte_carlo_sample_train})\n",
    "            total_train_time += current_milli_time() - start_train_time\n",
    "\n",
    "            ## After reaching enough iterations with Omega fixed, unfix it\n",
    "            if self.q_Omega_fixed_flag == True:\n",
    "                if iteration >= self.q_Omega_fixed:\n",
    "                    self.q_Omega_fixed_flag = False\n",
    "                    train_step = optimizer.minimize(self.loss, var_list=self.get_vars_fixing_some(all_variables))\n",
    "\n",
    "            if self.theta_fixed_flag == True:\n",
    "                if iteration >= self.theta_fixed:\n",
    "                    self.theta_fixed_flag = False\n",
    "                    train_step = optimizer.minimize(self.loss, var_list=self.get_vars_fixing_some(all_variables))\n",
    "\n",
    "            ## Display logs every \"FLAGS.display_step\" iterations\n",
    "            if iteration % display_step == 0:\n",
    "                start_predict_time = current_milli_time()\n",
    "\n",
    "                if less_prints:\n",
    "                    print(\"i=\" + repr(iteration), end = \" \")\n",
    "\n",
    "                else:\n",
    "                    nelbo, kl, ell, _ = self.session.run(self.get_nelbo(),\n",
    "                                                     feed_dict={self.X: data.X, self.Y: data.Y, self.mc: mc_train})\n",
    "                    print(\"i=\" + repr(iteration)  + \"  kl=\" + repr(kl) + \"  nell=\" + repr(-ell)  + \"  nelbo=\" + repr(nelbo), end=\" \")\n",
    "\n",
    "                    print(\" log-sigma2=\", self.session.run(self.log_theta_sigma2), end=\" \")\n",
    "                    # print(\" log-lengthscale=\", self.session.run(self.log_theta_lengthscale), end=\" \")\n",
    "                    # print(\" Omega=\", self.session.run(self.mean_Omega[0][0,:]), end=\" \")\n",
    "                    # print(\" W=\", self.session.run(self.mean_W[0][0,:]), end=\" \")\n",
    "\n",
    "                if loss_function is not None:\n",
    "                    pred, nll_test = self.predict(test, mc_test)\n",
    "                    elapsed_time = total_train_time + (current_milli_time() - start_predict_time)\n",
    "                    print(loss_function.get_name() + \"=\" + \"%.4f\" % loss_function.eval(test.Y, pred), end = \" \")\n",
    "                    print(\" nll_test=\" + \"%.5f\" % (nll_test / len(test.Y)), end = \" \")\n",
    "                print(\" time=\" + repr(elapsed_time), end = \" \")\n",
    "                print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
