{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0H9P55wcuZIxfduAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d2e66e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[10].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalInference(object):\n",
    "    def __init__(self, n_datapoints, neurons_per_layer, mc_samples, batch_size):\n",
    "        # SIZES\n",
    "        self.N = n_datapoints\n",
    "        self.layers = len(neurons_per_layer)\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.M = batch_size\n",
    "        ## Set the number of Monte Carlo samples as a placeholder so that it can be different for training and test\n",
    "        # self.L =  tf.placeholder(tf.int32)\n",
    "        self.L = mc_samples\n",
    "        \n",
    "        ## Batch data placeholders\n",
    "        with tf.name_scope('input'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[0]], name='x-input')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[-1]], name='y-input')\n",
    "            \n",
    "        with tf.name_scope('input_reshape'):\n",
    "            image_shaped_input = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            tf.summary.image('input', image_shaped_input, 10)\n",
    "        \n",
    "        # PRIOR OF WEIGHTS\n",
    "        self.prior_mean_W, self.log_prior_var_W = self.get_prior_W()\n",
    "    \n",
    "        # POSTERIOR OF WEIGHTS\n",
    "        self.mean_W, self.log_var_W = self.init_posterior_W()\n",
    "        ## Builds whole computational graph with relevant quantities as part of the class\n",
    "        # self.loss, self.kl, self.ell, self.layer_out = self.get_nelbo()\n",
    "        self.loss, self.kl, self.ell = self.get_nelbo()\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.Session()\n",
    "    \n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def get_prior_W(self):\n",
    "        \"\"\"\n",
    "        Define a prior for the weight distribution.\n",
    "        We assume them to be standard normal iid.\n",
    "        \"\"\"\n",
    "        prior_mean_W = []\n",
    "        log_prior_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            \n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_prior_weights\"):\n",
    "                prior_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                log_prior_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                tf.summary.histogram('prior_mean', tf.reshape(prior_mean, [-1]))\n",
    "                tf.summary.histogram('prior_logvar', tf.reshape(log_prior_var, [-1]))\n",
    "            \n",
    "            prior_mean_W.append(prior_mean)\n",
    "            log_prior_var_W.append(log_prior_var)\n",
    "        \n",
    "        return prior_mean_W, log_prior_var_W\n",
    "\n",
    "    def init_posterior_W(self):\n",
    "        \"\"\"\n",
    "        The (variational) posterior is assumed to be\n",
    "        drawn from P mutually independent normal distributions.\n",
    "        Hence, we have a diagonal covariance matrix and only need to store an array.\n",
    "        \"\"\"\n",
    "        mean_W = []\n",
    "        log_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "\n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_posterior_weights\"):\n",
    "                post_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                post_log_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                tf.summary.histogram('posterior_mean', tf.reshape(post_mean, [-1]))\n",
    "                tf.summary.histogram('posterior_logvar', tf.reshape(post_log_var, [-1]))\n",
    "            \n",
    "            mean_W.append(post_mean)\n",
    "            log_var_W.append(post_log_var)\n",
    "            \n",
    "        return mean_W, log_var_W\n",
    "    \n",
    "    def get_std_norm_samples(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        Draws N(0,1) samples of dimension [d_in, d_out].\n",
    "        \"\"\"\n",
    "        return tf.random_normal(shape=[d_in, d_out])\n",
    "\n",
    "    def sample_from_W(self):\n",
    "        \"\"\"\n",
    "        Samples from the variational posterior approximation.\n",
    "        We draw W-samples for each layer using the reparameterization trick.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            z = self.get_std_norm_samples(d_in, d_out)\n",
    "            ## division by 2 to obtain pure standard deviation\n",
    "            w_from_q = tf.add(tf.multiply(z, tf.exp(self.log_var_W[i] / 2)), self.mean_W[i])\n",
    "        \n",
    "            yield w_from_q\n",
    "    \n",
    "    def feedforward(self):\n",
    "        \"\"\"\n",
    "        Feedforward pass excluding last layer's transfer function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We will generate L output samples\n",
    "        for i in range(self.L):\n",
    "            \n",
    "            inputs = self.X\n",
    "            \n",
    "            # Go through each layer (one weight matrix at a time)\n",
    "            # and compute the (intermediate) output\n",
    "            j = 0\n",
    "            for weight_matrix in self.sample_from_W():\n",
    "                activations = tf.matmul(inputs, weight_matrix[1:,:]) + weight_matrix[0,:]\n",
    "                # tf.summary.histogram('activations', activations)\n",
    "\n",
    "                # if last layer is reached, do not use transfer function (softmax later on)\n",
    "                if j == (self.layers - 2):\n",
    "                    outputs = tf.sigmoid(activations)\n",
    "                else:\n",
    "                    outputs = tf.nn.softplus(activations)\n",
    "\n",
    "                #outputs = tf.sigmoid(activations)\n",
    "                #tf.summary.histogram('outputs', outputs)\n",
    "\n",
    "                inputs = outputs\n",
    "                j += 1\n",
    "                \n",
    "            # use generator to save memory space\n",
    "            yield outputs\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict using monte carlo sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_output = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            expected_output += output\n",
    "            \n",
    "        return expected_output / self.L\n",
    "    \n",
    "    def get_ell(self):\n",
    "        \"\"\"\n",
    "        Returns the expected log-likelihood of the lower bound.\n",
    "        For this we draw L samples from W, compute the log-likelihood for each\n",
    "        and average the log-likelihoods in the end (expectation approximation).\n",
    "        \"\"\"\n",
    "        \n",
    "        log_p = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            # y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]) + b)\n",
    "            # log_p_per_sample = tf.reduce_mean(tf.reduce_sum(self.Y * tf.log(y), reduction_indices=[1]))\n",
    "            # soft_max_cross_entropy_with_logits is a numerically stable version of cross entropy\n",
    "            # log_p_per_sample = tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.Y))\n",
    "            \n",
    "            log_p_per_sample = tf.reduce_mean(tf.reduce_sum(\n",
    "                                    self.Y * tf.log(output + 1e-10) + (1 - self.Y) * tf.log(1 - output + 1e-10),\n",
    "                                    reduction_indices=[1]))\n",
    "            log_p += log_p_per_sample\n",
    "        \n",
    "        return log_p / self.L\n",
    "\n",
    "    def get_kl(self, mean_W, log_var_W, prior_mean_W, log_prior_var_W):\n",
    "        \"\"\"\n",
    "        KL[q || p] returns the KL-divergence between the prior p and the variational posterior q.\n",
    "        :param mq: vector of means for q\n",
    "        :param log_vq: vector of log-variances for q\n",
    "        :param mp: vector of means for p\n",
    "        :param log_vp: vector of log-variances for p\n",
    "        :return: KL divergence between q and p\n",
    "        \"\"\"\n",
    "        mq = mean_W\n",
    "        log_vq = log_var_W\n",
    "        mp = prior_mean_W\n",
    "        log_vp = log_prior_var_W\n",
    "        \n",
    "        #log_vp = tf.reshape(log_vp, (-1, 1))\n",
    "        return 0.5 * tf.reduce_sum(log_vp - log_vq + (tf.pow(mq - mp, 2) / tf.exp(log_vp)) + tf.exp(log_vq - log_vp) - 1)\n",
    "\n",
    "    def get_kl_multi(self):\n",
    "        \"\"\"\n",
    "        Compute KL divergence between variational and prior using a multi-layer-network\n",
    "        \"\"\"\n",
    "        kl = 0\n",
    "        for i in range(self.layers - 1):\n",
    "            kl = kl + self.get_kl(self.mean_W[i], self.log_var_W[i], self.prior_mean_W[i], self.log_prior_var_W[i])\n",
    "        return kl\n",
    "    \n",
    "    def get_nelbo(self):\n",
    "        \"\"\" Returns the negative ELBOW, which allows us to minimize instead of maximize. \"\"\"\n",
    "        kl = self.get_kl_multi()\n",
    "        # ell, layer_out = self.get_ell()\n",
    "        ell = self.get_ell()\n",
    "        # DKL_gaussian - tf.mean([log_likelihood(w) for w in w_from_q])\n",
    "        nelbo = kl - self.N/tf.cast(self.M, \"float32\") * ell\n",
    "        # return nelbo, kl, ell, layer_out\n",
    "        return nelbo, kl, ell\n",
    "    \n",
    "    def learn(self, learning_rate=0.01, epochs=50):\n",
    "        \"\"\" Our learning procedure \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        tf.summary.scalar('negative_elbo', self.loss)\n",
    "        tf.summary.scalar('kl_div', self.kl)\n",
    "        tf.summary.scalar('ell', self.ell)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter('logs/train', self.session.graph)\n",
    "        test_writer = tf.summary.FileWriter('logs/test')        \n",
    "        \n",
    "        ## Initialize all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        ## Initialize TF session\n",
    "        self.session.run(init)\n",
    "\n",
    "        ## Set the folder where the logs are going to be written \n",
    "        # summary_writer = tf.train.SummaryWriter('logs/', self.sessihttp://localhost:8888/notebooks/Bayesian%20Neural%20Network.ipynb#on.graph)\n",
    "        #summary_writer = tf.summary.FileWriter('logs/', self.session.graph)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch: \", i)\n",
    "            \n",
    "            for batch_i in range(mnist.train.num_examples // self.M):\n",
    "                batch_xs, _ = mnist.train.next_batch(self.M)\n",
    "\n",
    "                _, summary = self.session.run([train_step, merged], feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "                train_writer.add_summary(summary, i)\n",
    "            \n",
    "            summary, nelbo = self.session.run([merged, self.get_nelbo()],\n",
    "                                              feed_dict={self.X: mnist.test.images, self.Y: mnist.test.images})\n",
    "            print(\"i=\" + repr(i)  + \"  kl=\" + repr(nelbo[1]) + \"  nell=\" + repr(-nelbo[2])  + \"  nelbo=\" + repr(nelbo[0]), end=\"\\n\")\n",
    "        \n",
    "            test_writer.add_summary(summary, i)\n",
    "        \n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "    def test_pred(self, input_vector):\n",
    "        output = self.predict()\n",
    "        return self.session.run(output, feed_dict={self.X: input_vector, self.Y: mnist.test.images})\n",
    "    \n",
    "    def benchmark(self):\n",
    "        output = self.predict()\n",
    "        correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(self.Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(self.session.run(accuracy, feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))\n",
    "        \n",
    "    def debug(self):\n",
    "        all_variables = tf.trainable_variables()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session.run(init)\n",
    "        print(self.session.run([self.prior_mean_W, self.log_prior_var_W], feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_datapoints, weight_dim, n_samples, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = mnist.train.num_examples\n",
    "# including input neurons\n",
    "mc_samples = 10\n",
    "batch_size = 100\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7fcd07d13128>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if GPU support is enabled\n",
    "tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b0fd916fe0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mneurons_per_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariationalInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_datapoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons_per_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d74e047d6f8e>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, learning_rate, epochs)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-projects/environments/tf-simd/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 2048, 2048, 2, 2048, 2048, 784]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "vi.learn(learning_rate=0.01, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.test.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(mnist.test.images[0], [1, -1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0050708d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[0].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5CcKFJOfKFBMfqJA/T+0SMW6WRbAIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdfae142668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_out = vi.test_pred(np.reshape(mnist.test.images[200], [1, -1]))\n",
    "\n",
    "plt.imshow(img_out.reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40948671  nell=3.7837021  nelbo=2081.4456\n",
      "i=100  kl=0.0097986162  nell=2.527931  nelbo=1390.3718\n",
      "i=200  kl=0.0025273561  nell=2.44379  nelbo=1344.087\n",
      "i=300  kl=0.004008472  nell=2.40974  nelbo=1325.361\n",
      "i=400  kl=0.0078063905  nell=2.2114987  nelbo=1216.3322\n",
      "i=500  kl=0.012652636  nell=2.0823584  nelbo=1145.3098\n",
      "i=600  kl=0.016389519  nell=2.0481839  nelbo=1126.5175\n",
      "i=700  kl=0.01134482  nell=2.0340207  nelbo=1118.7227\n",
      "i=800  kl=0.0098190308  nell=2.0190938  nelbo=1110.5114\n",
      "i=900  kl=0.012732029  nell=2.0130954  nelbo=1107.2152\n",
      "i=1000  kl=0.0087222755  nell=1.9915596  nelbo=1095.3665\n",
      "i=1100  kl=0.026929289  nell=1.9394716  nelbo=1066.7363\n",
      "i=1200  kl=0.034531385  nell=1.8631294  nelbo=1024.7557\n",
      "i=1300  kl=0.063946992  nell=1.7455273  nelbo=960.10394\n",
      "i=1400  kl=0.050398767  nell=1.6979859  nelbo=933.94263\n",
      "i=1500  kl=0.072067857  nell=1.6569531  nelbo=911.3963\n",
      "i=1600  kl=0.07861957  nell=1.6340542  nelbo=898.80841\n",
      "i=1700  kl=0.053618908  nell=1.6419401  nelbo=903.12067\n",
      "i=1800  kl=0.13229337  nell=1.6135145  nelbo=887.56525\n",
      "i=1900  kl=0.096142292  nell=1.6008078  nelbo=880.54041\n",
      "i=2000  kl=0.055015057  nell=1.5997508  nelbo=879.91791\n",
      "i=2100  kl=0.056247681  nell=1.6005688  nelbo=880.36908\n",
      "i=2200  kl=0.11911348  nell=1.5922419  nelbo=875.85217\n",
      "i=2300  kl=0.062289894  nell=1.579706  nelbo=868.90057\n",
      "i=2400  kl=0.11474741  nell=1.5719258  nelbo=864.67389\n",
      "i=2500  kl=0.13185343  nell=1.5364045  nelbo=845.1543\n",
      "i=2600  kl=0.13562751  nell=1.4264808  nelbo=784.70007\n",
      "i=2700  kl=0.21272939  nell=1.335958  nelbo=734.98962\n",
      "i=2800  kl=0.2468037  nell=1.102206  nelbo=606.46014\n",
      "i=2900  kl=0.29848257  nell=1.0523112  nelbo=579.06964\n",
      "i=3000  kl=0.27547702  nell=1.0161655  nelbo=559.1665\n",
      "i=3100  kl=0.25667536  nell=0.97336024  nelbo=535.6048\n",
      "i=3200  kl=0.36794147  nell=0.91614407  nelbo=504.24719\n",
      "i=3300  kl=0.38029578  nell=0.88068074  nelbo=484.75473\n",
      "i=3400  kl=0.44015628  nell=0.85761392  nelbo=472.12781\n",
      "i=3500  kl=0.46825051  nell=0.7722863  nelbo=425.22574\n",
      "i=3600  kl=0.52459395  nell=0.66057122  nelbo=363.83878\n",
      "i=3700  kl=0.50556594  nell=0.63410473  nelbo=349.26315\n",
      "i=3800  kl=0.46232873  nell=0.60888171  nelbo=335.34729\n",
      "i=3900  kl=0.60634136  nell=0.59175879  nelbo=326.0737\n",
      "i=4000  kl=0.45179713  nell=0.58245885  nelbo=320.80414\n",
      "i=4100  kl=0.47843435  nell=0.5712952  nelbo=314.6908\n",
      "i=4200  kl=0.42793521  nell=0.56561339  nelbo=311.51532\n",
      "i=4300  kl=0.36232567  nell=0.55961448  nelbo=308.1503\n",
      "i=4400  kl=0.40470195  nell=0.5656575  nelbo=311.51633\n",
      "i=4500  kl=0.39614737  nell=0.55668986  nelbo=306.57556\n",
      "i=4600  kl=0.4529824  nell=0.54105657  nelbo=298.03409\n",
      "i=4700  kl=0.46143031  nell=0.54159033  nelbo=298.33612\n",
      "i=4800  kl=0.380595  nell=0.53336573  nelbo=293.73172\n",
      "i=4900  kl=0.49263969  nell=0.5273751  nelbo=290.54895\n",
      "0.8661\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40487623  nell=3.7610855  nelbo=2069.0017\n",
      "i=100  kl=0.012763292  nell=2.5673923  nelbo=1412.0786\n",
      "i=200  kl=0.0024033487  nell=2.4512503  nelbo=1348.1901\n",
      "i=300  kl=0.00085803866  nell=2.3622317  nelbo=1299.2283\n",
      "i=400  kl=0.0012028217  nell=2.357306  nelbo=1296.5195\n",
      "i=500  kl=0.0013866723  nell=2.3526604  nelbo=1293.9646\n",
      "i=600  kl=0.00076860189  nell=2.335717  nelbo=1284.645\n",
      "i=700  kl=0.00076407194  nell=2.3366673  nelbo=1285.1677\n",
      "i=800  kl=0.001691401  nell=2.3222926  nelbo=1277.2626\n",
      "i=900  kl=0.0010777712  nell=2.3226094  nelbo=1277.4363\n",
      "i=1000  kl=0.0017136037  nell=2.3146696  nelbo=1273.0699\n",
      "i=1100  kl=0.0012540817  nell=2.3191648  nelbo=1275.5419\n",
      "i=1200  kl=0.0011963844  nell=2.3168225  nelbo=1274.2537\n",
      "i=1300  kl=0.0020878315  nell=2.3139679  nelbo=1272.6844\n",
      "i=1400  kl=0.0013838708  nell=2.309835  nelbo=1270.4105\n",
      "i=1500  kl=0.003454268  nell=2.3049176  nelbo=1267.7081\n",
      "i=1600  kl=0.055281848  nell=2.0548382  nelbo=1130.2163\n",
      "i=1700  kl=0.097657859  nell=2.0248241  nelbo=1113.751\n",
      "i=1800  kl=0.089481205  nell=2.0085025  nelbo=1104.7659\n",
      "i=1900  kl=0.052276403  nell=1.9970194  nelbo=1098.413\n",
      "i=2000  kl=0.081985593  nell=1.9798348  nelbo=1088.9912\n",
      "i=2100  kl=0.15503988  nell=1.8376513  nelbo=1010.8632\n",
      "i=2200  kl=0.26438686  nell=1.6732323  nelbo=920.54218\n",
      "i=2300  kl=0.3505173  nell=1.5794798  nelbo=869.06439\n",
      "i=2400  kl=0.56506097  nell=1.4203527  nelbo=781.75903\n",
      "i=2500  kl=0.51119769  nell=1.2166258  nelbo=669.65533\n",
      "i=2600  kl=0.70934093  nell=1.0430939  nelbo=574.41101\n",
      "i=2700  kl=0.68368292  nell=0.9097603  nelbo=501.05185\n",
      "i=2800  kl=0.60475111  nell=0.84483922  nelbo=465.2663\n",
      "i=2900  kl=0.52488625  nell=0.79020751  nelbo=435.13901\n",
      "i=3000  kl=0.66629028  nell=0.76230752  nelbo=419.93542\n",
      "i=3100  kl=0.38698798  nell=0.72487801  nelbo=399.06992\n",
      "i=3200  kl=0.57415032  nell=0.68527234  nelbo=377.47394\n",
      "i=3300  kl=0.51340365  nell=0.61119902  nelbo=336.67285\n",
      "i=3400  kl=0.75683075  nell=0.5603193  nelbo=308.93246\n",
      "i=3500  kl=0.48415148  nell=0.52217168  nelbo=287.67859\n",
      "i=3600  kl=0.54991424  nell=0.50539607  nelbo=278.51776\n",
      "i=3700  kl=0.53492677  nell=0.48117226  nelbo=265.17966\n",
      "i=3800  kl=0.86270428  nell=0.46795368  nelbo=258.23721\n",
      "i=3900  kl=0.4567813  nell=0.45330304  nelbo=249.77345\n",
      "i=4000  kl=0.65311629  nell=0.4419044  nelbo=243.70055\n",
      "i=4100  kl=0.47253725  nell=0.42837843  nelbo=236.08067\n",
      "i=4200  kl=0.50137115  nell=0.4175795  nelbo=230.1701\n",
      "i=4300  kl=0.45695493  nell=0.42028108  nelbo=231.61156\n",
      "i=4400  kl=0.87476444  nell=0.40721068  nelbo=224.84064\n",
      "i=4500  kl=0.63167083  nell=0.3994976  nelbo=220.35535\n",
      "i=4600  kl=0.57775754  nell=0.39062712  nelbo=215.42267\n",
      "i=4700  kl=0.69651473  nell=0.39944655  nelbo=220.39212\n",
      "i=4800  kl=0.54928845  nell=0.38847554  nelbo=214.21083\n",
      "i=4900  kl=0.74351627  nell=0.38370633  nelbo=211.782\n",
      "0.9034\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.41307092  nell=4.0774603  nelbo=2243.0164\n",
      "i=100  kl=0.023146302  nell=2.7363048  nelbo=1504.9908\n",
      "i=200  kl=0.0048297942  nell=2.4252973  nelbo=1333.9183\n",
      "i=300  kl=0.0023159683  nell=2.3810675  nelbo=1309.5895\n",
      "i=400  kl=0.0021038353  nell=2.3753421  nelbo=1306.4403\n",
      "i=500  kl=0.0023694932  nell=2.3523958  nelbo=1293.8199\n",
      "i=600  kl=0.0012452602  nell=2.3467257  nelbo=1290.7003\n",
      "i=700  kl=0.00090840459  nell=2.3342929  nelbo=1283.8619\n",
      "i=800  kl=0.0014319122  nell=2.3260868  nelbo=1279.3492\n",
      "i=900  kl=0.00095486641  nell=2.3189051  nelbo=1275.3988\n",
      "i=1000  kl=0.00072363019  nell=2.32341  nelbo=1277.8762\n",
      "i=1100  kl=0.0011562705  nell=2.3174789  nelbo=1274.6145\n",
      "i=1200  kl=0.0013821721  nell=2.3109155  nelbo=1271.0049\n",
      "i=1300  kl=0.0036362112  nell=2.3148322  nelbo=1273.1614\n",
      "i=1400  kl=0.002412647  nell=2.3137445  nelbo=1272.5619\n",
      "i=1500  kl=0.0027191937  nell=2.3131518  nelbo=1272.2362\n",
      "i=1600  kl=0.0019071698  nell=2.3109798  nelbo=1271.0409\n",
      "i=1700  kl=0.003613621  nell=2.3094106  nelbo=1270.1794\n",
      "i=1800  kl=0.0020056665  nell=2.3085735  nelbo=1269.7174\n",
      "i=1900  kl=0.0028785169  nell=2.3059871  nelbo=1268.2959\n",
      "i=2000  kl=0.0065725446  nell=2.306076  nelbo=1268.3484\n",
      "i=2100  kl=0.0055891871  nell=2.3074031  nelbo=1269.0773\n",
      "i=2200  kl=0.0064718127  nell=2.3062968  nelbo=1268.4697\n",
      "i=2300  kl=0.0045077205  nell=2.3050454  nelbo=1267.7794\n",
      "i=2400  kl=0.013353169  nell=2.3069799  nelbo=1268.8523\n",
      "i=2500  kl=0.0035472512  nell=2.3064837  nelbo=1268.5696\n",
      "i=2600  kl=0.0080041885  nell=2.3046279  nelbo=1267.5533\n",
      "i=2700  kl=0.0044919252  nell=2.3056476  nelbo=1268.1107\n",
      "i=2800  kl=0.010489285  nell=2.3041072  nelbo=1267.2694\n",
      "i=2900  kl=0.017656446  nell=2.3042541  nelbo=1267.3574\n",
      "i=3000  kl=0.0067665577  nell=2.3038421  nelbo=1267.1199\n",
      "i=3100  kl=0.018592507  nell=2.3022895  nelbo=1266.2778\n",
      "i=3200  kl=0.015156865  nell=2.3030269  nelbo=1266.6799\n",
      "i=3300  kl=0.47396865  nell=2.0852418  nelbo=1147.3569\n",
      "i=3400  kl=0.49587467  nell=1.9495201  nelbo=1072.7319\n",
      "i=3500  kl=0.30462429  nell=1.794926  nelbo=987.51398\n",
      "i=3600  kl=0.3093563  nell=1.7160451  nelbo=944.13416\n",
      "i=3700  kl=0.44269219  nell=1.6730807  nelbo=920.63708\n",
      "i=3800  kl=0.29805097  nell=1.6491808  nelbo=907.34747\n",
      "i=3900  kl=0.321538  nell=1.6159723  nelbo=889.10626\n",
      "i=4000  kl=0.40584511  nell=1.5526485  nelbo=854.36255\n",
      "i=4100  kl=0.85396785  nell=1.4847893  nelbo=817.48804\n",
      "i=4200  kl=0.6194272  nell=1.422062  nelbo=782.75354\n",
      "i=4300  kl=0.95694375  nell=1.351704  nelbo=744.39417\n",
      "i=4400  kl=0.5565362  nell=1.3009375  nelbo=716.07214\n",
      "i=4500  kl=0.83025521  nell=1.1221281  nelbo=618.00073\n",
      "i=4600  kl=0.7586329  nell=0.99394333  nelbo=547.42743\n",
      "i=4700  kl=0.6395753  nell=0.89885587  nelbo=495.01031\n",
      "i=4800  kl=0.72443765  nell=0.86107576  nelbo=474.3161\n",
      "i=4900  kl=0.52302229  nell=0.82860535  nelbo=456.25595\n",
      "i=5000  kl=0.60943305  nell=0.79382652  nelbo=437.21402\n",
      "i=5100  kl=0.43406454  nell=0.76943034  nelbo=423.62073\n",
      "i=5200  kl=0.59163713  nell=0.74606025  nelbo=410.92477\n",
      "i=5300  kl=0.46735671  nell=0.71491563  nelbo=393.67096\n",
      "i=5400  kl=0.56303048  nell=0.68450528  nelbo=377.04092\n",
      "i=5500  kl=0.58656013  nell=0.67014325  nelbo=369.16534\n",
      "i=5600  kl=0.76906967  nell=0.66327965  nelbo=365.57288\n",
      "i=5700  kl=0.73364103  nell=0.64929825  nelbo=357.84769\n",
      "i=5800  kl=0.55436563  nell=0.64597738  nelbo=355.84192\n",
      "i=5900  kl=0.46572948  nell=0.64401788  nelbo=354.67557\n",
      "i=6000  kl=0.66972154  nell=0.62134707  nelbo=342.41058\n",
      "i=6100  kl=0.87180156  nell=0.61155856  nelbo=337.229\n",
      "i=6200  kl=0.5293752  nell=0.60608226  nelbo=333.87463\n",
      "i=6300  kl=0.70501751  nell=0.59299982  nelbo=326.85492\n",
      "i=6400  kl=0.70464277  nell=0.585724  nelbo=322.85284\n",
      "i=6500  kl=0.56753653  nell=0.57003677  nelbo=314.08777\n",
      "i=6600  kl=1.0913408  nell=0.55772758  nelbo=307.84149\n",
      "i=6700  kl=0.83501297  nell=0.55329192  nelbo=305.14557\n",
      "i=6800  kl=0.51784229  nell=0.54695314  nelbo=301.34207\n",
      "i=6900  kl=0.71239489  nell=0.53523129  nelbo=295.0896\n",
      "i=7000  kl=1.1033146  nell=0.52521503  nelbo=289.97156\n",
      "i=7100  kl=0.78174353  nell=0.51644957  nelbo=284.82901\n",
      "i=7200  kl=0.52754205  nell=0.51424432  nelbo=283.36191\n",
      "i=7300  kl=1.0004497  nell=0.51364654  nelbo=283.50604\n",
      "i=7400  kl=1.0273237  nell=0.50846195  nelbo=280.6814\n",
      "i=7500  kl=0.6804868  nell=0.49609858  nelbo=273.5347\n",
      "i=7600  kl=1.0776412  nell=0.49924088  nelbo=275.66013\n",
      "i=7700  kl=0.80139422  nell=0.49967122  nelbo=275.62057\n",
      "i=7800  kl=0.78781617  nell=0.49627057  nelbo=273.73663\n",
      "i=7900  kl=1.0873618  nell=0.4886609  nelbo=269.85086\n",
      "i=8000  kl=0.60732919  nell=0.48510736  nelbo=267.41638\n",
      "i=8100  kl=0.83686519  nell=0.47754067  nelbo=263.48422\n",
      "i=8200  kl=0.89124805  nell=0.48048419  nelbo=265.15753\n",
      "i=8300  kl=1.1302853  nell=0.47751108  nelbo=263.76138\n",
      "i=8400  kl=1.1002082  nell=0.47343326  nelbo=261.48853\n",
      "i=8500  kl=1.0487852  nell=0.47159189  nelbo=260.42435\n",
      "i=8600  kl=0.70836174  nell=0.48048726  nelbo=264.97638\n",
      "i=8700  kl=0.98770601  nell=0.4657827  nelbo=257.16818\n",
      "i=8800  kl=1.0021315  nell=0.46680602  nelbo=257.74545\n",
      "i=8900  kl=1.1901791  nell=0.47073621  nelbo=260.09509\n",
      "i=9000  kl=1.2053242  nell=0.46253139  nelbo=255.59758\n",
      "i=9100  kl=0.74809951  nell=0.46223384  nelbo=254.9767\n",
      "i=9200  kl=0.84873909  nell=0.46535  nelbo=256.79123\n",
      "i=9300  kl=1.1171606  nell=0.45482159  nelbo=251.26903\n",
      "i=9400  kl=0.67318052  nell=0.45441595  nelbo=250.60196\n",
      "i=9500  kl=1.5023484  nell=0.45340022  nelbo=250.87247\n",
      "i=9600  kl=1.2696513  nell=0.45327392  nelbo=250.57031\n",
      "i=9700  kl=0.86977357  nell=0.44536772  nelbo=245.82202\n",
      "i=9800  kl=0.92380846  nell=0.44932207  nelbo=248.05095\n",
      "i=9900  kl=0.92155278  nell=0.4513936  nelbo=249.18803\n",
      "0.8475\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
