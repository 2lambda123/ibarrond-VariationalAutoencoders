{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0H9P55wcuZIxfduAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d2e66e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[10].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalInference(object):\n",
    "    def __init__(self, n_datapoints, neurons_per_layer, mc_samples, batch_size, constant_prior=False):\n",
    "        # SIZES\n",
    "        self.N = n_datapoints\n",
    "        self.layers = len(neurons_per_layer)\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.M = batch_size\n",
    "        ## Set the number of Monte Carlo samples as a placeholder so that it can be different for training and test\n",
    "        # self.L =  tf.placeholder(tf.int32)\n",
    "        self.L = mc_samples\n",
    "        \n",
    "        self.constant_prior = constant_prior\n",
    "        \n",
    "        ## Batch data placeholders\n",
    "        with tf.name_scope('input'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[0]], name='x-input')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[-1]], name='y-input')\n",
    "            \n",
    "        with tf.name_scope('input_reshape'):\n",
    "            image_shaped_input = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            tf.summary.image('input', image_shaped_input, 10)\n",
    "        \n",
    "        # PRIOR OF WEIGHTS\n",
    "        self.prior_mean_W, self.log_prior_var_W = self.get_prior_W(constant_prior)\n",
    "    \n",
    "        # POSTERIOR OF WEIGHTS\n",
    "        self.mean_W, self.log_var_W = self.init_posterior_W()\n",
    "        ## Builds whole computational graph with relevant quantities as part of the class\n",
    "        # self.loss, self.kl, self.ell, self.layer_out = self.get_nelbo()\n",
    "        self.loss, self.kl, self.ell = self.get_nelbo()\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.Session()\n",
    "    \n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def get_prior_W(self, constant_prior):\n",
    "        \"\"\"\n",
    "        Define a prior for the weight distribution.\n",
    "        We assume them to be standard normal iid.\n",
    "        \"\"\"\n",
    "        prior_mean_W = []\n",
    "        log_prior_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            \n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_prior_weights\"):\n",
    "                if constant_prior:\n",
    "                    prior_mean = tf.constant(0.0, shape=[d_in, d_out])\n",
    "                    log_prior_var = tf.constant(0.0, shape=[d_in, d_out])\n",
    "                else:\n",
    "                    prior_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                    log_prior_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                    \n",
    "                tf.summary.histogram('prior_mean', tf.reshape(prior_mean, [-1]))\n",
    "                tf.summary.histogram('prior_logvar', tf.reshape(log_prior_var, [-1]))\n",
    "            \n",
    "            prior_mean_W.append(prior_mean)\n",
    "            log_prior_var_W.append(log_prior_var)\n",
    "        \n",
    "        return prior_mean_W, log_prior_var_W\n",
    "\n",
    "    def init_posterior_W(self):\n",
    "        \"\"\"\n",
    "        The (variational) posterior is assumed to be\n",
    "        drawn from P mutually independent normal distributions.\n",
    "        Hence, we have a diagonal covariance matrix and only need to store an array.\n",
    "        \"\"\"\n",
    "        mean_W = []\n",
    "        log_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "\n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_posterior_weights\"):\n",
    "                post_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                post_log_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                tf.summary.histogram('posterior_mean', tf.reshape(post_mean, [-1]))\n",
    "                tf.summary.histogram('posterior_logvar', tf.reshape(post_log_var, [-1]))\n",
    "            \n",
    "            mean_W.append(post_mean)\n",
    "            log_var_W.append(post_log_var)\n",
    "            \n",
    "        return mean_W, log_var_W\n",
    "    \n",
    "    def get_std_norm_samples(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        Draws N(0,1) samples of dimension [d_in, d_out].\n",
    "        \"\"\"\n",
    "        return tf.random_normal(shape=[d_in, d_out])\n",
    "\n",
    "    def sample_from_W(self):\n",
    "        \"\"\"\n",
    "        Samples from the variational posterior approximation.\n",
    "        We draw W-samples for each layer using the reparameterization trick.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            z = self.get_std_norm_samples(d_in, d_out)\n",
    "            ## division by 2 to obtain pure standard deviation\n",
    "            w_from_q = tf.add(tf.multiply(z, tf.exp(self.log_var_W[i] / 2)), self.mean_W[i])\n",
    "        \n",
    "            yield w_from_q\n",
    "    \n",
    "    def feedforward(self):\n",
    "        \"\"\"\n",
    "        Feedforward pass excluding last layer's transfer function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We will generate L output samples\n",
    "        for i in range(self.L):\n",
    "            \n",
    "            inputs = self.X\n",
    "            \n",
    "            # Go through each layer (one weight matrix at a time)\n",
    "            # and compute the (intermediate) output\n",
    "            j = 0\n",
    "            for weight_matrix in self.sample_from_W():\n",
    "                activations = tf.matmul(inputs, weight_matrix[1:,:]) + weight_matrix[0,:]\n",
    "                # tf.summary.histogram('activations', activations)\n",
    "\n",
    "                # if last layer is reached, do not use transfer function (softmax later on)\n",
    "                if j == (self.layers - 2):\n",
    "                    outputs = tf.sigmoid(activations)\n",
    "                else:\n",
    "                    outputs = tf.nn.softmax(activations)\n",
    "\n",
    "                #outputs = tf.sigmoid(activations)\n",
    "                #tf.summary.histogram('outputs', outputs)\n",
    "\n",
    "                inputs = outputs\n",
    "                j += 1\n",
    "                \n",
    "            # use generator to save memory space\n",
    "            yield outputs\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict using monte carlo sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_output = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            expected_output += output\n",
    "            \n",
    "        return expected_output / self.L\n",
    "    \n",
    "    def get_ell(self):\n",
    "        \"\"\"\n",
    "        Returns the expected log-likelihood of the lower bound.\n",
    "        For this we draw L samples from W, compute the log-likelihood for each\n",
    "        and average the log-likelihoods in the end (expectation approximation).\n",
    "        \"\"\"\n",
    "        \n",
    "        log_p = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            # y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]) + b)\n",
    "            # log_p_per_sample = tf.reduce_mean(tf.reduce_sum(self.Y * tf.log(y), reduction_indices=[1]))\n",
    "            # soft_max_cross_entropy_with_logits is a numerically stable version of cross entropy\n",
    "            # log_p_per_sample = tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.Y))\n",
    "            \n",
    "            log_p_per_sample = tf.reduce_mean(tf.reduce_sum(\n",
    "                                    self.Y * tf.log(output + 1e-10) + (1 - self.Y) * tf.log(1 - output + 1e-10),\n",
    "                                    reduction_indices=[1]))\n",
    "            log_p += log_p_per_sample\n",
    "        \n",
    "        return log_p / self.L\n",
    "\n",
    "    def get_kl(self, mean_W, log_var_W, prior_mean_W, log_prior_var_W):\n",
    "        \"\"\"\n",
    "        KL[q || p] returns the KL-divergence between the prior p and the variational posterior q.\n",
    "        :param mq: vector of means for q\n",
    "        :param log_vq: vector of log-variances for q\n",
    "        :param mp: vector of means for p\n",
    "        :param log_vp: vector of log-variances for p\n",
    "        :return: KL divergence between q and p\n",
    "        \"\"\"\n",
    "        mq = mean_W\n",
    "        log_vq = log_var_W\n",
    "        mp = prior_mean_W\n",
    "        log_vp = log_prior_var_W\n",
    "        \n",
    "        #log_vp = tf.reshape(log_vp, (-1, 1))\n",
    "        return 0.5 * tf.reduce_sum(log_vp - log_vq + (tf.pow(mq - mp, 2) / tf.exp(log_vp)) + tf.exp(log_vq - log_vp) - 1)\n",
    "\n",
    "    def get_kl_multi(self):\n",
    "        \"\"\"\n",
    "        Compute KL divergence between variational and prior using a multi-layer-network\n",
    "        \"\"\"\n",
    "        kl = 0\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            kl = kl + self.get_kl(\n",
    "                        self.mean_W[i],\n",
    "                        self.log_var_W[i],\n",
    "                        self.prior_mean_W[i],\n",
    "                        self.log_prior_var_W[i]\n",
    "            )\n",
    "        \n",
    "        return kl\n",
    "    \n",
    "    def get_nelbo(self):\n",
    "        \"\"\" Returns the negative ELBOW, which allows us to minimize instead of maximize. \"\"\"\n",
    "        kl = self.get_kl_multi()\n",
    "        # ell, layer_out = self.get_ell()\n",
    "        ell = self.get_ell()\n",
    "        # DKL_gaussian - tf.mean([log_likelihood(w) for w in w_from_q])\n",
    "        nelbo = kl - self.N/tf.cast(self.M, \"float32\") * ell\n",
    "        # return nelbo, kl, ell, layer_out\n",
    "        return nelbo, kl, ell\n",
    "    \n",
    "    def learn(self, learning_rate=0.01, epochs=50):\n",
    "        \"\"\" Our learning procedure \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        tf.summary.scalar('negative_elbo', self.loss)\n",
    "        tf.summary.scalar('kl_div', self.kl)\n",
    "        tf.summary.scalar('ell', self.ell)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter('logs/train', self.session.graph)\n",
    "        test_writer = tf.summary.FileWriter('logs/test')        \n",
    "        \n",
    "        ## Initialize all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        ## Initialize TF session\n",
    "        self.session.run(init)\n",
    "\n",
    "        ## Set the folder where the logs are going to be written \n",
    "        # summary_writer = tf.train.SummaryWriter('logs/', self.sessihttp://localhost:8888/notebooks/Bayesian%20Neural%20Network.ipynb#on.graph)\n",
    "        #summary_writer = tf.summary.FileWriter('logs/', self.session.graph)\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch: \", i)\n",
    "            train_cost = 0\n",
    "            \n",
    "            for batch_i in range(mnist.train.num_examples // self.M):\n",
    "                batch_xs, _ = mnist.train.next_batch(self.M)\n",
    "\n",
    "                _, loss = self.session.run([train_step, self.loss], feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "                #_, summary, nelbo = self.session.run(\n",
    "                #                        [train_step, merged, self.get_nelbo()],\n",
    "                #                        feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "                #train_writer.add_summary(summary, i)\n",
    "                train_cost += loss\n",
    "            \n",
    "            #summary, nelbo = self.session.run([merged, self.get_nelbo()],\n",
    "            #                                  feed_dict={self.X: mnist.test.images, self.Y: mnist.test.images})\n",
    "            #print(\"i=\" + repr(i)  + \"  kl=\" + repr(nelbo[1]) + \"  nell=\" + repr(-nelbo[2])  + \"  nelbo=\" + repr(nelbo[0]), end=\"\\n\")\n",
    "            print(\"NELBO: \", train_cost / (mnist.train.num_examples // batch_size))\n",
    "            #test_writer.add_summary(summary, i)\n",
    "        \n",
    "        \n",
    "        print('Training time: ', time.time() - start_time)\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "    def benchmark(self, validation=False):\n",
    "        if validation:\n",
    "            benchmark_data = mnist.validation\n",
    "            label = 'Validation loss:'\n",
    "        else:\n",
    "            benchmark_data = mnist.test\n",
    "            label = 'Test loss:'\n",
    "        \n",
    "        cost = 0\n",
    "        for batch_i in range(benchmark_data.num_examples // self.M):\n",
    "            batch_xs, _ = benchmark_data.next_batch(batch_size)\n",
    "            cost += self.session.run(self.loss,\n",
    "                                   feed_dict={self.X: batch_xs, self.Y: batch_xs})[0]\n",
    "        print(label, cost /\n",
    "              (benchmark_data.num_examples // self.M))\n",
    "        \n",
    "    def enc_dec(self, input_vector):\n",
    "        input_vector = np.reshape(input_vector, [1, -1])\n",
    "        output = self.predict()\n",
    "        return self.session.run(output, feed_dict={self.X: input_vector})\n",
    "        \n",
    "    def debug(self):\n",
    "        all_variables = tf.trainable_variables()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session.run(init)\n",
    "        print(self.session.run([self.prior_mean_W, self.log_prior_var_W], feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_datapoints, weight_dim, n_samples, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = mnist.train.num_examples\n",
    "# including input neurons\n",
    "mc_samples = 1\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f0f031dc6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if GPU support is enabled\n",
    "tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "NELBO:  235796.806672\n",
      "Epoch:  1\n",
      "NELBO:  183126.887347\n",
      "Epoch:  2\n",
      "NELBO:  150119.026588\n",
      "Epoch:  3\n",
      "NELBO:  129249.535001\n",
      "Epoch:  4\n",
      "NELBO:  116445.781596\n",
      "Epoch:  5\n",
      "NELBO:  108933.104294\n",
      "Epoch:  6\n",
      "NELBO:  104123.923569\n",
      "Epoch:  7\n",
      "NELBO:  100703.096445\n",
      "Epoch:  8\n",
      "NELBO:  98361.699756\n",
      "Epoch:  9\n",
      "NELBO:  96633.1016353\n",
      "Epoch:  10\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 512, 10, 512, 784]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size, constant_prior=False)\n",
    "vi.learn(learning_rate=0.001, epochs=epochs)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(mnist.test.images[0], [1, -1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWX6pKMsvFWX5paIsv1SU5ZeK+n97O76ggwNIogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37dfad64a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[999].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP1GkGPxEkWLwE0WKwU8Uqf8FZsv2D2VKqq4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37dd4296d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_out = vi.enc_dec(mnist.test.images[999])\n",
    "\n",
    "plt.imshow(img_out.reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40948671  nell=3.7837021  nelbo=2081.4456\n",
      "i=100  kl=0.0097986162  nell=2.527931  nelbo=1390.3718\n",
      "i=200  kl=0.0025273561  nell=2.44379  nelbo=1344.087\n",
      "i=300  kl=0.004008472  nell=2.40974  nelbo=1325.361\n",
      "i=400  kl=0.0078063905  nell=2.2114987  nelbo=1216.3322\n",
      "i=500  kl=0.012652636  nell=2.0823584  nelbo=1145.3098\n",
      "i=600  kl=0.016389519  nell=2.0481839  nelbo=1126.5175\n",
      "i=700  kl=0.01134482  nell=2.0340207  nelbo=1118.7227\n",
      "i=800  kl=0.0098190308  nell=2.0190938  nelbo=1110.5114\n",
      "i=900  kl=0.012732029  nell=2.0130954  nelbo=1107.2152\n",
      "i=1000  kl=0.0087222755  nell=1.9915596  nelbo=1095.3665\n",
      "i=1100  kl=0.026929289  nell=1.9394716  nelbo=1066.7363\n",
      "i=1200  kl=0.034531385  nell=1.8631294  nelbo=1024.7557\n",
      "i=1300  kl=0.063946992  nell=1.7455273  nelbo=960.10394\n",
      "i=1400  kl=0.050398767  nell=1.6979859  nelbo=933.94263\n",
      "i=1500  kl=0.072067857  nell=1.6569531  nelbo=911.3963\n",
      "i=1600  kl=0.07861957  nell=1.6340542  nelbo=898.80841\n",
      "i=1700  kl=0.053618908  nell=1.6419401  nelbo=903.12067\n",
      "i=1800  kl=0.13229337  nell=1.6135145  nelbo=887.56525\n",
      "i=1900  kl=0.096142292  nell=1.6008078  nelbo=880.54041\n",
      "i=2000  kl=0.055015057  nell=1.5997508  nelbo=879.91791\n",
      "i=2100  kl=0.056247681  nell=1.6005688  nelbo=880.36908\n",
      "i=2200  kl=0.11911348  nell=1.5922419  nelbo=875.85217\n",
      "i=2300  kl=0.062289894  nell=1.579706  nelbo=868.90057\n",
      "i=2400  kl=0.11474741  nell=1.5719258  nelbo=864.67389\n",
      "i=2500  kl=0.13185343  nell=1.5364045  nelbo=845.1543\n",
      "i=2600  kl=0.13562751  nell=1.4264808  nelbo=784.70007\n",
      "i=2700  kl=0.21272939  nell=1.335958  nelbo=734.98962\n",
      "i=2800  kl=0.2468037  nell=1.102206  nelbo=606.46014\n",
      "i=2900  kl=0.29848257  nell=1.0523112  nelbo=579.06964\n",
      "i=3000  kl=0.27547702  nell=1.0161655  nelbo=559.1665\n",
      "i=3100  kl=0.25667536  nell=0.97336024  nelbo=535.6048\n",
      "i=3200  kl=0.36794147  nell=0.91614407  nelbo=504.24719\n",
      "i=3300  kl=0.38029578  nell=0.88068074  nelbo=484.75473\n",
      "i=3400  kl=0.44015628  nell=0.85761392  nelbo=472.12781\n",
      "i=3500  kl=0.46825051  nell=0.7722863  nelbo=425.22574\n",
      "i=3600  kl=0.52459395  nell=0.66057122  nelbo=363.83878\n",
      "i=3700  kl=0.50556594  nell=0.63410473  nelbo=349.26315\n",
      "i=3800  kl=0.46232873  nell=0.60888171  nelbo=335.34729\n",
      "i=3900  kl=0.60634136  nell=0.59175879  nelbo=326.0737\n",
      "i=4000  kl=0.45179713  nell=0.58245885  nelbo=320.80414\n",
      "i=4100  kl=0.47843435  nell=0.5712952  nelbo=314.6908\n",
      "i=4200  kl=0.42793521  nell=0.56561339  nelbo=311.51532\n",
      "i=4300  kl=0.36232567  nell=0.55961448  nelbo=308.1503\n",
      "i=4400  kl=0.40470195  nell=0.5656575  nelbo=311.51633\n",
      "i=4500  kl=0.39614737  nell=0.55668986  nelbo=306.57556\n",
      "i=4600  kl=0.4529824  nell=0.54105657  nelbo=298.03409\n",
      "i=4700  kl=0.46143031  nell=0.54159033  nelbo=298.33612\n",
      "i=4800  kl=0.380595  nell=0.53336573  nelbo=293.73172\n",
      "i=4900  kl=0.49263969  nell=0.5273751  nelbo=290.54895\n",
      "0.8661\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40487623  nell=3.7610855  nelbo=2069.0017\n",
      "i=100  kl=0.012763292  nell=2.5673923  nelbo=1412.0786\n",
      "i=200  kl=0.0024033487  nell=2.4512503  nelbo=1348.1901\n",
      "i=300  kl=0.00085803866  nell=2.3622317  nelbo=1299.2283\n",
      "i=400  kl=0.0012028217  nell=2.357306  nelbo=1296.5195\n",
      "i=500  kl=0.0013866723  nell=2.3526604  nelbo=1293.9646\n",
      "i=600  kl=0.00076860189  nell=2.335717  nelbo=1284.645\n",
      "i=700  kl=0.00076407194  nell=2.3366673  nelbo=1285.1677\n",
      "i=800  kl=0.001691401  nell=2.3222926  nelbo=1277.2626\n",
      "i=900  kl=0.0010777712  nell=2.3226094  nelbo=1277.4363\n",
      "i=1000  kl=0.0017136037  nell=2.3146696  nelbo=1273.0699\n",
      "i=1100  kl=0.0012540817  nell=2.3191648  nelbo=1275.5419\n",
      "i=1200  kl=0.0011963844  nell=2.3168225  nelbo=1274.2537\n",
      "i=1300  kl=0.0020878315  nell=2.3139679  nelbo=1272.6844\n",
      "i=1400  kl=0.0013838708  nell=2.309835  nelbo=1270.4105\n",
      "i=1500  kl=0.003454268  nell=2.3049176  nelbo=1267.7081\n",
      "i=1600  kl=0.055281848  nell=2.0548382  nelbo=1130.2163\n",
      "i=1700  kl=0.097657859  nell=2.0248241  nelbo=1113.751\n",
      "i=1800  kl=0.089481205  nell=2.0085025  nelbo=1104.7659\n",
      "i=1900  kl=0.052276403  nell=1.9970194  nelbo=1098.413\n",
      "i=2000  kl=0.081985593  nell=1.9798348  nelbo=1088.9912\n",
      "i=2100  kl=0.15503988  nell=1.8376513  nelbo=1010.8632\n",
      "i=2200  kl=0.26438686  nell=1.6732323  nelbo=920.54218\n",
      "i=2300  kl=0.3505173  nell=1.5794798  nelbo=869.06439\n",
      "i=2400  kl=0.56506097  nell=1.4203527  nelbo=781.75903\n",
      "i=2500  kl=0.51119769  nell=1.2166258  nelbo=669.65533\n",
      "i=2600  kl=0.70934093  nell=1.0430939  nelbo=574.41101\n",
      "i=2700  kl=0.68368292  nell=0.9097603  nelbo=501.05185\n",
      "i=2800  kl=0.60475111  nell=0.84483922  nelbo=465.2663\n",
      "i=2900  kl=0.52488625  nell=0.79020751  nelbo=435.13901\n",
      "i=3000  kl=0.66629028  nell=0.76230752  nelbo=419.93542\n",
      "i=3100  kl=0.38698798  nell=0.72487801  nelbo=399.06992\n",
      "i=3200  kl=0.57415032  nell=0.68527234  nelbo=377.47394\n",
      "i=3300  kl=0.51340365  nell=0.61119902  nelbo=336.67285\n",
      "i=3400  kl=0.75683075  nell=0.5603193  nelbo=308.93246\n",
      "i=3500  kl=0.48415148  nell=0.52217168  nelbo=287.67859\n",
      "i=3600  kl=0.54991424  nell=0.50539607  nelbo=278.51776\n",
      "i=3700  kl=0.53492677  nell=0.48117226  nelbo=265.17966\n",
      "i=3800  kl=0.86270428  nell=0.46795368  nelbo=258.23721\n",
      "i=3900  kl=0.4567813  nell=0.45330304  nelbo=249.77345\n",
      "i=4000  kl=0.65311629  nell=0.4419044  nelbo=243.70055\n",
      "i=4100  kl=0.47253725  nell=0.42837843  nelbo=236.08067\n",
      "i=4200  kl=0.50137115  nell=0.4175795  nelbo=230.1701\n",
      "i=4300  kl=0.45695493  nell=0.42028108  nelbo=231.61156\n",
      "i=4400  kl=0.87476444  nell=0.40721068  nelbo=224.84064\n",
      "i=4500  kl=0.63167083  nell=0.3994976  nelbo=220.35535\n",
      "i=4600  kl=0.57775754  nell=0.39062712  nelbo=215.42267\n",
      "i=4700  kl=0.69651473  nell=0.39944655  nelbo=220.39212\n",
      "i=4800  kl=0.54928845  nell=0.38847554  nelbo=214.21083\n",
      "i=4900  kl=0.74351627  nell=0.38370633  nelbo=211.782\n",
      "0.9034\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.41307092  nell=4.0774603  nelbo=2243.0164\n",
      "i=100  kl=0.023146302  nell=2.7363048  nelbo=1504.9908\n",
      "i=200  kl=0.0048297942  nell=2.4252973  nelbo=1333.9183\n",
      "i=300  kl=0.0023159683  nell=2.3810675  nelbo=1309.5895\n",
      "i=400  kl=0.0021038353  nell=2.3753421  nelbo=1306.4403\n",
      "i=500  kl=0.0023694932  nell=2.3523958  nelbo=1293.8199\n",
      "i=600  kl=0.0012452602  nell=2.3467257  nelbo=1290.7003\n",
      "i=700  kl=0.00090840459  nell=2.3342929  nelbo=1283.8619\n",
      "i=800  kl=0.0014319122  nell=2.3260868  nelbo=1279.3492\n",
      "i=900  kl=0.00095486641  nell=2.3189051  nelbo=1275.3988\n",
      "i=1000  kl=0.00072363019  nell=2.32341  nelbo=1277.8762\n",
      "i=1100  kl=0.0011562705  nell=2.3174789  nelbo=1274.6145\n",
      "i=1200  kl=0.0013821721  nell=2.3109155  nelbo=1271.0049\n",
      "i=1300  kl=0.0036362112  nell=2.3148322  nelbo=1273.1614\n",
      "i=1400  kl=0.002412647  nell=2.3137445  nelbo=1272.5619\n",
      "i=1500  kl=0.0027191937  nell=2.3131518  nelbo=1272.2362\n",
      "i=1600  kl=0.0019071698  nell=2.3109798  nelbo=1271.0409\n",
      "i=1700  kl=0.003613621  nell=2.3094106  nelbo=1270.1794\n",
      "i=1800  kl=0.0020056665  nell=2.3085735  nelbo=1269.7174\n",
      "i=1900  kl=0.0028785169  nell=2.3059871  nelbo=1268.2959\n",
      "i=2000  kl=0.0065725446  nell=2.306076  nelbo=1268.3484\n",
      "i=2100  kl=0.0055891871  nell=2.3074031  nelbo=1269.0773\n",
      "i=2200  kl=0.0064718127  nell=2.3062968  nelbo=1268.4697\n",
      "i=2300  kl=0.0045077205  nell=2.3050454  nelbo=1267.7794\n",
      "i=2400  kl=0.013353169  nell=2.3069799  nelbo=1268.8523\n",
      "i=2500  kl=0.0035472512  nell=2.3064837  nelbo=1268.5696\n",
      "i=2600  kl=0.0080041885  nell=2.3046279  nelbo=1267.5533\n",
      "i=2700  kl=0.0044919252  nell=2.3056476  nelbo=1268.1107\n",
      "i=2800  kl=0.010489285  nell=2.3041072  nelbo=1267.2694\n",
      "i=2900  kl=0.017656446  nell=2.3042541  nelbo=1267.3574\n",
      "i=3000  kl=0.0067665577  nell=2.3038421  nelbo=1267.1199\n",
      "i=3100  kl=0.018592507  nell=2.3022895  nelbo=1266.2778\n",
      "i=3200  kl=0.015156865  nell=2.3030269  nelbo=1266.6799\n",
      "i=3300  kl=0.47396865  nell=2.0852418  nelbo=1147.3569\n",
      "i=3400  kl=0.49587467  nell=1.9495201  nelbo=1072.7319\n",
      "i=3500  kl=0.30462429  nell=1.794926  nelbo=987.51398\n",
      "i=3600  kl=0.3093563  nell=1.7160451  nelbo=944.13416\n",
      "i=3700  kl=0.44269219  nell=1.6730807  nelbo=920.63708\n",
      "i=3800  kl=0.29805097  nell=1.6491808  nelbo=907.34747\n",
      "i=3900  kl=0.321538  nell=1.6159723  nelbo=889.10626\n",
      "i=4000  kl=0.40584511  nell=1.5526485  nelbo=854.36255\n",
      "i=4100  kl=0.85396785  nell=1.4847893  nelbo=817.48804\n",
      "i=4200  kl=0.6194272  nell=1.422062  nelbo=782.75354\n",
      "i=4300  kl=0.95694375  nell=1.351704  nelbo=744.39417\n",
      "i=4400  kl=0.5565362  nell=1.3009375  nelbo=716.07214\n",
      "i=4500  kl=0.83025521  nell=1.1221281  nelbo=618.00073\n",
      "i=4600  kl=0.7586329  nell=0.99394333  nelbo=547.42743\n",
      "i=4700  kl=0.6395753  nell=0.89885587  nelbo=495.01031\n",
      "i=4800  kl=0.72443765  nell=0.86107576  nelbo=474.3161\n",
      "i=4900  kl=0.52302229  nell=0.82860535  nelbo=456.25595\n",
      "i=5000  kl=0.60943305  nell=0.79382652  nelbo=437.21402\n",
      "i=5100  kl=0.43406454  nell=0.76943034  nelbo=423.62073\n",
      "i=5200  kl=0.59163713  nell=0.74606025  nelbo=410.92477\n",
      "i=5300  kl=0.46735671  nell=0.71491563  nelbo=393.67096\n",
      "i=5400  kl=0.56303048  nell=0.68450528  nelbo=377.04092\n",
      "i=5500  kl=0.58656013  nell=0.67014325  nelbo=369.16534\n",
      "i=5600  kl=0.76906967  nell=0.66327965  nelbo=365.57288\n",
      "i=5700  kl=0.73364103  nell=0.64929825  nelbo=357.84769\n",
      "i=5800  kl=0.55436563  nell=0.64597738  nelbo=355.84192\n",
      "i=5900  kl=0.46572948  nell=0.64401788  nelbo=354.67557\n",
      "i=6000  kl=0.66972154  nell=0.62134707  nelbo=342.41058\n",
      "i=6100  kl=0.87180156  nell=0.61155856  nelbo=337.229\n",
      "i=6200  kl=0.5293752  nell=0.60608226  nelbo=333.87463\n",
      "i=6300  kl=0.70501751  nell=0.59299982  nelbo=326.85492\n",
      "i=6400  kl=0.70464277  nell=0.585724  nelbo=322.85284\n",
      "i=6500  kl=0.56753653  nell=0.57003677  nelbo=314.08777\n",
      "i=6600  kl=1.0913408  nell=0.55772758  nelbo=307.84149\n",
      "i=6700  kl=0.83501297  nell=0.55329192  nelbo=305.14557\n",
      "i=6800  kl=0.51784229  nell=0.54695314  nelbo=301.34207\n",
      "i=6900  kl=0.71239489  nell=0.53523129  nelbo=295.0896\n",
      "i=7000  kl=1.1033146  nell=0.52521503  nelbo=289.97156\n",
      "i=7100  kl=0.78174353  nell=0.51644957  nelbo=284.82901\n",
      "i=7200  kl=0.52754205  nell=0.51424432  nelbo=283.36191\n",
      "i=7300  kl=1.0004497  nell=0.51364654  nelbo=283.50604\n",
      "i=7400  kl=1.0273237  nell=0.50846195  nelbo=280.6814\n",
      "i=7500  kl=0.6804868  nell=0.49609858  nelbo=273.5347\n",
      "i=7600  kl=1.0776412  nell=0.49924088  nelbo=275.66013\n",
      "i=7700  kl=0.80139422  nell=0.49967122  nelbo=275.62057\n",
      "i=7800  kl=0.78781617  nell=0.49627057  nelbo=273.73663\n",
      "i=7900  kl=1.0873618  nell=0.4886609  nelbo=269.85086\n",
      "i=8000  kl=0.60732919  nell=0.48510736  nelbo=267.41638\n",
      "i=8100  kl=0.83686519  nell=0.47754067  nelbo=263.48422\n",
      "i=8200  kl=0.89124805  nell=0.48048419  nelbo=265.15753\n",
      "i=8300  kl=1.1302853  nell=0.47751108  nelbo=263.76138\n",
      "i=8400  kl=1.1002082  nell=0.47343326  nelbo=261.48853\n",
      "i=8500  kl=1.0487852  nell=0.47159189  nelbo=260.42435\n",
      "i=8600  kl=0.70836174  nell=0.48048726  nelbo=264.97638\n",
      "i=8700  kl=0.98770601  nell=0.4657827  nelbo=257.16818\n",
      "i=8800  kl=1.0021315  nell=0.46680602  nelbo=257.74545\n",
      "i=8900  kl=1.1901791  nell=0.47073621  nelbo=260.09509\n",
      "i=9000  kl=1.2053242  nell=0.46253139  nelbo=255.59758\n",
      "i=9100  kl=0.74809951  nell=0.46223384  nelbo=254.9767\n",
      "i=9200  kl=0.84873909  nell=0.46535  nelbo=256.79123\n",
      "i=9300  kl=1.1171606  nell=0.45482159  nelbo=251.26903\n",
      "i=9400  kl=0.67318052  nell=0.45441595  nelbo=250.60196\n",
      "i=9500  kl=1.5023484  nell=0.45340022  nelbo=250.87247\n",
      "i=9600  kl=1.2696513  nell=0.45327392  nelbo=250.57031\n",
      "i=9700  kl=0.86977357  nell=0.44536772  nelbo=245.82202\n",
      "i=9800  kl=0.92380846  nell=0.44932207  nelbo=248.05095\n",
      "i=9900  kl=0.92155278  nell=0.4513936  nelbo=249.18803\n",
      "0.8475\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
