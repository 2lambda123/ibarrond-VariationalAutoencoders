{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncuZIxfduAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f207cc0bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[10].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalInference(object):\n",
    "    def __init__(self, n_datapoints, neurons_per_layer, mc_samples, batch_size):\n",
    "        # SIZES\n",
    "        self.N = n_datapoints\n",
    "        self.layers = len(neurons_per_layer)\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.M = batch_size\n",
    "        ## Set the number of Monte Carlo samples as a placeholder so that it can be different for training and test\n",
    "        # self.L =  tf.placeholder(tf.int32)\n",
    "        self.L = mc_samples\n",
    "        \n",
    "        ## Batch data placeholders\n",
    "        with tf.name_scope('input'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[0]], name='x-input')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, neurons_per_layer[-1]], name='y-input')\n",
    "            \n",
    "        with tf.name_scope('input_reshape'):\n",
    "            image_shaped_input = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            tf.summary.image('input', image_shaped_input, 10)\n",
    "        \n",
    "        # PRIOR OF WEIGHTS\n",
    "        self.prior_mean_W, self.log_prior_var_W = self.get_prior_W()\n",
    "    \n",
    "        # POSTERIOR OF WEIGHTS\n",
    "        self.mean_W, self.log_var_W = self.init_posterior_W()\n",
    "        ## Builds whole computational graph with relevant quantities as part of the class\n",
    "        # self.loss, self.kl, self.ell, self.layer_out = self.get_nelbo()\n",
    "        self.loss, self.kl, self.ell = self.get_nelbo()\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.Session()\n",
    "    \n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def get_prior_W(self):\n",
    "        \"\"\"\n",
    "        Define a prior for the weight distribution.\n",
    "        We assume them to be standard normal iid.\n",
    "        \"\"\"\n",
    "        prior_mean_W = []\n",
    "        log_prior_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            \n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_general\"):\n",
    "                with tf.name_scope('prior_weights'):\n",
    "                    prior_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                    log_prior_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"p_W\")\n",
    "                    tf.summary.histogram('prior_mean', tf.reshape(prior_mean, [-1]))\n",
    "                    tf.summary.histogram('prior_logvar', tf.reshape(log_prior_var, [-1]))\n",
    "            \n",
    "            prior_mean_W.append(prior_mean)\n",
    "            log_prior_var_W.append(log_prior_var)\n",
    "        \n",
    "        return prior_mean_W, log_prior_var_W\n",
    "\n",
    "    def init_posterior_W(self):\n",
    "        \"\"\"\n",
    "        The (variational) posterior is assumed to be\n",
    "        drawn from P mutually independent normal distributions.\n",
    "        Hence, we have a diagonal covariance matrix and only need to store an array.\n",
    "        \"\"\"\n",
    "        mean_W = []\n",
    "        log_var_W = []\n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "\n",
    "            with tf.name_scope(\"layer_\" + str(i+1) + \"_general\"):\n",
    "                with tf.name_scope('posterior_weights'):\n",
    "                    post_mean = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                    post_log_var = tf.Variable(tf.zeros([d_in, d_out]), name=\"q_W\")\n",
    "                    tf.summary.histogram('posterior_mean', tf.reshape(post_mean, [-1]))\n",
    "                    tf.summary.histogram('posterior_logvar', tf.reshape(post_log_var, [-1]))\n",
    "            \n",
    "            mean_W.append(post_mean)\n",
    "            log_var_W.append(post_log_var)\n",
    "            \n",
    "        return mean_W, log_var_W\n",
    "    \n",
    "    def get_samples(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        Draws L N(0,1) samples of dimension P.\n",
    "        \"\"\"\n",
    "        return tf.random_normal(shape=[self.L, d_in, d_out])\n",
    "\n",
    "    def sample_from_W(self):\n",
    "        \"\"\"\n",
    "        Samples from the variational posterior approximation.\n",
    "        We draw L w-samples of dimension P using the reparameterization trick.\n",
    "        \"\"\"\n",
    "        w_from_q = []\n",
    "        for i in range(self.layers - 1):\n",
    "            d_in = self.neurons_per_layer[i] + 1 # + 1 because of bias weight\n",
    "            d_out = self.neurons_per_layer[i+1]\n",
    "            z = self.get_samples(d_in, d_out)\n",
    "            ## division by 2 to obtain pure standard deviation\n",
    "            w_from_q.append(tf.add(tf.multiply(z, tf.exp(self.log_var_W[i] / 2)), self.mean_W[i]))\n",
    "        return w_from_q\n",
    "    \n",
    "    def feedforward(self):\n",
    "        \"\"\"\n",
    "        Feedforward pass excluding last layer's transfer function.\n",
    "        \"\"\"\n",
    "        \n",
    "        W_sample = self.sample_from_W()\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            inputs = self.X\n",
    "            \n",
    "            for j in range(self.layers - 1):\n",
    "                with tf.name_scope(\"layer_\" + str(j+1) + \"_sample_\" + str(i + 1)):\n",
    "                    with tf.name_scope(\"activations\"):\n",
    "                        activations = tf.matmul(inputs, W_sample[j][i,1:,:]) + \\\n",
    "                                        tf.reshape(W_sample[j][i,0,:], [1,-1])\n",
    "                        tf.summary.histogram('activations', activations)\n",
    "                        \n",
    "                    with tf.name_scope(\"outputs\"):\n",
    "                        # if last layer is reached, do not use transfer function (softmax later on)\n",
    "                        if j == (self.layers - 2):\n",
    "                            outputs = activations\n",
    "                        else:\n",
    "                            outputs = tf.sigmoid(activations)\n",
    "                            \n",
    "                        tf.summary.histogram('outputs', outputs)\n",
    "                    \n",
    "                    inputs = outputs\n",
    "                \n",
    "            # use generator to save memory space\n",
    "            yield outputs\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict using monte carlo sampling.\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_output = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            expected_output += tf.nn.softmax(output)\n",
    "            \n",
    "        return expected_output / self.L\n",
    "    \n",
    "    def get_ell(self):\n",
    "        \"\"\"\n",
    "        Returns the expected log-likelihood of the lower bound.\n",
    "        For this we draw L samples from W, compute the log-likelihood for each\n",
    "        and average the log-likelihoods in the end (expectation approximation).\n",
    "        \"\"\"\n",
    "        \n",
    "        log_p = 0\n",
    "        \n",
    "        for output in self.feedforward():\n",
    "            #y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]) + b)\n",
    "            # y = tf.nn.softmax(tf.matmul(self.X, W_sample[i]))\n",
    "            # log_p_per_sample = tf.reduce_mean(tf.reduce_sum(self.Y * tf.log(y), reduction_indices=[1]))\n",
    "            # output = tf.matmul(self.X, W_sample[i,1:,:]) + tf.reshape(W_sample[i,0,:], [1,-1])\n",
    "            # soft_max_cross_entropy_with_logits is a numerically stable version of cross entropy\n",
    "            log_p_per_sample = tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.Y))\n",
    "            log_p += log_p_per_sample\n",
    "        \n",
    "        return log_p / self.L\n",
    "\n",
    "    def get_kl(self, mean_W, log_var_W, prior_mean_W, log_prior_var_W):\n",
    "        \"\"\"\n",
    "        KL[q || p] returns the KL-divergence between the prior p and the variational posterior q.\n",
    "        :param mq: vector of means for q\n",
    "        :param log_vq: vector of log-variances for q\n",
    "        :param mp: vector of means for p\n",
    "        :param log_vp: vector of log-variances for p\n",
    "        :return: KL divergence between q and p\n",
    "        \"\"\"\n",
    "        mq = mean_W\n",
    "        log_vq = log_var_W\n",
    "        mp = prior_mean_W\n",
    "        log_vp = log_prior_var_W\n",
    "        \n",
    "        #log_vp = tf.reshape(log_vp, (-1, 1))\n",
    "        return 0.5 * tf.reduce_sum(log_vp - log_vq + (tf.pow(mq - mp, 2) / tf.exp(log_vp)) + tf.exp(log_vq - log_vp) - 1)\n",
    "\n",
    "    def get_kl_multi(self):\n",
    "        \"\"\"\n",
    "        Compute KL divergence between variational and prior using a multi-layer-network\n",
    "        \"\"\"\n",
    "        kl = 0\n",
    "        for i in range(self.layers - 1):\n",
    "            kl = kl + self.get_kl(self.mean_W[i], self.log_var_W[i], self.prior_mean_W[i], self.log_prior_var_W[i])\n",
    "        return kl\n",
    "    \n",
    "    def get_nelbo(self):\n",
    "        \"\"\" Returns the negative ELBOW, which allows us to minimize instead of maximize. \"\"\"\n",
    "        kl = self.get_kl_multi()\n",
    "        # ell, layer_out = self.get_ell()\n",
    "        ell = self.get_ell()\n",
    "        # DKL_gaussian - tf.mean([log_likelihood(w) for w in w_from_q])\n",
    "        nelbo = kl - self.N/tf.cast(self.M, \"float32\") * ell\n",
    "        # return nelbo, kl, ell, layer_out\n",
    "        return nelbo, kl, ell\n",
    "    \n",
    "    def learn(self, learning_rate=0.01, iterations=5000):\n",
    "        \"\"\" Our learning procedure \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        tf.summary.scalar('negative_elbo', self.loss)\n",
    "        tf.summary.scalar('kl_div', self.kl)\n",
    "        tf.summary.scalar('ell', self.ell)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter('logs/train', self.session.graph)\n",
    "        test_writer = tf.summary.FileWriter('logs/test')        \n",
    "        \n",
    "        ## Initialize all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        ## Initialize TF session\n",
    "        self.session.run(init)\n",
    "\n",
    "        ## Set the folder where the logs are going to be written \n",
    "        # summary_writer = tf.train.SummaryWriter('logs/', self.sessihttp://localhost:8888/notebooks/Bayesian%20Neural%20Network.ipynb#on.graph)\n",
    "        #summary_writer = tf.summary.FileWriter('logs/', self.session.graph)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(self.M)\n",
    "            _, summary = self.session.run([train_step, merged], feed_dict={self.X: batch_xs, self.Y: batch_ys})\n",
    "            train_writer.add_summary(summary, i)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                summary, nelbo, prior_mean_W = self.session.run([merged, self.get_nelbo(), self.prior_mean_W[0]],\n",
    "                                feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels})\n",
    "                print(\"i=\" + repr(i)  + \"  kl=\" + repr(nelbo[1]) + \"  nell=\" + repr(-nelbo[2])  + \"  nelbo=\" + repr(nelbo[0]), end=\"\\n\")\n",
    "                print(\"Prior means max: \", prior_mean_W.max())\n",
    "                print(\"Prior means min: \", prior_mean_W.min())\n",
    "        \n",
    "                test_writer.add_summary(summary, i)\n",
    "        mean_W_final = self.session.run(self.mean_W)\n",
    "        log_var_W_final = self.session.run(self.log_var_W)\n",
    "        \n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "        return mean_W_final, log_var_W_final\n",
    "    \n",
    "    def benchmark(self):\n",
    "        output = self.predict()\n",
    "        correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(self.Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(self.session.run(accuracy, feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))\n",
    "        \n",
    "    def debug(self):\n",
    "        all_variables = tf.trainable_variables()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session.run(init)\n",
    "        print(self.session.run([self.prior_mean_W, self.log_prior_var_W], feed_dict={self.X: mnist.test.images, self.Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_datapoints, weight_dim, n_samples, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_datapoints = mnist.train.num_examples\n",
    "# including input neurons\n",
    "mc_samples = 10\n",
    "batch_size = 100\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.39947671  nell=14.220953  nelbo=7821.9233\n",
      "Prior means max:  0.0\n",
      "Prior means min:  0.0\n",
      "i=100  kl=0.075237483  nell=3.1465797  nelbo=1730.6941\n",
      "Prior means max:  0.699626\n",
      "Prior means min:  -0.692911\n",
      "i=200  kl=0.044884026  nell=2.121242  nelbo=1166.728\n",
      "Prior means max:  1.14588\n",
      "Prior means min:  -1.12578\n",
      "i=300  kl=0.048678845  nell=1.8928223  nelbo=1041.101\n",
      "Prior means max:  1.46619\n",
      "Prior means min:  -1.43015\n",
      "i=400  kl=0.050823182  nell=1.5886823  nelbo=873.82611\n",
      "Prior means max:  1.76452\n",
      "Prior means min:  -1.71433\n",
      "i=500  kl=0.04948774  nell=1.4615352  nelbo=803.89386\n",
      "Prior means max:  2.04013\n",
      "Prior means min:  -1.93318\n",
      "i=600  kl=0.035393387  nell=1.3485022  nelbo=741.71161\n",
      "Prior means max:  2.21347\n",
      "Prior means min:  -2.16536\n",
      "i=700  kl=0.046927571  nell=1.2438221  nelbo=684.14911\n",
      "Prior means max:  2.39376\n",
      "Prior means min:  -2.30934\n",
      "i=800  kl=0.055631578  nell=1.2779105  nelbo=702.90637\n",
      "Prior means max:  2.61807\n",
      "Prior means min:  -2.47361\n",
      "i=900  kl=0.080535829  nell=1.0906807  nelbo=599.9549\n",
      "Prior means max:  2.79018\n",
      "Prior means min:  -2.50735\n",
      "0.9184\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40948671  nell=3.7837021  nelbo=2081.4456\n",
      "i=100  kl=0.0097986162  nell=2.527931  nelbo=1390.3718\n",
      "i=200  kl=0.0025273561  nell=2.44379  nelbo=1344.087\n",
      "i=300  kl=0.004008472  nell=2.40974  nelbo=1325.361\n",
      "i=400  kl=0.0078063905  nell=2.2114987  nelbo=1216.3322\n",
      "i=500  kl=0.012652636  nell=2.0823584  nelbo=1145.3098\n",
      "i=600  kl=0.016389519  nell=2.0481839  nelbo=1126.5175\n",
      "i=700  kl=0.01134482  nell=2.0340207  nelbo=1118.7227\n",
      "i=800  kl=0.0098190308  nell=2.0190938  nelbo=1110.5114\n",
      "i=900  kl=0.012732029  nell=2.0130954  nelbo=1107.2152\n",
      "i=1000  kl=0.0087222755  nell=1.9915596  nelbo=1095.3665\n",
      "i=1100  kl=0.026929289  nell=1.9394716  nelbo=1066.7363\n",
      "i=1200  kl=0.034531385  nell=1.8631294  nelbo=1024.7557\n",
      "i=1300  kl=0.063946992  nell=1.7455273  nelbo=960.10394\n",
      "i=1400  kl=0.050398767  nell=1.6979859  nelbo=933.94263\n",
      "i=1500  kl=0.072067857  nell=1.6569531  nelbo=911.3963\n",
      "i=1600  kl=0.07861957  nell=1.6340542  nelbo=898.80841\n",
      "i=1700  kl=0.053618908  nell=1.6419401  nelbo=903.12067\n",
      "i=1800  kl=0.13229337  nell=1.6135145  nelbo=887.56525\n",
      "i=1900  kl=0.096142292  nell=1.6008078  nelbo=880.54041\n",
      "i=2000  kl=0.055015057  nell=1.5997508  nelbo=879.91791\n",
      "i=2100  kl=0.056247681  nell=1.6005688  nelbo=880.36908\n",
      "i=2200  kl=0.11911348  nell=1.5922419  nelbo=875.85217\n",
      "i=2300  kl=0.062289894  nell=1.579706  nelbo=868.90057\n",
      "i=2400  kl=0.11474741  nell=1.5719258  nelbo=864.67389\n",
      "i=2500  kl=0.13185343  nell=1.5364045  nelbo=845.1543\n",
      "i=2600  kl=0.13562751  nell=1.4264808  nelbo=784.70007\n",
      "i=2700  kl=0.21272939  nell=1.335958  nelbo=734.98962\n",
      "i=2800  kl=0.2468037  nell=1.102206  nelbo=606.46014\n",
      "i=2900  kl=0.29848257  nell=1.0523112  nelbo=579.06964\n",
      "i=3000  kl=0.27547702  nell=1.0161655  nelbo=559.1665\n",
      "i=3100  kl=0.25667536  nell=0.97336024  nelbo=535.6048\n",
      "i=3200  kl=0.36794147  nell=0.91614407  nelbo=504.24719\n",
      "i=3300  kl=0.38029578  nell=0.88068074  nelbo=484.75473\n",
      "i=3400  kl=0.44015628  nell=0.85761392  nelbo=472.12781\n",
      "i=3500  kl=0.46825051  nell=0.7722863  nelbo=425.22574\n",
      "i=3600  kl=0.52459395  nell=0.66057122  nelbo=363.83878\n",
      "i=3700  kl=0.50556594  nell=0.63410473  nelbo=349.26315\n",
      "i=3800  kl=0.46232873  nell=0.60888171  nelbo=335.34729\n",
      "i=3900  kl=0.60634136  nell=0.59175879  nelbo=326.0737\n",
      "i=4000  kl=0.45179713  nell=0.58245885  nelbo=320.80414\n",
      "i=4100  kl=0.47843435  nell=0.5712952  nelbo=314.6908\n",
      "i=4200  kl=0.42793521  nell=0.56561339  nelbo=311.51532\n",
      "i=4300  kl=0.36232567  nell=0.55961448  nelbo=308.1503\n",
      "i=4400  kl=0.40470195  nell=0.5656575  nelbo=311.51633\n",
      "i=4500  kl=0.39614737  nell=0.55668986  nelbo=306.57556\n",
      "i=4600  kl=0.4529824  nell=0.54105657  nelbo=298.03409\n",
      "i=4700  kl=0.46143031  nell=0.54159033  nelbo=298.33612\n",
      "i=4800  kl=0.380595  nell=0.53336573  nelbo=293.73172\n",
      "i=4900  kl=0.49263969  nell=0.5273751  nelbo=290.54895\n",
      "0.8661\n"
     ]
    }
   ],
   "source": [
    "# n_datapoints, n_layers, neurons_per_layer, mc_samples, batch_size\n",
    "neurons_per_layer = [784, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.40487623  nell=3.7610855  nelbo=2069.0017\n",
      "i=100  kl=0.012763292  nell=2.5673923  nelbo=1412.0786\n",
      "i=200  kl=0.0024033487  nell=2.4512503  nelbo=1348.1901\n",
      "i=300  kl=0.00085803866  nell=2.3622317  nelbo=1299.2283\n",
      "i=400  kl=0.0012028217  nell=2.357306  nelbo=1296.5195\n",
      "i=500  kl=0.0013866723  nell=2.3526604  nelbo=1293.9646\n",
      "i=600  kl=0.00076860189  nell=2.335717  nelbo=1284.645\n",
      "i=700  kl=0.00076407194  nell=2.3366673  nelbo=1285.1677\n",
      "i=800  kl=0.001691401  nell=2.3222926  nelbo=1277.2626\n",
      "i=900  kl=0.0010777712  nell=2.3226094  nelbo=1277.4363\n",
      "i=1000  kl=0.0017136037  nell=2.3146696  nelbo=1273.0699\n",
      "i=1100  kl=0.0012540817  nell=2.3191648  nelbo=1275.5419\n",
      "i=1200  kl=0.0011963844  nell=2.3168225  nelbo=1274.2537\n",
      "i=1300  kl=0.0020878315  nell=2.3139679  nelbo=1272.6844\n",
      "i=1400  kl=0.0013838708  nell=2.309835  nelbo=1270.4105\n",
      "i=1500  kl=0.003454268  nell=2.3049176  nelbo=1267.7081\n",
      "i=1600  kl=0.055281848  nell=2.0548382  nelbo=1130.2163\n",
      "i=1700  kl=0.097657859  nell=2.0248241  nelbo=1113.751\n",
      "i=1800  kl=0.089481205  nell=2.0085025  nelbo=1104.7659\n",
      "i=1900  kl=0.052276403  nell=1.9970194  nelbo=1098.413\n",
      "i=2000  kl=0.081985593  nell=1.9798348  nelbo=1088.9912\n",
      "i=2100  kl=0.15503988  nell=1.8376513  nelbo=1010.8632\n",
      "i=2200  kl=0.26438686  nell=1.6732323  nelbo=920.54218\n",
      "i=2300  kl=0.3505173  nell=1.5794798  nelbo=869.06439\n",
      "i=2400  kl=0.56506097  nell=1.4203527  nelbo=781.75903\n",
      "i=2500  kl=0.51119769  nell=1.2166258  nelbo=669.65533\n",
      "i=2600  kl=0.70934093  nell=1.0430939  nelbo=574.41101\n",
      "i=2700  kl=0.68368292  nell=0.9097603  nelbo=501.05185\n",
      "i=2800  kl=0.60475111  nell=0.84483922  nelbo=465.2663\n",
      "i=2900  kl=0.52488625  nell=0.79020751  nelbo=435.13901\n",
      "i=3000  kl=0.66629028  nell=0.76230752  nelbo=419.93542\n",
      "i=3100  kl=0.38698798  nell=0.72487801  nelbo=399.06992\n",
      "i=3200  kl=0.57415032  nell=0.68527234  nelbo=377.47394\n",
      "i=3300  kl=0.51340365  nell=0.61119902  nelbo=336.67285\n",
      "i=3400  kl=0.75683075  nell=0.5603193  nelbo=308.93246\n",
      "i=3500  kl=0.48415148  nell=0.52217168  nelbo=287.67859\n",
      "i=3600  kl=0.54991424  nell=0.50539607  nelbo=278.51776\n",
      "i=3700  kl=0.53492677  nell=0.48117226  nelbo=265.17966\n",
      "i=3800  kl=0.86270428  nell=0.46795368  nelbo=258.23721\n",
      "i=3900  kl=0.4567813  nell=0.45330304  nelbo=249.77345\n",
      "i=4000  kl=0.65311629  nell=0.4419044  nelbo=243.70055\n",
      "i=4100  kl=0.47253725  nell=0.42837843  nelbo=236.08067\n",
      "i=4200  kl=0.50137115  nell=0.4175795  nelbo=230.1701\n",
      "i=4300  kl=0.45695493  nell=0.42028108  nelbo=231.61156\n",
      "i=4400  kl=0.87476444  nell=0.40721068  nelbo=224.84064\n",
      "i=4500  kl=0.63167083  nell=0.3994976  nelbo=220.35535\n",
      "i=4600  kl=0.57775754  nell=0.39062712  nelbo=215.42267\n",
      "i=4700  kl=0.69651473  nell=0.39944655  nelbo=220.39212\n",
      "i=4800  kl=0.54928845  nell=0.38847554  nelbo=214.21083\n",
      "i=4900  kl=0.74351627  nell=0.38370633  nelbo=211.782\n",
      "0.9034\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0  kl=0.41307092  nell=4.0774603  nelbo=2243.0164\n",
      "i=100  kl=0.023146302  nell=2.7363048  nelbo=1504.9908\n",
      "i=200  kl=0.0048297942  nell=2.4252973  nelbo=1333.9183\n",
      "i=300  kl=0.0023159683  nell=2.3810675  nelbo=1309.5895\n",
      "i=400  kl=0.0021038353  nell=2.3753421  nelbo=1306.4403\n",
      "i=500  kl=0.0023694932  nell=2.3523958  nelbo=1293.8199\n",
      "i=600  kl=0.0012452602  nell=2.3467257  nelbo=1290.7003\n",
      "i=700  kl=0.00090840459  nell=2.3342929  nelbo=1283.8619\n",
      "i=800  kl=0.0014319122  nell=2.3260868  nelbo=1279.3492\n",
      "i=900  kl=0.00095486641  nell=2.3189051  nelbo=1275.3988\n",
      "i=1000  kl=0.00072363019  nell=2.32341  nelbo=1277.8762\n",
      "i=1100  kl=0.0011562705  nell=2.3174789  nelbo=1274.6145\n",
      "i=1200  kl=0.0013821721  nell=2.3109155  nelbo=1271.0049\n",
      "i=1300  kl=0.0036362112  nell=2.3148322  nelbo=1273.1614\n",
      "i=1400  kl=0.002412647  nell=2.3137445  nelbo=1272.5619\n",
      "i=1500  kl=0.0027191937  nell=2.3131518  nelbo=1272.2362\n",
      "i=1600  kl=0.0019071698  nell=2.3109798  nelbo=1271.0409\n",
      "i=1700  kl=0.003613621  nell=2.3094106  nelbo=1270.1794\n",
      "i=1800  kl=0.0020056665  nell=2.3085735  nelbo=1269.7174\n",
      "i=1900  kl=0.0028785169  nell=2.3059871  nelbo=1268.2959\n",
      "i=2000  kl=0.0065725446  nell=2.306076  nelbo=1268.3484\n",
      "i=2100  kl=0.0055891871  nell=2.3074031  nelbo=1269.0773\n",
      "i=2200  kl=0.0064718127  nell=2.3062968  nelbo=1268.4697\n",
      "i=2300  kl=0.0045077205  nell=2.3050454  nelbo=1267.7794\n",
      "i=2400  kl=0.013353169  nell=2.3069799  nelbo=1268.8523\n",
      "i=2500  kl=0.0035472512  nell=2.3064837  nelbo=1268.5696\n",
      "i=2600  kl=0.0080041885  nell=2.3046279  nelbo=1267.5533\n",
      "i=2700  kl=0.0044919252  nell=2.3056476  nelbo=1268.1107\n",
      "i=2800  kl=0.010489285  nell=2.3041072  nelbo=1267.2694\n",
      "i=2900  kl=0.017656446  nell=2.3042541  nelbo=1267.3574\n",
      "i=3000  kl=0.0067665577  nell=2.3038421  nelbo=1267.1199\n",
      "i=3100  kl=0.018592507  nell=2.3022895  nelbo=1266.2778\n",
      "i=3200  kl=0.015156865  nell=2.3030269  nelbo=1266.6799\n",
      "i=3300  kl=0.47396865  nell=2.0852418  nelbo=1147.3569\n",
      "i=3400  kl=0.49587467  nell=1.9495201  nelbo=1072.7319\n",
      "i=3500  kl=0.30462429  nell=1.794926  nelbo=987.51398\n",
      "i=3600  kl=0.3093563  nell=1.7160451  nelbo=944.13416\n",
      "i=3700  kl=0.44269219  nell=1.6730807  nelbo=920.63708\n",
      "i=3800  kl=0.29805097  nell=1.6491808  nelbo=907.34747\n",
      "i=3900  kl=0.321538  nell=1.6159723  nelbo=889.10626\n",
      "i=4000  kl=0.40584511  nell=1.5526485  nelbo=854.36255\n",
      "i=4100  kl=0.85396785  nell=1.4847893  nelbo=817.48804\n",
      "i=4200  kl=0.6194272  nell=1.422062  nelbo=782.75354\n",
      "i=4300  kl=0.95694375  nell=1.351704  nelbo=744.39417\n",
      "i=4400  kl=0.5565362  nell=1.3009375  nelbo=716.07214\n",
      "i=4500  kl=0.83025521  nell=1.1221281  nelbo=618.00073\n",
      "i=4600  kl=0.7586329  nell=0.99394333  nelbo=547.42743\n",
      "i=4700  kl=0.6395753  nell=0.89885587  nelbo=495.01031\n",
      "i=4800  kl=0.72443765  nell=0.86107576  nelbo=474.3161\n",
      "i=4900  kl=0.52302229  nell=0.82860535  nelbo=456.25595\n",
      "i=5000  kl=0.60943305  nell=0.79382652  nelbo=437.21402\n",
      "i=5100  kl=0.43406454  nell=0.76943034  nelbo=423.62073\n",
      "i=5200  kl=0.59163713  nell=0.74606025  nelbo=410.92477\n",
      "i=5300  kl=0.46735671  nell=0.71491563  nelbo=393.67096\n",
      "i=5400  kl=0.56303048  nell=0.68450528  nelbo=377.04092\n",
      "i=5500  kl=0.58656013  nell=0.67014325  nelbo=369.16534\n",
      "i=5600  kl=0.76906967  nell=0.66327965  nelbo=365.57288\n",
      "i=5700  kl=0.73364103  nell=0.64929825  nelbo=357.84769\n",
      "i=5800  kl=0.55436563  nell=0.64597738  nelbo=355.84192\n",
      "i=5900  kl=0.46572948  nell=0.64401788  nelbo=354.67557\n",
      "i=6000  kl=0.66972154  nell=0.62134707  nelbo=342.41058\n",
      "i=6100  kl=0.87180156  nell=0.61155856  nelbo=337.229\n",
      "i=6200  kl=0.5293752  nell=0.60608226  nelbo=333.87463\n",
      "i=6300  kl=0.70501751  nell=0.59299982  nelbo=326.85492\n",
      "i=6400  kl=0.70464277  nell=0.585724  nelbo=322.85284\n",
      "i=6500  kl=0.56753653  nell=0.57003677  nelbo=314.08777\n",
      "i=6600  kl=1.0913408  nell=0.55772758  nelbo=307.84149\n",
      "i=6700  kl=0.83501297  nell=0.55329192  nelbo=305.14557\n",
      "i=6800  kl=0.51784229  nell=0.54695314  nelbo=301.34207\n",
      "i=6900  kl=0.71239489  nell=0.53523129  nelbo=295.0896\n",
      "i=7000  kl=1.1033146  nell=0.52521503  nelbo=289.97156\n",
      "i=7100  kl=0.78174353  nell=0.51644957  nelbo=284.82901\n",
      "i=7200  kl=0.52754205  nell=0.51424432  nelbo=283.36191\n",
      "i=7300  kl=1.0004497  nell=0.51364654  nelbo=283.50604\n",
      "i=7400  kl=1.0273237  nell=0.50846195  nelbo=280.6814\n",
      "i=7500  kl=0.6804868  nell=0.49609858  nelbo=273.5347\n",
      "i=7600  kl=1.0776412  nell=0.49924088  nelbo=275.66013\n",
      "i=7700  kl=0.80139422  nell=0.49967122  nelbo=275.62057\n",
      "i=7800  kl=0.78781617  nell=0.49627057  nelbo=273.73663\n",
      "i=7900  kl=1.0873618  nell=0.4886609  nelbo=269.85086\n",
      "i=8000  kl=0.60732919  nell=0.48510736  nelbo=267.41638\n",
      "i=8100  kl=0.83686519  nell=0.47754067  nelbo=263.48422\n",
      "i=8200  kl=0.89124805  nell=0.48048419  nelbo=265.15753\n",
      "i=8300  kl=1.1302853  nell=0.47751108  nelbo=263.76138\n",
      "i=8400  kl=1.1002082  nell=0.47343326  nelbo=261.48853\n",
      "i=8500  kl=1.0487852  nell=0.47159189  nelbo=260.42435\n",
      "i=8600  kl=0.70836174  nell=0.48048726  nelbo=264.97638\n",
      "i=8700  kl=0.98770601  nell=0.4657827  nelbo=257.16818\n",
      "i=8800  kl=1.0021315  nell=0.46680602  nelbo=257.74545\n",
      "i=8900  kl=1.1901791  nell=0.47073621  nelbo=260.09509\n",
      "i=9000  kl=1.2053242  nell=0.46253139  nelbo=255.59758\n",
      "i=9100  kl=0.74809951  nell=0.46223384  nelbo=254.9767\n",
      "i=9200  kl=0.84873909  nell=0.46535  nelbo=256.79123\n",
      "i=9300  kl=1.1171606  nell=0.45482159  nelbo=251.26903\n",
      "i=9400  kl=0.67318052  nell=0.45441595  nelbo=250.60196\n",
      "i=9500  kl=1.5023484  nell=0.45340022  nelbo=250.87247\n",
      "i=9600  kl=1.2696513  nell=0.45327392  nelbo=250.57031\n",
      "i=9700  kl=0.86977357  nell=0.44536772  nelbo=245.82202\n",
      "i=9800  kl=0.92380846  nell=0.44932207  nelbo=248.05095\n",
      "i=9900  kl=0.92155278  nell=0.4513936  nelbo=249.18803\n",
      "0.8475\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [784, 10, 10, 10, 10]\n",
    "vi = VariationalInference(n_datapoints, neurons_per_layer, mc_samples, batch_size)\n",
    "mu, log_var_W_final = vi.learn(learning_rate=0.01, iterations=iterations)\n",
    "vi.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
