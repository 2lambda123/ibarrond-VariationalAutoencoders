{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, name,\n",
    "                 n_inputs=784,\n",
    "                 n_neurons_encoder = [2048, 256],\n",
    "                 n_latent=2,\n",
    "                 n_neurons_decoder = [256, 2048],\n",
    "                 batch_size = 128,\n",
    "                 activation = tf.nn.tanh):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        # ATTRIBUTES\n",
    "        self.N = n_inputs\n",
    "        self.n_encoder = n_neurons_encoder\n",
    "        self.n_decoder = n_neurons_decoder\n",
    "        self.n_latent = n_latent\n",
    "        self.length_encoder = len(n_neurons_encoder)\n",
    "        self.length_decoder = len(n_neurons_decoder)\n",
    "        self.layers = self.length_encoder + self.length_decoder + 1 \n",
    "        self.activ = activation\n",
    "        \n",
    "        ## DATA PLACEHOLDERS (BATCHES)\n",
    "        with tf.name_scope('input'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, self.N], name='x-input')\n",
    "            \n",
    "        with tf.name_scope('input_reshape'):\n",
    "            image_shaped_input = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            tf.summary.image('input', image_shaped_input, 10)\n",
    "        \n",
    "        # INITIALIZE WEIGHTS & BIASES\n",
    "        self.W_enc, self.W_z_mu, self.W_z_log_sigma, self.W_dec = self.initialize_W()\n",
    "        self.b_enc, self.b_z_mu, self.b_z_log_sigma, self.b_dec = self.initialize_b()\n",
    "        \n",
    "        for w, b in zip(self.W_enc, self.b_enc):\n",
    "            print(w.get_shape(), b.get_shape())\n",
    "          \n",
    "        print(self.W_z_mu.get_shape(), self.b_z_mu.get_shape())\n",
    "        print(self.W_z_log_sigma.get_shape(), self.b_z_log_sigma.get_shape())\n",
    "        \n",
    "        for w, b in zip(self.W_dec, self.b_dec):\n",
    "            print(w.get_shape(), b.get_shape())\n",
    "            \n",
    "        ## COMPUTATIONAL GRAPH\n",
    "        self.Y, self.z_mu, self.z_log_sigma = self.feedforward(self.X)\n",
    "        self.loss, self.kl, self.ell = self.get_nelbo(self.z_mu, self.z_log_o, self.X, self.Y)\n",
    "\n",
    "        ## Initialize the session\n",
    "        self.session = tf.InteractiveSession()\n",
    "    \n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "\n",
    "    ## ---------------------------------------------------------------------            \n",
    "    ## --------------- TF WEIGHTS & BIASES INITIALIZATION ------------------\n",
    "    ## ---------------------------------------------------------------------\n",
    "    \n",
    "    def initialize_W(self):\n",
    "        \"\"\"\n",
    "        Define all the weights for the network.\n",
    "        We initialize them to standard normal iid using Xavier Initializer\n",
    "        \"\"\"\n",
    "        \n",
    "        W_encoder = []\n",
    "        W_latent_mu = []\n",
    "        W_latent_log_sigma = []\n",
    "        W_decoder = []\n",
    "        \n",
    "        with tf.name_scope(\"Encoder_layer_weights\"):\n",
    "            W_encoder.append(w_xavier(shape=[self.N, self.n_encoder[0]]))\n",
    "            for i in range(1, self.length_encoder):\n",
    "                W_encoder.append(w_xavier(shape=[self.n_encoder[i-1], self.n_encoder[i]]))\n",
    "        \n",
    "        with tf.name_scope(\"Latent_layer_weights\"):\n",
    "            W_latent_mu = w_xavier(shape =[self.n_encoder[-1], self.n_latent]) \n",
    "            W_latent_log_sigma = w_xavier(shape =[self.n_encoder[-1], self.n_latent])      \n",
    "            \n",
    "        with tf.name_scope(\"Decoder_layer_weights\"):\n",
    "            W_decoder.append(w_xavier(shape=[self.n_latent, self.n_decoder[0]]))\n",
    "            for i in range(2, self.length_encoder):\n",
    "                W_decoder.append(w_xavier(shape=[self.n_decoder[i-1], self.n_decoder[i]]))\n",
    "            W_decoder.append(w_xavier(shape=[self.n_decoder[-1], self.N]))\n",
    "        \n",
    "        return W_encoder, W_latent_mu, W_latent_log_sigma, W_decoder\n",
    "\n",
    "    \n",
    "    def initialize_b(self):\n",
    "        \"\"\"\n",
    "        Define all the biases for the network.\n",
    "        We initialize them to standard normal iid using Xavier Initializer\n",
    "        \"\"\"\n",
    "        \n",
    "        b_encoder = []\n",
    "        b_latent_mu = []\n",
    "        b_latent_log_sigma = []\n",
    "        b_decoder = []\n",
    "        \n",
    "        with tf.name_scope(\"Encoder_layer_biases\"):\n",
    "            b_encoder.append(w_xavier(shape=[self.n_encoder[0]]))\n",
    "            for i in range(1, self.length_encoder):\n",
    "                b_encoder.append(w_xavier(shape=[self.n_encoder[i]]))\n",
    "        \n",
    "        with tf.name_scope(\"Latent_layer_biases\"):\n",
    "            b_latent_mu = w_xavier(shape =[self.n_latent]) \n",
    "            b_latent_log_sigma = w_xavier(shape =[self.n_latent])      \n",
    "            \n",
    "        with tf.name_scope(\"Decoder_layer_biases\"):\n",
    "            b_decoder.append(w_xavier(shape=[self.n_decoder[0]]))\n",
    "            for i in range(2, self.length_encoder):\n",
    "                b_decoder.append(w_xavier(shape=[self.n_decoder[i]]))\n",
    "            b_decoder.append(w_xavier(shape=[self.N]))\n",
    "            \n",
    "        return b_encoder, b_latent_mu, b_latent_log_sigma, b_decoder\n",
    "\n",
    "    \n",
    "    ## ---------------------------------------------------------------------            \n",
    "    ## ----------------- SAMPLING FROM THE LATENT SPACE --------------------\n",
    "    ## ---------------------------------------------------------------------\n",
    "    \n",
    "    def get_samples(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        Sample from noise distribution p(eps) ~ N(0, 1)\n",
    "        a matrix of samples of dimension [d_in, d_out].\n",
    "        \"\"\"\n",
    "        return tf.random_normal(shape=[d_in, d_out])\n",
    "\n",
    "    def sample_from_Z(self, z_mu, z_log_o):\n",
    "        \"\"\"\n",
    "        Samples from the posterior of the variational latent space.\n",
    "        We draw samples using the reparameterization trick.\n",
    "        \"\"\"\n",
    "        return z_mu + tf.exp(z_log_o) * self.get_samples(tf.shape(self.X)[0], self.n_latent)\n",
    "    \n",
    "        \n",
    "        \n",
    "    ## ---------------------------------------------------------------------            \n",
    "    ## --------------------------- FEEDFORWARD -----------------------------\n",
    "    ## ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        \"\"\"\n",
    "        Feedforward pass excluding last layer's transfer function.\n",
    "        intermediate : index of intermediate layer for output generation\n",
    "        \"\"\"\n",
    "        net = X\n",
    "\n",
    "        # ENCODER\n",
    "        for i in range(self.length_encoder):\n",
    "            net = self.activ(tf.matmul(net, self.W_enc[i]) + self.b_enc[i])\n",
    "        \n",
    "        # LATENT\n",
    "        z_mu = tf.matmul(net, self.W_z_mu) + self.b_z_mu\n",
    "        z_log_sigma = 0.5 * (tf.matmul(net, self.W_z_log_sigma) + self.b_z_log_sigma)\n",
    "\n",
    "        # Sample from posterior\n",
    "        z = self.sample_from_Z(z_mu, z_log_sigma)\n",
    "\n",
    "        # DECODER\n",
    "        net = self.activ(tf.matmul(z, self.W_dec[0]) + self.b_dec[0])\n",
    "        for i in range(1, self.length_decoder-1):\n",
    "            net = self.activ(tf.matmul(net, self.W_dec[i]) + self.b_dec[i])\n",
    "        \n",
    "        Y = tf.nn.softmax(tf.matmul(net, self.W_dec[-1]) + self.b_dec[-1])        \n",
    "                \n",
    "        return Y, z_mu, z_log_sigma\n",
    "    \n",
    "    \n",
    "    ## ---------------------------------------------------------------------            \n",
    "    ## ------------------- LOSS: EXPECTED LOWER BOUND ----------------------\n",
    "    ## ---------------------------------------------------------------------    \n",
    "    \n",
    "    def get_ell(self, x, y):\n",
    "        \"\"\"\n",
    "        Returns the expected log-likelihood of the lower bound.\n",
    "        For this we use a bernouilli LL.\n",
    "        \"\"\"\n",
    "        # p(x|z)        \n",
    "        return -tf.reduce_sum(x * tf.log(y + 1e-10) + (1 - x) * tf.log(1 - y + 1e-10), 1)\n",
    "\n",
    "    \n",
    "    def get_kl(self, z_mu, z_log_o):\n",
    "        \"\"\"\n",
    "        d_kl(q(z|x)||p(z)) returns the KL-divergence between the prior p and the variational posterior q.\n",
    "        :return: KL divergence between q and p\n",
    "        \"\"\"        \n",
    "        # Formula: 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        return 0.5 * tf.reduce_sum( 1.0 + 2.0 * z_log_o - tf.square(z_mu) - tf.exp(2.0 * z_log_o), 1)\n",
    "        \n",
    "        \n",
    "    def get_nelbo(self, z_mu, z_log_o, x, y):\n",
    "        \"\"\" Returns the negative ELBOW, which allows us to minimize instead of maximize. \"\"\"\n",
    "        kl = self.get_kl(z_mu, z_log_o)\n",
    "        ell = self.get_ell(x, y)\n",
    "        nelbo = tf.reduce_mean(kl + ell)\n",
    "        return nelbo, kl, ell\n",
    "    \n",
    "    \n",
    "    ## ---------------------------------------------------------------------            \n",
    "    ## --------------------------- LEARNING --------------------------------\n",
    "    ## ---------------------------------------------------------------------  \n",
    "    \n",
    "    def learn(self, learning_rate=0.001, batch_size=128,\n",
    "                    epochs=100, display_step=1):\n",
    "        \"\"\" Our learning procedure \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        ## Set all_variables to contain the complete set of TF variables to optimize\n",
    "        all_variables = tf.trainable_variables()\n",
    "\n",
    "        ## Define the optimizer\n",
    "        train_step = optimizer.minimize(self.loss, var_list=all_variables)\n",
    "\n",
    "        tf.summary.scalar('negative_elbo', self.loss)\n",
    "        tf.summary.scalar('kl_div', self.kl)\n",
    "        tf.summary.scalar('ell', self.ell)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter('logs/train', self.session.graph)\n",
    "        test_writer = tf.summary.FileWriter('logs/test')        \n",
    "        \n",
    "        ## Initialize all variables\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initial model print\n",
    "        print(\"*MODEL [\", self.name,\"] {l_r: %.4f; n_iter: %d; batch: %d}\"%\\\n",
    "              (self.l_r, training_epochs, batch_size))\n",
    "        print (\" -> START Training!\")\n",
    "        \n",
    "        t = time.time()\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_cost = 0.\n",
    "            for batch_i in range(total_batch):\n",
    "                batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                _, loss, summary = self.session.run([train_step, self.loss, merged],\n",
    "                                    feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "                \n",
    "                train_writer.add_summary(summary, epoch * total_batch + batc_i)\n",
    "                train_cost += loss / total_batch\n",
    "            \n",
    "            print(\"   [%.1f] Epoch: %02d | NELBO: %.6f\"%(time.time()-t,\n",
    "                                                         epoch, train_cost))\n",
    "        \n",
    "        print (\" -> Training FINISHED in %.1f seconds.\"%(time.time()-t))\n",
    "        self.serialize('model/')\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "    def benchmark(self, validation=False, batch_size = 128):\n",
    "        if validation:\n",
    "            benchmark_data = mnist.validation\n",
    "            title = 'Validation loss:'\n",
    "        else:\n",
    "            benchmark_data = mnist.test\n",
    "            title = 'Test loss:'\n",
    "        \n",
    "        total_batch = benchmark_data.num_examples // batch_size\n",
    "        cost = 0\n",
    "        for batch_i in range(total_batch):\n",
    "            batch_xs, _ = benchmark_data.next_batch(batch_size)\n",
    "            c = self.session.run(self.loss,\n",
    "                   feed_dict={self.X: batch_xs, self.Y: batch_xs})\n",
    "            cost+= c/total_batch\n",
    "        print(title, cost)\n",
    "        return cost\n",
    "        \n",
    "    def serialize(self, path):\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.session, path)\n",
    "        print(\"Model saved in file: %s\" % save_path+self.name)\n",
    "        \n",
    "    def restore(self, path):\n",
    "        saver = tf.train.Saver()   \n",
    "        sess = tf.InteractiveSession()\n",
    "        saver.restore(sess, save_path=path)\n",
    "        self.session = sess\n",
    "    \n",
    "    def encode(self, input_vector):\n",
    "        _, z = self.session.run(self.feedforward,\n",
    "                                feed_dict={self.X: input_vector})\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "def w_xavier(shape):\n",
    "    initial = tf.random_normal(shape, mean=0.0,\n",
    "                               stddev=tf.sqrt(3./sum(shape)))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# %%\n",
    "def b_xavier(shape):\n",
    "    initial = tf.random_normal(shape, mean=0.0,\n",
    "                               stddev=sqrt(3./sum(shape)))\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 256) (256,)\n",
      "(256, 128) (128,)\n",
      "(128, 2) (2,)\n",
      "(128, 2) (2,)\n",
      "(2, 128) (128,)\n",
      "(256, 784) (784,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 128 and 256 for 'MatMul_35' (op: 'MatMul') with input shapes: [?,128], [256,784].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 128 and 256 for 'MatMul_35' (op: 'MatMul') with input shapes: [?,128], [256,784].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3e7a0ae5fde7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mn_latent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mn_neurons_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                  batch_size = 128)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-63373bcc5411>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, n_inputs, n_neurons_encoder, n_latent, n_neurons_decoder, batch_size, activation)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m## COMPUTATIONAL GRAPH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_log_sigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nelbo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_log_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-63373bcc5411>\u001b[0m in \u001b[0;36mfeedforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_dec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_dec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_dec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_dec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_log_sigma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1800\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[1;32m-> 1801\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   1802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   1261\u001b[0m   \"\"\"\n\u001b[0;32m   1262\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[1;32m-> 1263\u001b[1;33m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   1264\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2338\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1719\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1720\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 128 and 256 for 'MatMul_35' (op: 'MatMul') with input shapes: [?,128], [256,784]."
     ]
    }
   ],
   "source": [
    "VA =  VariationalAutoencoder('VA_1',\n",
    "                 n_inputs=784,\n",
    "                 n_neurons_encoder = [256, 128],\n",
    "                 n_latent=2,\n",
    "                 n_neurons_decoder = [128, 256],\n",
    "                 batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# %%\n",
    "def bias_variable(shape):\n",
    "    initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Variational autoencoder with 2 layer fully-connected\n",
    "encoder/decoders and gaussian noise distribution.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "def VAE(input_shape=[None, 784],\n",
    "        n_components_encoder=2048,\n",
    "        n_components_decoder=2048,\n",
    "        n_hidden=2,\n",
    "        debug=False):\n",
    "    # %%\n",
    "    # Input placeholder\n",
    "    if debug:\n",
    "        input_shape = [50, 784]\n",
    "        x = tf.Variable(np.zeros((input_shape), dtype=np.float32))\n",
    "    else:\n",
    "        x = tf.placeholder(tf.float32, input_shape)\n",
    "\n",
    "    activation = tf.nn.softplus\n",
    "\n",
    "    dims = x.get_shape().as_list()\n",
    "    n_features = dims[1]\n",
    "\n",
    "    W_enc1 = weight_variable([n_features, n_components_encoder])\n",
    "    b_enc1 = bias_variable([n_components_encoder])\n",
    "    h_enc1 = activation(tf.matmul(x, W_enc1) + b_enc1)\n",
    "\n",
    "    W_enc2 = weight_variable([n_components_encoder, n_components_encoder])\n",
    "    b_enc2 = bias_variable([n_components_encoder])\n",
    "    h_enc2 = activation(tf.matmul(h_enc1, W_enc2) + b_enc2)\n",
    "\n",
    "    W_enc3 = weight_variable([n_components_encoder, n_components_encoder])\n",
    "    b_enc3 = bias_variable([n_components_encoder])\n",
    "    h_enc3 = activation(tf.matmul(h_enc2, W_enc3) + b_enc3)\n",
    "\n",
    "    W_mu = weight_variable([n_components_encoder, n_hidden])\n",
    "    b_mu = bias_variable([n_hidden])\n",
    "\n",
    "    W_log_sigma = weight_variable([n_components_encoder, n_hidden])\n",
    "    b_log_sigma = bias_variable([n_hidden])\n",
    "\n",
    "    z_mu = tf.matmul(h_enc3, W_mu) + b_mu\n",
    "    z_log_sigma = 0.5 * (tf.matmul(h_enc3, W_log_sigma) + b_log_sigma)\n",
    "\n",
    "    # %%\n",
    "    # Sample from noise distribution p(eps) ~ N(0, 1)\n",
    "    if debug:\n",
    "        epsilon = tf.random_normal(\n",
    "            [dims[0], n_hidden])\n",
    "    else:\n",
    "        epsilon = tf.random_normal(\n",
    "            tf.stack([tf.shape(x)[0], n_hidden]))\n",
    "\n",
    "    # Sample from posterior\n",
    "    z = z_mu + tf.exp(z_log_sigma) * epsilon\n",
    "\n",
    "    W_dec1 = weight_variable([n_hidden, n_components_decoder])\n",
    "    b_dec1 = bias_variable([n_components_decoder])\n",
    "    h_dec1 = activation(tf.matmul(z, W_dec1) + b_dec1)\n",
    "\n",
    "    W_dec2 = weight_variable([n_components_decoder, n_components_decoder])\n",
    "    b_dec2 = bias_variable([n_components_decoder])\n",
    "    h_dec2 = activation(tf.matmul(h_dec1, W_dec2) + b_dec2)\n",
    "\n",
    "    W_dec3 = weight_variable([n_components_decoder, n_components_decoder])\n",
    "    b_dec3 = bias_variable([n_components_decoder])\n",
    "    h_dec3 = activation(tf.matmul(h_dec2, W_dec3) + b_dec3)\n",
    "\n",
    "    W_mu_dec = weight_variable([n_components_decoder, n_features])\n",
    "    b_mu_dec = bias_variable([n_features])\n",
    "    y = tf.nn.sigmoid(tf.matmul(h_dec3, W_mu_dec) + b_mu_dec)\n",
    "\n",
    "    # p(x|z)\n",
    "    log_px_given_z = -tf.reduce_sum(\n",
    "        x * tf.log(y + 1e-10) +\n",
    "        (1 - x) * tf.log(1 - y + 1e-10), 1)\n",
    "\n",
    "    # d_kl(q(z|x)||p(z))\n",
    "    # Appendix B: 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_div = -0.5 * tf.reduce_sum(\n",
    "        1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),\n",
    "        1)\n",
    "    loss = tf.reduce_mean(log_px_given_z + kl_div)\n",
    "\n",
    "    return {'cost': loss, 'x': x, 'z': z, 'y': y}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def test_mnist():\n",
    "    \"\"\"Summary\n",
    "    Returns\n",
    "    -------\n",
    "    name : TYPE\n",
    "        Description\n",
    "    \"\"\"\n",
    "    # %%\n",
    "\n",
    "\n",
    "    # %%\n",
    "    # load MNIST as before\n",
    "    ae = VAE()\n",
    "\n",
    "    # %%\n",
    "    learning_rate = 0.001\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(ae['cost'])\n",
    "\n",
    "    # %%\n",
    "    # We create a session to use the graph\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # %%\n",
    "    # Fit all training data\n",
    "    t_i = 0\n",
    "    batch_size = 100\n",
    "    n_epochs = 50\n",
    "    n_examples = 20\n",
    "    test_xs, _ = mnist.test.next_batch(n_examples)\n",
    "    xs, ys = mnist.test.images, mnist.test.labels\n",
    "    fig_manifold, ax_manifold = plt.subplots(1, 1)\n",
    "    fig_reconstruction, axs_reconstruction = plt.subplots(2, n_examples, figsize=(10, 2))\n",
    "    fig_image_manifold, ax_image_manifold = plt.subplots(1, 1)\n",
    "    for epoch_i in range(n_epochs):\n",
    "        print('--- Epoch', epoch_i)\n",
    "        train_cost = 0\n",
    "        for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "            train_cost += sess.run([ae['cost'], optimizer],\n",
    "                                   feed_dict={ae['x']: batch_xs})[0]\n",
    "            if batch_i == 1:\n",
    "                # %%\n",
    "                # Plot example reconstructions from latent layer\n",
    "                imgs = []\n",
    "                for img_i in np.linspace(-3, 3, n_examples):\n",
    "                    for img_j in np.linspace(-3, 3, n_examples):\n",
    "                        z = np.array([[img_i, img_j]], dtype=np.float32)\n",
    "                        recon = sess.run(ae['y'], feed_dict={ae['z']: z})\n",
    "                        imgs.append(np.reshape(recon, (1, 28, 28, 1)))\n",
    "                imgs_cat = np.concatenate(imgs)\n",
    "                ax_manifold.imshow(montage_batch(imgs_cat))\n",
    "                fig_manifold.savefig('manifold_%08d.png' % t_i)\n",
    "\n",
    "                # %%\n",
    "                # Plot example reconstructions\n",
    "                recon = sess.run(ae['y'], feed_dict={ae['x']: test_xs})\n",
    "                print(recon.shape)\n",
    "                for example_i in range(n_examples):\n",
    "                    axs_reconstruction[0][example_i].imshow(\n",
    "                        np.reshape(test_xs[example_i, :], (28, 28)),\n",
    "                        cmap='gray')\n",
    "                    axs_reconstruction[1][example_i].imshow(\n",
    "                        np.reshape(\n",
    "                            np.reshape(recon[example_i, ...], (784,)),\n",
    "                            (28, 28)),\n",
    "                        cmap='gray')\n",
    "                    axs_reconstruction[0][example_i].axis('off')\n",
    "                    axs_reconstruction[1][example_i].axis('off')\n",
    "                fig_reconstruction.savefig('reconstruction_%08d.png' % t_i)\n",
    "\n",
    "                # %%\n",
    "                # Plot manifold of latent layer\n",
    "                zs = sess.run(ae['z'], feed_dict={ae['x']: xs})\n",
    "                ax_image_manifold.clear()\n",
    "                ax_image_manifold.scatter(zs[:, 0], zs[:, 1],\n",
    "                    c=np.argmax(ys, 1), alpha=0.2)\n",
    "                ax_image_manifold.set_xlim([-6, 6])\n",
    "                ax_image_manifold.set_ylim([-6, 6])\n",
    "                ax_image_manifold.axis('off')\n",
    "                fig_image_manifold.savefig('image_manifold_%08d.png' % t_i)\n",
    "\n",
    "                t_i += 1\n",
    "\n",
    "\n",
    "        print('Train cost:', train_cost /\n",
    "              (mnist.train.num_examples // batch_size))\n",
    "\n",
    "        valid_cost = 0\n",
    "        for batch_i in range(mnist.validation.num_examples // batch_size):\n",
    "            batch_xs, _ = mnist.validation.next_batch(batch_size)\n",
    "            valid_cost += sess.run([ae['cost']],\n",
    "                                   feed_dict={ae['x']: batch_xs})[0]\n",
    "        print('Validation cost:', valid_cost /\n",
    "              (mnist.validation.num_examples // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def montage_batch(images):\n",
    "    \"\"\"Draws all filters (n_input * n_output filters) as a\n",
    "    montage image separated by 1 pixel borders.\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : numpy.ndarray\n",
    "        Input array to create montage of.\n",
    "    Returns\n",
    "    -------\n",
    "    m : numpy.ndarray\n",
    "        Montage image.\n",
    "    \"\"\"\n",
    "    img_h = images.shape[1]\n",
    "    img_w = images.shape[2]\n",
    "    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    m = np.ones(\n",
    "        (images.shape[1] * n_plots + n_plots + 1,\n",
    "         images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n",
    "\n",
    "    for i in range(n_plots):\n",
    "        for j in range(n_plots):\n",
    "            this_filter = i * n_plots + j\n",
    "            if this_filter < images.shape[0]:\n",
    "                this_img = images[this_filter, ...]\n",
    "                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n",
    "                  1 + j + j * img_w:1 + j + (j + 1) * img_w, :] = this_img\n",
    "    return m\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
